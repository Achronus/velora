{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from velora import History, Trajectory, Episodes\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "def iterate_batches(env: gym.Env, net: nn.Module, batch_size: int) -> Generator[Episodes, None, None]:\n",
    "    batch = Episodes()\n",
    "    episode = History()\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action_scores = net(obs.unsqueeze(0))\n",
    "        probs: torch.Tensor = torch.softmax(action_scores, dim=-1)\n",
    "        action = Categorical(probs).sample().item()\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        episode.add(\n",
    "            Trajectory(\n",
    "                action=action, \n",
    "                observation=obs, \n",
    "                reward=float(reward),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if terminated or truncated:\n",
    "            batch.add(episode)\n",
    "            episode = History()\n",
    "            next_obs, _ = env.reset()\n",
    "            \n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = Episodes()\n",
    "        \n",
    "        obs = next_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch: Episodes, percentile: int) -> tuple[torch.Tensor, torch.Tensor, float, float]:\n",
    "    ep_scores = batch.scores().numpy()\n",
    "    reward_bound = np.percentile(ep_scores, percentile).item()\n",
    "    reward_mean = np.mean(ep_scores).item()\n",
    "\n",
    "    best_batches = Episodes()\n",
    "\n",
    "    for ep in batch:\n",
    "        if ep.score() >= reward_bound:\n",
    "            best_batches.add(ep)\n",
    "    \n",
    "    return best_batches.observations(), best_batches.actions(), reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import NumpyToTorch\n",
    "import torch.optim as optim\n",
    "\n",
    "env: gym.Env = NumpyToTorch(gym.make(\"CartPole-v1\"))\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.688, reward_mean=19.7, rw_bound=20.0\n",
      "1: loss=0.683, reward_mean=21.6, rw_bound=22.5\n",
      "2: loss=0.640, reward_mean=14.9, rw_bound=15.0\n",
      "3: loss=0.681, reward_mean=18.6, rw_bound=21.0\n",
      "4: loss=0.623, reward_mean=13.7, rw_bound=14.5\n",
      "5: loss=0.688, reward_mean=19.7, rw_bound=21.5\n",
      "6: loss=0.659, reward_mean=19.7, rw_bound=25.5\n",
      "7: loss=0.688, reward_mean=17.6, rw_bound=19.5\n",
      "8: loss=0.658, reward_mean=18.7, rw_bound=22.5\n",
      "9: loss=0.693, reward_mean=25.8, rw_bound=25.5\n",
      "10: loss=0.657, reward_mean=24.0, rw_bound=30.0\n",
      "11: loss=0.669, reward_mean=23.2, rw_bound=23.0\n",
      "12: loss=0.667, reward_mean=34.9, rw_bound=44.0\n",
      "13: loss=0.670, reward_mean=28.1, rw_bound=35.0\n",
      "14: loss=0.657, reward_mean=28.9, rw_bound=32.0\n",
      "15: loss=0.650, reward_mean=33.7, rw_bound=37.5\n",
      "16: loss=0.658, reward_mean=27.4, rw_bound=32.5\n",
      "17: loss=0.640, reward_mean=45.9, rw_bound=52.0\n",
      "18: loss=0.640, reward_mean=33.5, rw_bound=33.0\n",
      "19: loss=0.631, reward_mean=42.1, rw_bound=45.0\n",
      "20: loss=0.631, reward_mean=32.9, rw_bound=35.5\n",
      "21: loss=0.622, reward_mean=36.1, rw_bound=36.5\n",
      "22: loss=0.612, reward_mean=51.2, rw_bound=61.5\n",
      "23: loss=0.627, reward_mean=54.8, rw_bound=64.5\n",
      "24: loss=0.611, reward_mean=47.1, rw_bound=50.5\n",
      "25: loss=0.614, reward_mean=51.9, rw_bound=57.5\n",
      "26: loss=0.614, reward_mean=36.8, rw_bound=43.0\n",
      "27: loss=0.614, reward_mean=43.3, rw_bound=46.5\n",
      "28: loss=0.612, reward_mean=57.3, rw_bound=67.5\n",
      "29: loss=0.598, reward_mean=50.9, rw_bound=57.5\n",
      "30: loss=0.594, reward_mean=59.9, rw_bound=66.0\n",
      "31: loss=0.624, reward_mean=62.4, rw_bound=71.0\n",
      "32: loss=0.614, reward_mean=73.4, rw_bound=80.5\n",
      "33: loss=0.597, reward_mean=81.1, rw_bound=85.5\n",
      "34: loss=0.609, reward_mean=69.2, rw_bound=88.0\n",
      "35: loss=0.619, reward_mean=69.1, rw_bound=76.5\n",
      "36: loss=0.588, reward_mean=59.1, rw_bound=74.5\n",
      "37: loss=0.583, reward_mean=80.7, rw_bound=99.0\n",
      "38: loss=0.607, reward_mean=71.3, rw_bound=90.0\n",
      "39: loss=0.598, reward_mean=109.3, rw_bound=131.5\n",
      "40: loss=0.581, reward_mean=123.0, rw_bound=140.5\n",
      "41: loss=0.581, reward_mean=108.2, rw_bound=142.0\n",
      "42: loss=0.592, reward_mean=104.4, rw_bound=112.0\n",
      "43: loss=0.582, reward_mean=103.6, rw_bound=128.5\n",
      "44: loss=0.558, reward_mean=111.8, rw_bound=134.5\n",
      "45: loss=0.573, reward_mean=114.3, rw_bound=162.5\n",
      "46: loss=0.593, reward_mean=108.2, rw_bound=130.5\n",
      "47: loss=0.569, reward_mean=145.2, rw_bound=168.0\n",
      "48: loss=0.552, reward_mean=116.1, rw_bound=133.5\n",
      "49: loss=0.564, reward_mean=140.1, rw_bound=195.0\n",
      "50: loss=0.560, reward_mean=170.0, rw_bound=208.5\n",
      "51: loss=0.565, reward_mean=161.8, rw_bound=186.0\n",
      "52: loss=0.553, reward_mean=159.5, rw_bound=190.0\n",
      "53: loss=0.547, reward_mean=209.7, rw_bound=248.5\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "solve_threshold = 195\n",
    "\n",
    "for i_batch, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "\n",
    "    if len(obs_v) == 0:\n",
    "        continue\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v: torch.Tensor = loss(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"{i_batch}: loss={loss_v:.3f}, reward_mean={reward_m:.1f}, rw_bound={reward_b:.1f}\")\n",
    "\n",
    "    if reward_m > solve_threshold:\n",
    "        print(\"Solved!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
