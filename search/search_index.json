{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Velora","text":"<p> Velora, a Liquid RL framework for NeuroFlow agents, empowering Autonomous Cyber Defence. </p> <p>Docs</p> <p>Code</p> <p>Velora is a lightweight and modular framework built on top of powerful libraries like Gymnasium [] and PyTorch []. It is home to a new type of RL agent called NeuroFlow (NF) that specializes in Autonomous Cyber Defence through a novel Deep Reinforcement Learning (RL) approach we call Liquid RL.</p>"},{"location":"#benefits","title":"Benefits","text":"<ul> <li>Explainability: NF agents use Liquid Neural Networks [] (LNNs) and Neural Circuit Policies [] (NCPs) to model Cyber system dynamics, not just data patterns. Also, they use sparse NCP connections to mimic biological efficiency, enabling clear, interpretable strategies via a labeled Strategy Library.</li> <li>Adaptability: NF agents dynamically grow their networks using a fitness score, adding more neurons to a backbone only when new Cyber strategies emerge, keeping agents compact and robust.</li> <li>Planning: NF agents use a Strategy Library and learned environment model to plan strategic sequences for proactive Cyber defense.</li> <li>Always Learning: using EWC [], NF agents refine existing strategies and learn new ones post-training, adapting to evolving Cyber threats like new attack patterns.</li> <li>Customizable: NF agents are PyTorch-based [], designed to be intuitive, easy to use, and modular so you can easily build your own!</li> </ul> <ul> <li> <p> Getting Started</p> <p>What are you waiting for?!</p> <p> Get Started</p> </li> <li> <p> Open Source, MIT</p> <p>Velora is licensed under the MIT License.</p> <p> License</p> </li> </ul>"},{"location":"#active-development","title":"Active Development","text":"<p>Velora is a tool that is continuously being developed. There's still a lot to do to make it a great framework, such as detailed API documentation, and expanding our NeuroFlow agents.</p> <p>Our goal is to provide a quality open-source product that works 'out-of-the-box' that everyone can experiment with, and then gradually fix unexpected bugs and introduce more features on the road to a <code>v1</code> release.</p> <ul> <li> <p> Roadmap</p> <p>Check out what we have planned for Velora.</p> <p> Explore</p> </li> </ul>"},{"location":"help/","title":"Help","text":"<p>This chapter focuses on steps you can take when encountering errors while using Velora.</p> <p>The easiest way to get help is to create a ticket on GitHub [].</p> <p>Currently we have no set policy or format for tickets. All that we ask is that you give us some information about the problem you are experiencing and the code you are using so we can try and reproduce the error and debug it accordingly \ud83d\ude0a.</p>"},{"location":"changelog/v0.0.1/","title":"v0.0.1 (Alpha) - 2024-12-08","text":""},{"location":"changelog/v0.0.1/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>(policy) Added decay rate methods to <code>EpsilonPolicy</code>.</li> <li>(analytics) Added <code>W&amp;B</code> Analytics class.</li> <li>(models) Added family of Sarsa models.</li> <li>(sarsa) Added <code>disable_logging</code> flag.</li> <li>(controller) Added logic to <code>RLController</code>.</li> <li>(gymhandler) Added core logic for <code>GymEnvHandler</code>.</li> <li>(metrics) Added Metric storage and logging.</li> <li>(plots) Added <code>plot_vf</code> method to controller.</li> </ul>"},{"location":"changelog/v0.0.1/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>(policy) Fixed epsilon exponential decay method.</li> <li>(metrics) Update metric implementation to simplify and fix bugs.</li> </ul>"},{"location":"changelog/v0.0.1/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>(env) Simplified env wrapper implementation.</li> <li>(rollouts) Added device validation.</li> <li>(utils) Refactored <code>utils</code> file to dir for simplicity</li> <li>(vfuncs) QOL changes to <code>V</code>, <code>Q</code> for simplicity</li> <li>(config) Updated <code>Config</code> settings to simplify.</li> <li>(agent) Updated policy, models, and config to simplify.</li> <li>(policy) Updated method argument name for clarity.</li> <li>(policy) Updated method names for clarity.</li> <li>(config) Updated settings for usability.</li> <li>(rollouts) Simplified agent storage methods.</li> <li>(mypy) Global updates to fix mypy errors.</li> <li>(agent) Modified Agent models to generalised form to simplify.</li> <li>(sarsa) Simplified Sarsa model implementation.</li> </ul>"},{"location":"changelog/v0.0.1/#performance","title":"\u26a1 Performance","text":"<ul> <li>(storage) Added <code>action</code>, <code>reward</code> tensor methods</li> <li>(returns) Refactored <code>returns</code> method for batching</li> </ul>"},{"location":"changelog/v0.0.1/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>(agent) Added <code>EpsilonPolicy</code>, <code>BaseSarsa</code> tests</li> <li>(sarsa) Added unit tests for Sarsa agents.</li> <li>(qtable) Add missing <code>QTable</code> tests.</li> <li>(analytics) Added missing Analytics tests.</li> <li>(controller) Updated and added unit tests.</li> </ul>"},{"location":"changelog/v0.0.1/#miscellaneous-tasks","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>(analytics) Updated docstrings for analytics.</li> <li>(utils) Added utility method <code>ignore_empty_dicts</code></li> <li>(tests) Added automated testing packages.</li> <li>(examples) Removed old examples.</li> <li>(controller) Add controller example.</li> </ul>"},{"location":"changelog/v0.0.2/","title":"v0.0.2 (Beta)","text":""},{"location":"changelog/v0.0.2/#v0021-beta-2025-02-18","title":"v0.0.21 (Beta) - 2025-02-18","text":""},{"location":"changelog/v0.0.2/#other","title":"\ud83d\udcbc Other","text":"<ul> <li>(torch) Refactored torch device into <code>cpu</code> and <code>cuda</code> groups.</li> </ul>"},{"location":"changelog/v0.0.2/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>(fix) Fixed broken <code>gym</code> and <code>buffer</code> tests.</li> </ul>"},{"location":"changelog/v0.0.2/#miscellaneous-tasks","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>(workflow) Updated workflows to use deps from docker containers.</li> </ul>"},{"location":"changelog/v0.0.2/#v002-beta-2025-02-18","title":"v0.0.2 (Beta) - 2025-02-18","text":""},{"location":"changelog/v0.0.2/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>(policy) Added CNN and MLP backbone policies.</li> <li>(agent) Added agent base classes.</li> <li>(lnn) Add Liquid Neural Network model.</li> <li>(gym) Added <code>Gymnasium</code> and generic utility methods.</li> <li>(buffer) Added <code>Replay</code> and <code>Rollout</code> buffer logic.</li> <li>(ddpg) Added a Liquid variant of DDPG.</li> <li>(cnn) Added basic CNN backbone.</li> </ul>"},{"location":"changelog/v0.0.2/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>(lnn) Fixed <code>x</code> dimension bug in <code>forward</code> method.</li> <li>(lnn) Updated <code>Cell</code> implementation to fix training bug.</li> <li>(lnn) Fixed <code>LiquidNCP</code> training bug.</li> <li>(models) Fixed PyTorch hook and device transfer compatibility.</li> </ul>"},{"location":"changelog/v0.0.2/#other_1","title":"\ud83d\udcbc Other","text":"<ul> <li>(poetry) Upgraded Poetry to v2.0.1 and deps to latest.</li> <li>(poetry) Updated all packages to latest versions.</li> </ul>"},{"location":"changelog/v0.0.2/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>(models) Migrated framework to Deep-RL methods.</li> <li>(env) Overhauled <code>GymEnvHandler</code> to improve functionality.</li> <li>(controller) Major overhaul of <code>RLController</code> for simplicity.</li> <li>(old) Removed old ValueFunctions and EpsilonPolicy.</li> <li>(lnn) Updated project code and docstrings for clarity.</li> <li>(alpha) Removed <code>v0.0.1</code> implementation for fresh start.</li> <li>(lnn) Added <code>device</code> parameter for CUDA.</li> <li>(buffer) Moved <code>to_tensor</code> as utility method and improved.</li> <li>(ddpg) Updated <code>DDPG</code> for modularity and accuracy.</li> <li>(lnn) Removed <code>timespans</code> logic to simplify.</li> <li>(ddpg) Removed <code>action_bounds</code> to simplify.</li> <li>(utils) Added and updated utility methods.</li> <li>(alpha) Removed remaining old <code>alpha</code> logic. and files</li> <li>(wiring) Moved <code>Wiring</code> -&gt; <code>models</code> directory for clarity.</li> <li>(gym) Utility method improvements.</li> <li>(wiring) Updated wiring methods.</li> </ul>"},{"location":"changelog/v0.0.2/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>(lnn) Updated <code>forward()</code> docstrings for clarity.</li> <li>(core) Added core documentation pages and content with assets.</li> <li>(api) Added docs for the <code>LNN</code> model API reference.</li> <li>(models) Improved model docstrings format and content.</li> <li>(history) Added brief history and motivation of Velora.</li> <li>(api) Added API reference documentation.</li> </ul>"},{"location":"changelog/v0.0.2/#testing_1","title":"\ud83e\uddea Testing","text":"<ul> <li>(missing) Added <code>buffer</code> and <code>gym</code> unit tests for coverage.</li> </ul>"},{"location":"changelog/v0.0.2/#miscellaneous-tasks_1","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>(prep) Added scripts and packages for CI prep.</li> <li>(readme) Updated README</li> <li>(readme) Updated README</li> </ul>"},{"location":"changelog/v0.0.3/","title":"v0.0.3 (Beta) - 2025-02-25","text":""},{"location":"changelog/v0.0.3/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>(gym) Updated methods and added environment search functionality.</li> <li>(buffer) Added <code>save</code> and <code>load</code> functionality.</li> <li>(utils) Added <code>total</code>, <code>active</code> parameter methods.</li> <li>(ddpg) Added model <code>save</code>, <code>load</code> functionality.</li> </ul>"},{"location":"changelog/v0.0.3/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>(gym) Fixed environment wrapping duplication bug.</li> <li>(ddpg) Removed frozen target parameters.</li> <li>(lnn) Fixed sparse weight connection update bug.</li> </ul>"},{"location":"changelog/v0.0.3/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>(buffer) Refactored buffers to inherit from a base class.</li> <li>(flake8) Project updates to fix <code>flake8</code> errors.</li> </ul>"},{"location":"changelog/v0.0.3/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>(api) Added utility <code>model</code> API docs.</li> <li>(customize) Added customize documentation, plus placeholders.</li> <li>(customize) Added new docs and updated existing ones.</li> <li>(tutorials) Added tutorials for utility methods.</li> <li>(api) Added documentation links to API.</li> <li>(core) Updated <code>roadmap</code>, added <code>DDPG</code> docs.</li> </ul>"},{"location":"changelog/v0.0.3/#styling","title":"\ud83c\udfa8 Styling","text":"<ul> <li>(isort) Organised file import statements.</li> </ul>"},{"location":"changelog/v0.0.3/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>(models) Added tests for <code>models</code> directory.</li> <li>(other) Added tests for <code>wiring</code>, <code>noise</code>, and <code>utils</code>.</li> <li>(cov) Added unit tests for line coverage.</li> <li>(gym) Added unit tests for new gym code.</li> <li>(cov) Added tests for coverage and fixed broken ones.</li> </ul>"},{"location":"changelog/v0.0.3/#miscellaneous-tasks","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>(workflow) Fix failing workflows.</li> <li>(workflow) Revert <code>docs</code> build workflow.</li> </ul>"},{"location":"changelog/v0.1.1/","title":"v0.1.1 - 2025-03-23","text":""},{"location":"changelog/v0.1.1/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>(callbacks) Added training callbacks.</li> <li>(watch) Added method to watch trained agents (notebooks).</li> <li>(record) Added <code>RecordVideos</code> callback with minor optimizations</li> <li>(train) Enhanced agent training loop and metric storage.</li> <li>(metrics) Moved <code>TrainMetrics</code> into<code>TrainState</code> for callbacks.</li> <li>(gradients) Added method to compute gradients of NCP networks.</li> <li>(analytics) Added <code>CometAnalytics</code> callback for cloud tracking.</li> <li>(version) Made <code>velora</code> compatible with <code>Python 3.11</code>.</li> </ul>"},{"location":"changelog/v0.1.1/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>(buffer) Reorganized buffer items to fix circular imports.</li> <li>(config) Fixed config copy bug.</li> <li>(imports) Fixed import circulation errors.</li> <li>(buffer) Fixed actions for multi-dimensional spaces.</li> <li>(ddpg) Fixed DDPG learning bug.</li> <li>(wiring) Fixed number of neuron connections bug.</li> <li>(metrics) Fixed metrics <code>std()</code> call bug when <code>window_size=1</code>.</li> </ul>"},{"location":"changelog/v0.1.1/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>(callbacks) Moved callbacks location for clarity.</li> <li>(save) Enhanced saved method to include <code>config.json</code>.</li> <li>(buffer) Added <code>warm</code> method to <code>ReplayBuffer</code> for simplicity.</li> <li>(config) Simplified config creation in <code>LiquidDDPG</code>.</li> <li>(watch) Removed <code>watch_notebook</code> method - redundant.</li> <li>(callbacks) Simplified callback handling in <code>train</code> method.</li> <li>(old) Removed old redundant code.</li> <li>(config) Simplified agent <code>config</code> creation.</li> <li>(callback) Refactored <code>SaveCheckpoints</code> to simplify init.</li> <li>(metrics) Simplified and enhanced metric implementation.</li> <li>(metrics) Added attributes for stored metrics.</li> <li>(callbacks) Modified <code>RecordVideo</code> env wrapper logic.</li> <li>(metrics) Minor updates + custom gradient removal.</li> <li>(callbacks) Simplified <code>RecordVideos</code> callback parameters.</li> <li>(metrics) Updated metric tracking to use SQLite database.</li> <li>(record) Added <code>root_path</code> to <code>record_last_episode</code> method.</li> <li>(checkpoints) Updated checkpoint target condition to simplify.</li> <li>(lnn) Updated LNN architecture for accuracy.</li> <li>(lnn) QoL additions and improvements.</li> <li>(ncp) Simplified and improved architecture.</li> <li>(tweaks) Minor changes to DDPG and buffer methods.</li> <li>(ncp) Added <code>NCPModule</code> base class to simplify Actor-Critics.</li> <li>(train) Improved console training output.</li> <li>(sparse) Removed <code>SparseParameter</code> (deprecated).</li> <li>(output) Added running step count to training output.</li> <li>(save) Updated saving and loading to <code>safetensors</code>.</li> </ul>"},{"location":"changelog/v0.1.1/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>(callbacks) Added and updated documentation for better flow.</li> <li>(config) Added <code>config</code> API reference and updated <code>save</code> docs.</li> <li>(install) Updated install docs for GPU and CPU versions.</li> <li>(callbacks) Added docs for <code>RecordVideos</code> &amp; updated existing ones.</li> <li>(metrics) Added docs for training metrics.</li> <li>(roadmap) Added content to roadmap.</li> <li>(additions) Updated and added docs for Comet &amp; new metric system.</li> <li>(docstrings) Updated codebase docstrings for clarity.</li> <li>(lnn) Updated customization docs for clarity.</li> <li>(updates) Updated <code>Buffer</code> and <code>Metrics</code> docs with new changes.</li> <li>(save): Added API references for saving and loading models.</li> </ul>"},{"location":"changelog/v0.1.1/#performance","title":"\u26a1 Performance","text":"<ul> <li>(buffer) Reduced total training time by 63%.</li> <li>(metrics) Reduced total training time by a further 40%.</li> <li>(ncp) Reduced training time with NCP compiling.</li> <li>(compile) Simplified compilation to agent level.</li> </ul>"},{"location":"changelog/v0.1.1/#styling","title":"\ud83c\udfa8 Styling","text":"<ul> <li>(output) Updated format for training output.</li> <li>(train) Improved training output details.</li> </ul>"},{"location":"changelog/v0.1.1/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>(buffer) Fixed failed tests.</li> <li>(callbacks) Added unit tests for new feature coverage.</li> <li>(save) Added and fixed broken tests for saving and loading.</li> <li>(cov) Added tests for coverage.</li> <li>(fix) Updated test container packages &amp; removed redundant tests.</li> <li>(fix) Fixed broken tests for new training logic.</li> <li>(fix) Fixed broken test cases for <code>TrainState.metrics</code> refactor.</li> <li>(fix) Added tests and fixed broken ones.</li> <li>(fix) Fixed broken DDPG test.</li> <li>(fix) Fixed unit tests.</li> <li>(stopping) Fixed <code>EarlyStopping</code>tests.</li> <li>(fix) Fixed broken tests with recent code changes.</li> <li>(all) Updated and added tests cases for coverage.</li> <li>(fix) Removed pre-release code to fix test cases.</li> <li>(fix) Updated tests to accommodate recent changes.</li> </ul>"},{"location":"changelog/v0.1.1/#miscellaneous-tasks","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>(build) Added <code>matplotlib</code> package as main dependency.</li> <li>(build) Removed redundant packages and updated existing ones.</li> <li>(packages) Refactored packages to fix <code>torch</code> install bug.</li> <li>(torch) Moved <code>torch</code> dependencies to group.</li> </ul>"},{"location":"changelog/v0.2.0/","title":"v0.2.0 - 2025-04-22","text":""},{"location":"changelog/v0.2.0/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>(callbacks) Enhanced callbacks for flexibility.</li> <li>(force) Added <code>force</code> flag to save methods for file overwrites.</li> <li>(ppo) Added <code>LiquidPPO</code> algorithm.</li> <li>(handler) Added file saving for training completion details.</li> <li>(ncp) Added multiple weight initialization options.</li> <li>(sac) Added <code>LiquidSAC</code> agent for continuous action spaces.</li> <li>(sac) Added <code>LiquidSACDiscrete</code> agent for discrete action spaces.</li> <li>(neuroflow) Added main logic for <code>NeuroFlow</code>.</li> <li>(agent) Added <code>NeuroFlowDiscrete</code> agent.</li> </ul>"},{"location":"changelog/v0.2.0/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>(ddpg) Fixed noise handling and prediction bugs.</li> <li>(cell) Fixed <code>sparsity_mask</code> assignment bug.</li> <li>(params) Fixed parameter counts in in DDPG.</li> <li>(ppo) Fixed PPO callback bugs and metric tracking.</li> <li>(config) Fixed bug with <code>train_params</code> in <code>RLAgentConfig</code>.</li> <li>(load) Fixed model loading bug.</li> <li>(buffer) Fixed <code>warm</code> method bug when <code>num_envs=1</code>.</li> <li>(buffer) Fixed save bug where directories don't exist.</li> </ul>"},{"location":"changelog/v0.2.0/#other","title":"\ud83d\udcbc Other","text":"<ul> <li>(box) Added Gymnasium box2d environments by default.</li> </ul>"},{"location":"changelog/v0.2.0/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>(ncp) Added <code>update_mask</code> helper methods.</li> <li>(metrics) Updated training metrics name for clarity.</li> <li>(metrics) Simplified metric classes using base class.</li> <li>(train) Refactored <code>TrainHandler</code>, <code>TrainConfig</code> to simplify.</li> <li>(buffer) Added Actor hidden state to buffer.</li> <li>(seed) Improved random seed generation.</li> <li>(save) Simplified <code>save</code>, <code>load</code> method implementations.</li> <li>(sac) Moved <code>SAC</code> agents to separate folder for simplicity.</li> <li>(ncp) Renamed <code>NCPModule</code> -&gt; <code>LiquidNCPModule</code> for clarity.</li> <li>(agents) Refactored framework to centre around <code>NeuroFlow</code>.</li> <li>(save) Moved <code>completed.json</code> to save directory.</li> <li>(warm) Improved buffer warming step implementation.</li> <li>(utils) Simplified <code>capture</code> utility methods.</li> </ul>"},{"location":"learn/customize/","title":"Customization - User Guide","text":"<p>This tutorial series shows you how to create your own algorithms using Velora's building blocks, such as the <code>Wiring</code>, <code>ReplayBuffer</code>, <code>SACActor</code>, and <code>SACCritic</code> classes.</p> <p>While we offer predefined algorithms, sometimes you need more flexibility or may want to explore and create your own. In fact, we encourage it! \ud83d\ude01</p> <p>So, to help you do that we've deliberately made it easy to plug-and-play with any of our classes in the framework. As long as you know how they work of course! \ud83d\ude09</p> In a Hurry? <p>Looking to jump to something specific? Use the navigation menu on the left \ud83d\udc48!</p> <p>Firstly, we'll start off with the <code>Wiring</code> class and then move onto Liquid Networks and other utility classes.</p>"},{"location":"learn/customize/backbone/","title":"Working with Prebuilt Backbones","text":"<p>Backbones are a way of performing feature extraction techniques before passing through it through the core prediction algorithm.</p> <p>Velora has two prebuilt options for this: an <code>MLP</code> and a <code>BasicCNN</code>.</p>"},{"location":"learn/customize/backbone/#mlp","title":"MLP","text":"API Docs <p><code>velora.models.backbone.MLP(in_features, n_hidden, out_features)</code></p> <p>The <code>MLP</code> is a dynamic class for building Multi-layer Perceptron Networks - the traditional fully-connected neuron architecture.</p> <p>Even though our algorithms focus on LNNs, we've added this into the framework on purpose to make it easy to compare the difference between the two for your own experiments.</p> <p>The main component is the <code>n_hidden</code> parameter that is a <code>List[int]</code> (or a single <code>int</code> for one layer). This creates the <code>nn.Linear</code> hidden layers for you automatically.</p> <p>It also comes with a few optional arguments, such as <code>activation</code> and <code>dropout_p</code>:</p> <ul> <li><code>activation</code> - defines the activation function between the layers, default is <code>relu</code>.</li> <li><code>dropout_p</code> - the dropout probability assigned between each layer, default is <code>0.0</code> meaning no dropout layers are applied.</li> </ul> <p>Here's a code example:</p> Python<pre><code>from velora.models.backbone import MLP\nimport torch\n\nnn = MLP(\n    in_features=4, \n    n_hidden=[256, 128, 64],  # (1) \n    out_features=2,\n    activation=\"relu\",  # (2)\n    dropout_p=0.2  # (3)\n)\n\nx = torch.ones((1, 4))\n\ny_pred = nn(x)\n</code></pre> <ol> <li>3 hidden layers</li> <li>Activation function used between the layers (optional)</li> <li>Dropout used between layers, 20% probability (optional)</li> </ol> <p>This code should work 'as is'.</p>"},{"location":"learn/customize/backbone/#basiccnn","title":"BasicCNN","text":"API Docs <p><code>velora.models.backbone.BasicCNN(in_channels)</code></p> <p>The <code>BasicCNN</code> uses a static architecture from the DQN Nature paper: Human-level control through deep reinforcement learning [].</p> <p>The paper used it for Atari games, but has been adopted in other libraries such as Stable-Baselines3 [] as a go-to CNN architecture, so we thought we'd use the same one! \ud83d\ude0a</p> <p>As an added bonus, it makes things easier for comparing SB3 baselines with our algorithms \ud83d\ude09.</p> Backbones with Velora agents <p>Currently, Velora doesn't directly use backbones in it's agents, they are strictly LNN or NCP architectures. So, you need to manually apply them yourself (we'll show you how to do this shortly).</p> <p>Typically, cyber environments don't use images as inputs, so we have no intention of changing this.</p> <p>To use the <code>BasicCNN</code> architecture, we pass in the number of <code>in_channels</code> and then can call the <code>forward()</code> or <code>out_size()</code> methods:</p> Python<pre><code>from velora.models.backbone import BasicCNN\nimport torch\n\ncnn = BasicCNN(1)\n\nx = torch.ones((1, 64, 64))  # (in_channels, height, width)\n\nn_feature_maps = cnn.out_size(x.size()[-2:])  # (1) 1024\ny_pred = cnn(x.unsqueeze(0))  # (1, 1024)\n</code></pre> <ol> <li>Number of <code>in_features</code> to an NCP or MLP</li> </ol> <p>This code should work 'as is'.</p>"},{"location":"learn/customize/backbone/#usage-with-a-custom-lnn","title":"Usage With a Custom LNN","text":"<p>To use the <code>BasicCNN</code> with a custom LNN, we can combine it with the <code>LiquidNCPNetwork</code> module:</p> Python<pre><code>from velora.models import LiquidNCPNetwork\nfrom velora.models.backbone import BasicCNN\n\nimport torch\n\ncnn = BasicCNN(1)\n\nx = torch.ones((1, 64, 64))  # (in_channels, height, width)\n\nn_neurons = 10\nout_features = 3\n\nn_feature_maps = cnn.out_size(x.size()[-2:])  # (1) 1024\n\nncp = LiquidNCPNetwork(n_feature_maps, n_neurons, out_features)\n\ncnn_pred = cnn(x.unsqueeze(0))  # (1, 1024)\ny_pred, hidden = ncp(cnn_pred)\n</code></pre> <p>This code should work 'as is'.</p> <p>Or, as a module:</p> Python<pre><code>from typing import Tuple\n\nfrom velora.models import LiquidNCPNetwork\nfrom velora.models.backbone import BasicCNN\n\nimport torch\nimport torch.nn as nn\n\n\nclass LiquidCNN(nn.Module):\n    \"\"\"\n    A basic Liquid Network with a CNN backbone.\n    \"\"\"\n    def __init__(self, \n        img_shape: Tuple[int, int, int], \n        n_neurons: int, \n        out_features: int\n    ) -&gt; None:\n        super().__init__()\n\n        if len(img_shape) != 3:\n            raise ValueError(\n                f\"Invalid '{img_shape=}'. Must be '(in_channels, height, width)'.\"\n            )\n\n        self.cnn = BasicCNN(img_shape[0])\n\n        in_features = self.cnn.out_size(img_shape[-2:])\n        self.ncp = LiquidNCPNetwork(in_features, n_neurons, out_features)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\"\"\"\n        out_features = self.cnn(x)\n        y_pred, hidden = self.ncp(out_features)\n        return y_pred, hidden\n\n\nx = torch.ones((1, 64, 64))  # (in_channels, height, width)\n\nmodel = LiquidCNN(x.shape, 10, 3)\n\ny_pred, hidden = model(x.unsqueeze(0))\n</code></pre> <p>This code should work 'as is'.</p> <p>And that completes the customization tutorials! Well done for getting this far! \ud83d\udc4f</p> <p>Still eager to learn more? Try one of the options \ud83d\udc47:</p> <ul> <li> <p> User Guide</p> <p>Learn how to use Velora's core algorithms and utility methods.</p> <p> Read more</p> </li> <li> <p> Theory</p> <p>Read the theory behind the RL algorithms Velora uses.</p> <p> Start learning</p> </li> </ul>"},{"location":"learn/customize/buffers/","title":"Working with Buffers","text":"<p>Buffers are a central piece for RL algorithms and are used heavily in our own implementations.</p> <p>In Off-Policy agents we use a <code>ReplayBuffer</code> and in On-Policy, a <code>RolloutBuffer</code>.</p> Rollout Buffers <p>We've recently discontinued the <code>RolloutBuffer</code> and removed it from the framework due to instability issues with LNNs and on-policy agents.</p> <p>So, you'll only see the docs for the <code>ReplayBuffer</code> here!</p> <p>We have our own implementations of these that are easy to work with \ud83d\ude0a.</p>"},{"location":"learn/customize/buffers/#replay-buffer","title":"Replay Buffer","text":"API Docs <p><code>velora.buffer.ReplayBuffer(capacity, state_dim, action_dim)</code></p> <p>To create a <code>ReplayBuffer</code>, simply give it a <code>capacity</code>, <code>state_dim</code>, <code>action_dim</code>, <code>hidden_dim</code> and a <code>torch.device</code> (optional):</p> Python<pre><code>from velora.buffer import ReplayBuffer\nfrom velora.utils import set_device\n\ndevice = set_device()\nbuffer = ReplayBuffer(\n    capacity=100_000, \n    state_dim=11, \n    action_dim=3,\n    hidden_dim=8, \n    device=device\n)\n</code></pre>"},{"location":"learn/customize/buffers/#add-one-item","title":"Add One Item","text":"API Docs <p><code>velora.buffer.ReplayBuffer.add(state, action, reward, next_state, done)</code></p> <p>To add an item, we use the <code>add()</code> method with a set of experience from a <code>Tuple</code> or the individual items:</p> Python<pre><code>import torch\n\nexp = (\n    torch.zeros((1, 4)),\n    torch.tensor((1.)),\n    2.,\n    torch.zeros((1, 4)),\n    False,\n)\n\nbuffer.add(*exp)\nbuffer.add(\n    torch.zeros((1, 4)),\n    torch.tensor((1.)),\n    2.,\n    torch.zeros((1, 4)),\n    False,\n)\n</code></pre>"},{"location":"learn/customize/buffers/#add-multiple-items","title":"Add Multiple Items","text":"API Docs <p><code>velora.buffer.ReplayBuffer.add_multi(state, action, reward, next_state, done)</code></p> <p>Or, we can add multiple values at once using the <code>add_multi()</code> method. Like before, we can use a set of experience from a <code>Tuple</code> or the individual items.</p> <p>The only difference, is that everything must be a <code>torch.Tensor</code>:</p> Python<pre><code>import torch\n\nexp = (\n    torch.zeros((5, 4)),\n    torch.ones((5, 1)),\n    torch.ones((5, 1)),\n    torch.zeros((5, 4)),\n    torch.zeros(5, 1),\n)\n\nbuffer.add_multi(*exp)\nbuffer.add_multi(\n    torch.zeros((5, 4)),\n    torch.ones((5, 1)),\n    torch.ones((5, 1)),\n    torch.zeros((5, 4)),\n    torch.zeros(5, 1),\n)\n</code></pre>"},{"location":"learn/customize/buffers/#get-samples","title":"Get Samples","text":"API Docs <p><code>velora.buffer.ReplayBuffer.sample(batch_size)</code></p> <p>We can then <code>sample()</code> a batch of experience:</p> Python<pre><code>batch = buffer.sample(batch_size=128)\n</code></pre> <p>This gives us a <code>BatchExperience</code> object. We'll talk about this later.</p> <p>Warning</p> <p>We can only sample from the buffer after we have enough experience. This is dictated by your <code>batch_size</code>.</p>"},{"location":"learn/customize/buffers/#warming-the-buffer","title":"Warming the Buffer","text":"API Docs <p><code>velora.buffer.ReplayBuffer.warm(agent, n_samples)</code></p> <p>Since the <code>ReplayBuffer</code> needs to have samples in it before we can <code>sample</code> from it. We can use a warming process to pre-populate the buffer.</p> <p>We have a dedicated method for this called <code>warm()</code> that automatically gathers experience up to <code>n_samples</code> without affecting your <code>episode</code> count during training.</p> <p>It requires two parameters:</p> Parameter Description Example <code>agent</code> The Velora agent instance to generate samples with. <code>NeuroFlow</code> <code>n_samples</code> The number of samples to generate. <code>batch_size * 2</code> <p>And has one optional parameter:</p> Parameter Description Default <code>num_envs</code> The number of vectorized environments to use for warming. <code>8</code> Python<pre><code>buffer.warm(agent, 1024)\n</code></pre>"},{"location":"learn/customize/buffers/#check-size","title":"Check Size","text":"<p>Lastly, we can check the <code>current size</code> of the buffer:</p> Python<pre><code>len(buffer)  # 1\n</code></pre>"},{"location":"learn/customize/buffers/#full-replay-example","title":"Full Replay Example","text":"<p>Here's a complete example of the code we've just seen:</p> Python<pre><code>from velora.utils import set_device\nfrom velora.models import NeuroFlow\n\nimport torch\n\ndevice = set_device()\n\nagent = NeuroFlow(\"CartPole-v1\", 20, 128, device=device)\n\n# Warm with at least 5 samples - can go over!\nagent.buffer.warm(agent, 5, num_envs=2)\n\n# Single experience\nexp = (\n    torch.zeros(agent.state_dim, device=device),\n    torch.tensor((1.), device=device),\n    2.,\n    torch.zeros(agent.state_dim, device=device),\n    False,\n    torch.zeros(agent.actor.hidden_size, device=device),\n)\nagent.buffer.add(*exp)\n\n# 3 more samples\nexp = (\n    torch.zeros((3, agent.state_dim), device=device),\n    torch.ones((3, 1), device=device),\n    torch.ones((3, 1), device=device),\n    torch.zeros((3, agent.state_dim), device=device),\n    torch.ones((3, 1), device=device),\n    torch.zeros((3, agent.actor.hidden_size), device=device),\n)\nagent.buffer.add_multi(*exp)\n\n# Get a batch\nbatch = agent.buffer.sample(batch_size=5)\n\nlen(agent.buffer)  # 10\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/customize/buffers/#saving-and-loading-buffers","title":"Saving and Loading Buffers","text":"<p>Sometimes you might want to reuse a buffers state in a different project. Well, now you can!</p> <p>We provide both a <code>save()</code> and <code>load()</code> feature for all buffers \ud83d\ude0e.</p>"},{"location":"learn/customize/buffers/#saving","title":"Saving","text":"API Docs <p><code>velora.buffer.BufferBase.save(dirpath)</code></p> <p>Once you've created a buffer and used it, simply pass in a <code>dirpath</code> to the <code>save</code> method. The final folder in the <code>dirpath</code> will be used to store the buffer's state. This includes:</p> <ul> <li><code>buffer_metadata.json</code> - the buffers metadata</li> <li><code>buffer_state.safetensors</code> - the buffers tensor state</li> </ul> <p>You can change the filename prefix <code>buffer_</code> with the optional <code>prefix</code> parameter:</p> Python<pre><code>from velora.buffer import ReplayBuffer\n\nbuffer = ReplayBuffer(100, 11, 3, 8, device=\"cpu\")\n\nbuffer.save(\n    'checkpoints/nf/CartPole_100', \n    prefix=\"buffer_\" # (1)\n)\n</code></pre> <ol> <li>Optional</li> </ol> <p>This code should work 'as is'.</p>"},{"location":"learn/customize/buffers/#loading","title":"Loading","text":"API Docs <p><code>velora.buffer.BufferBase.load(state_path, metadata)</code></p> <p>Then, to restore it into a new buffer instance, we use the <code>load()</code> method with the path to the <code>safetensors</code> file and the preloaded <code>metadata</code>:</p> Python<pre><code>import json\nfrom pathlib import Path\nfrom velora.buffer import ReplayBuffer\n\nroot_path = 'checkpoints/nf/CartPole_100'\n\nwith Path(root_path, 'buffer_metadata.json').open(\"r\") as f:\n    metadata = json.load(f)\n\nbuffer = ReplayBuffer.load(Path(root_path, 'buffer_state'), metadata)\nprint(buffer.metadata())\n# {\n# 'capacity': 100,\n# 'state_dim': 11,\n# 'action_dim': 3,\n# 'hidden_dim': 8,\n# 'position': 0,\n# 'size': 0,\n# 'device': 'cpu'\n# }\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/customize/buffers/#batchexperience","title":"BatchExperience","text":"API Docs <p><code>velora.buffer.BatchExperience</code></p> <p>Earlier, we mentioned the <code>BatchExperience</code> object. This is a dataclass that stores ours experience as separate tensors and allows you to easily extract them using their attributes.</p> <p>As mentioned, <code>BatchExperience</code> is the one you get out of the buffer:</p> Python<pre><code>from dataclasses import dataclass\nimport torch\n\n@dataclass\nclass BatchExperience:\n    \"\"\"\n    Storage container for a batch agent experiences.\n\n    Parameters:\n        states (torch.Tensor): a batch of environment observations\n        actions (torch.Tensor): a batch of agent actions taken in the states\n        rewards (torch.Tensor): a batch of rewards obtained for taking the actions\n        next_states (torch.Tensor): a batch of newly generated environment\n            observations following the actions taken\n        dones (torch.Tensor): a batch of environment completion statuses\n        hiddens (torch.Tensor): a batch of prediction network hidden states\n            (e.g., Actor)\n    \"\"\"\n\n    states: torch.Tensor\n    actions: torch.Tensor\n    rewards: torch.Tensor\n    next_states: torch.Tensor\n    dones: torch.Tensor\n    hiddens: torch.Tensor\n</code></pre> <p>All items in this class have the same shape <code>(batch_size, features)</code> and are easily accessible through their attributes, such as <code>batch.states</code>.</p> <p>It's super convenient for doing calculations like this \ud83d\ude09:</p> Python<pre><code>target_q = batch.rewards + (1 - batch.dones) * gamma * target_q\n</code></pre> <p>Next, we're going to talk about accessing existing models <code>Actor</code> and <code>Critic</code> classes \ud83d\udc4b.</p>"},{"location":"learn/customize/liquid/","title":"Building You're Own Liquid Networks","text":"<p>Now that we know how to build our sparsely connected neurons, we can start building our own LNNs.</p>"},{"location":"learn/customize/liquid/#cells-as-layers","title":"Cells as Layers","text":"API Docs <p><code>velora.models.lnn.NCPLiquidCell</code></p> <p>For this example, we'll focus on creating a network made purely of <code>NCPLiquidCells</code>.</p> <p>Initializing the layers is the easy part:</p> Python<pre><code>from collections import OrderedDict\n\nfrom velora.models.lnn import NCPLiquidCell\nfrom velora.wiring import Wiring\n\nin_features = 4\nn_neurons = 10\nout_features = 1\n\nwiring = Wiring(in_features, n_neurons, out_features)\nmasks, counts = wiring.data()\n\nnames = [\"inter\", \"command\", \"motor\"]\nlayers = [\n    NCPLiquidCell(in_features, counts.inter, masks.inter),\n    NCPLiquidCell(counts.inter, counts.command, masks.command),\n    NCPLiquidCell(counts.command, counts.motor, masks.motor)\n]\nlayers = OrderedDict([(name, layer) for name, layer in zip(names, layers)])\n</code></pre> <p>This code should work 'as is'.</p> <p>However, using them is tricky. Since we are using a Recurrent architecture, we need to iterate through each layer manually and retain their respective hidden states.</p> <p>This requires a bit of wizardry \u2728. Let's look at some more code first:</p> Python<pre><code>from typing import Tuple\n\nimport torch\n\nout_sizes = [layer.n_hidden for layer in layers.values()]\n\ndef ncp_forward(x: torch.Tensor, hidden: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\"\"\"\n    h_state = torch.split(hidden, out_sizes, dim=1) # (1)\n\n    new_h_state = []\n    inputs = x\n\n    # Handle layer independence\n    for i, layer in enumerate(layers.values()):\n        y_pred, h = layer(inputs, h_state[i]) # (2)\n        inputs = y_pred  # (batch_size, layer_out_features)\n        new_h_state.append(h)\n\n    new_h_state = torch.cat(new_h_state, dim=1)  # (3) (batch_size, n_units)\n    return y_pred, new_h_state\n</code></pre> <ol> <li>(1) Split hidden state -&gt; layer batches</li> <li>(2) Iterate over each layer -&gt; store hidden state</li> <li>(3) Merge the hidden states -&gt; a single tensor</li> </ol> layer(input) vs layer.forward(input) <p>In the previous code block we used <code>layer(input)</code> instead of <code>layer.forward(input)</code>. This is a best practice and recommended when using PyTorch <code>nn.Modules</code>. </p> <p>PyTorch automatically invokes the layer's <code>__call__</code> method which performs the following steps:</p> <ol> <li>Checks if the module is in training or evaluation mode.</li> <li>Handles any registered hooks (pre-forward and forward hooks).</li> <li>Calls the <code>forward()</code> method.</li> <li>Handles any registered backward hooks.</li> <li>Takes care of autograd (automatic differentiation) mechanics.</li> </ol> <p>Using <code>layer.forward()</code> would only execute the forward pass logic, removing a lot of the added benefits such as:</p> <ul> <li>Proper handling of training vs evaluation modes (affects dropout, batch norm, etc.).</li> <li>Execution of any registered hooks that might be needed for debugging or monitoring.</li> <li>Correct setup of the autograd graph for backpropagation.</li> </ul> <p>\u2757 Therefore, it is highly recommended to use <code>layer(input)</code> rather than calling the <code>forward()</code> method directly to maximize the added benefits. </p> <p>Keep an eye out for this when building your own models! \ud83d\ude09</p> <p>Firstly, we calculate the <code>output sizes</code> for each layer and store them in a list.</p> <p>Next, we go into the function, split our hidden state into batches that match the output sizes.</p> <p>Then, we iterate over each layer, passing the layers prediction into the next layer, while storing the layer's hidden state in a list.</p> <p>Lastly, we flatten the networks hidden states back into a single tensor and return the final layer's prediction with the flattened hidden state array.</p> <p>Now we can start using our network! Given some input <code>x</code> and an empty hidden state, we can make a prediction:</p> Python<pre><code>import torch\n\n# Set our inputs\nx = torch.tensor(\n    [-6.6, -1.1, -5.98, -1.69]\n).unsqueeze(0)\n\n# Compute our hidden state size\nn_units = n_neurons + out_features\n# n_units = sum(out_sizes)  # Same as above!\nbatch_size = x.size()[0]\n\n# Create a starting hidden state\nh_state = torch.zeros((batch_size, n_units))\n\n# Make a prediction\ny_pred, h_state = ncp_forward(x, h_state)\n</code></pre>"},{"location":"learn/customize/liquid/#full-code-example","title":"Full Code Example","text":"<p>Here's the complete code we've looked at:</p> Python<pre><code>from collections import OrderedDict\nfrom typing import Tuple\n\nfrom velora.models.lnn.cell import NCPLiquidCell\nfrom velora.wiring import Wiring\n\nimport torch\n\nin_features = 4\nn_neurons = 10\nout_features = 1\n\nwiring = Wiring(in_features, n_neurons, out_features)\nmasks, counts = wiring.data()\n\nnames = [\"inter\", \"command\", \"motor\"]\nlayers = [\n    NCPLiquidCell(in_features, counts.inter, masks.inter),\n    NCPLiquidCell(counts.inter, counts.command, masks.command),\n    NCPLiquidCell(counts.command, counts.motor, masks.motor)\n]\nlayers = OrderedDict([(name, layer) for name, layer in zip(names, layers)])\n\nout_sizes = [layer.n_hidden for layer in layers.values()]\n\ndef ncp_forward(x: torch.Tensor, hidden: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\"\"\"\n    h_state = torch.split(hidden, out_sizes, dim=1)\n\n    new_h_state = []\n    inputs = x\n\n    # Handle layer independence\n    for i, layer in enumerate(layers.values()):\n        y_pred, h = layer(inputs, h_state[i])\n        inputs = y_pred  # (batch_size, layer_out_features)\n        new_h_state.append(h)\n\n    new_h_state = torch.cat(new_h_state, dim=1)  # (batch_size, n_units)\n    return y_pred, new_h_state\n\n\n# Set our inputs\nx = torch.tensor(\n    [-6.6, -1.1, -5.98, -1.69]\n).unsqueeze(0)\n\n# Compute our hidden state size\nn_units = n_neurons + out_features\n# n_units = sum(out_sizes)  # Same as above!\nbatch_size = x.size()[0]\n\n# Create a starting hidden state\nh_state = torch.zeros((batch_size, n_units))\n\n# Make a prediction\ny_pred, h_state = ncp_forward(x, h_state)\n</code></pre> <p>This code should work 'as is'.</p> <p>And that's it! Now you can use cells individually! But honestly, all of that is a exhausting \ud83e\udd71.</p> <p>So instead, why don't we use a prebuilt version? \ud83d\ude09</p>"},{"location":"learn/customize/liquid/#prebuilt-networks","title":"Prebuilt Networks","text":""},{"location":"learn/customize/liquid/#liquid-ncp-networks","title":"Liquid NCP Networks","text":"API Docs <p><code>velora.models.lnn.LiquidNCPNetwork(in_features, n_neurons, out_features)</code></p> <p>We can eliminate the need to do the previous step by using our prebuilt <code>LiquidNCPNetwork</code>.</p> <p>Admittedly, it uses a different architecture but follows a relatively similar format.</p> The Real Architecture <p>For those interested, the real architecture consists of:</p> <ul> <li>Two <code>SparseLinear</code> layers</li> <li>One <code>NCPLiquidCell</code> layer</li> </ul> <p>We initially tested them as three <code>NCPLiquidCells</code> and found their stability lacking. We hypothesis that additional cells can interfere with the agent's ability to learn an accurate set of system dynamics.</p> <p>However, we still encourage you to test this out for yourselves! Exploration is not just for RL agents! \ud83d\ude09</p> <p>The main thing here is that all you need to do is write two lines of code and boom \ud83d\udca5, you've got a working, and heavily tested Liquid Network! \ud83d\ude09</p> Python<pre><code>from velora.models import LiquidNCPNetwork\nimport torch\n\nx = torch.tensor([-6.6, -1.1, -5.98, -1.69]).unsqueeze(0)  # Must be 2d\n\nncp = LiquidNCPNetwork(4, 10, 1)\ny_pred, h_state = ncp(x)\n</code></pre> <p>This code should work 'as is'.</p> <p>Notice how we don't need to manually define a hidden state. It's done for us automatically! \ud83d\ude0e</p>"},{"location":"learn/customize/liquid/#ncp-networks","title":"NCP Networks","text":"API Docs <p><code>velora.models.lnn.NCPNetwork(in_features, n_neurons, out_features)</code></p> <p>We also have a non-liquid variant called an <code>NCPNetwork</code>. This removes the RNN capabilities and treats the network as a standard sparse linear one using NCP principles.</p> <p>The API is the same but the output is just a <code>y_pred</code> instead of a <code>(y_pred, h_state)</code>.</p> Python<pre><code>from velora.models import NCPNetwork\nimport torch\n\nx = torch.tensor([-6.6, -1.1, -5.98, -1.69]).unsqueeze(0)  # Must be 2d\n\nncp = NCPNetwork(4, 10, 1)\ny_pred = ncp(x)\n</code></pre> <p>This code should work 'as is'.</p> <p>Next, we'll look at working with buffers! \ud83e\udd29</p>"},{"location":"learn/customize/wiring/","title":"Creating Sparse Neurons","text":"<p>A major part of the NCP algorithm is sparse connections. This is handled through the <code>Wiring</code> class which is then used by our <code>SparseLinear</code> module.</p>"},{"location":"learn/customize/wiring/#wiring-and-layer-masks","title":"Wiring and Layer Masks","text":"API Docs <p><code>velora.wiring.Wiring(in_features, n_neurons, out_features)</code></p> <p>The <code>Wiring</code> class's main purpose is to create sparsity masks for the three NCP layers and store the information in dataclasses: <code>NeuronCounts</code>, <code>SynapseCounts</code>, and <code>LayerMasks</code>.</p>"},{"location":"learn/customize/wiring/#basic-usage","title":"Basic Usage","text":"<p>To use it, we create an instance of the <code>Wiring</code> class and then call the <code>data()</code> method to retrieve the <code>NeuronCounts</code> and <code>LayerMasks</code>:</p> SynapseCounts <p><code>SynapseCounts</code> is strictly used internally inside the wiring class. Typically, you won't need to access this or apply it elsewhere. </p> <p>However, you can access it using the <code>n_connections</code> attribute if you need to.</p> Python<pre><code>from velora.wiring import Wiring\n\nwiring = Wiring(in_features=3, n_neurons=10, out_features=1)\nmasks, counts = wiring.data()\n</code></pre> <p>This code should work 'as is'.</p> <p>The rest is all done for you automatically behind the scenes.</p>"},{"location":"learn/customize/wiring/#creating-an-ncp-network","title":"Creating an NCP Network","text":"<p>Then, to create your own NCP network, you use them like this:</p> Python<pre><code>from velora.models.lnn import NCPLiquidCell\nfrom velora.wiring import Wiring\n\nin_features = 3\n\nwiring = Wiring(in_features, n_neurons=10, out_features=1)\nmasks, counts = wiring.data()\n\nlayers = [\n    NCPLiquidCell(in_features, counts.inter, masks.inter),\n    NCPLiquidCell(counts.inter, counts.command, masks.command),\n    NCPLiquidCell(counts.command, counts.motor, masks.motor)\n]\n</code></pre> <p>This code should work 'as is'.</p> <p>There is also an optional <code>sparsity_level</code> parameter that controls the connection sparsity between neurons:</p> <ol> <li>When <code>0.1</code> neurons are very dense, close to a traditional Neural Network.</li> <li>When <code>0.9</code> neurons are extremely sparse.</li> </ol> Python<pre><code>wiring = Wiring(\n    in_features, \n    n_neurons=10, \n    out_features=1,\n    sparsity_level=0.5 # (1)\n)\n</code></pre> <ol> <li>Optional</li> </ol> <p>Experimenting with this could be interesting for your own use cases.</p> <p>We've found <code>0.5</code> to be optimal (which is default) for most cases, providing a decent balance between training speed and performance. So, we recommend you start with this first! \ud83d\ude0a</p>"},{"location":"learn/customize/wiring/#dataclasses","title":"Dataclasses","text":"<p>When calling <code>wiring.data()</code> we receive two dataclasses: <code>NeuronCounts</code> and <code>LayerMasks</code>.</p> <p>Both are designed to be simple and intuitive.</p>"},{"location":"learn/customize/wiring/#neuroncounts","title":"NeuronCounts","text":"API Docs <p><code>velora.wiring.NeuronCounts</code></p> <p><code>NeuronCounts</code> holds the counts for each type of node:</p> Python<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass NeuronCounts:\n    \"\"\"\n    Storage container for NCP neuron category counts.\n\n    Parameters:\n        sensory (int): number of input nodes\n        inter (int): number of decision nodes\n        command (int): number of high-level decision nodes\n        motor (int): number of output nodes\n    \"\"\"\n\n    sensory: int\n    inter: int\n    command: int\n    motor: int\n</code></pre>"},{"location":"learn/customize/wiring/#layermasks","title":"LayerMasks","text":"API Docs <p><code>velora.wiring.LayerMasks</code></p> <p><code>LayerMasks</code> holds the created sparsity masks for each NCP layer:</p> Python<pre><code>from dataclasses import dataclass\n\nimport torch\n\n@dataclass\nclass LayerMasks:\n    \"\"\"\n    Storage container for layer masks.\n\n    Parameters:\n        inter (torch.Tensor): sparse weight mask for input layer\n        command (torch.Tensor): sparse weight mask for hidden layer\n        motor (torch.Tensor): sparse weight mask for output layer\n        recurrent (torch.Tensor): sparse weight mask for recurrent connections\n    \"\"\"\n\n    inter: torch.Tensor\n    command: torch.Tensor\n    motor: torch.Tensor\n    recurrent: torch.Tensor\n</code></pre> <p>The masks will vary depending on the network size and the <code>seed</code> you set. They are random connections after all!</p> Want to know how they work? <p>You can read more about them in the Theory - Liquid Neural Networks page.</p> <p>We highly recommend you set a <code>seed</code> using the <code>set_seed</code> utility method first before creating a <code>Wiring</code> instance. This will help you maintain reproducibility between experiments:</p> Python<pre><code>from velora.utils import set_seed\nfrom velora.wiring import Wiring\n\nset_seed(64)\n\nwiring = Wiring(in_features=3, n_neurons=10, out_features=1)\nmasks, counts = wiring.data()\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/customize/wiring/#synapsecounts","title":"SynapseCounts","text":"API Docs <p><code>velora.wiring.SynapseCounts</code></p> <p><code>SynapseCounts</code> holds the synapse connection counts for each node type:</p> Python<pre><code>@dataclass\nclass SynapseCounts:\n    \"\"\"\n    Storage container for NCP neuron synapse connection counts.\n\n    Parameters:\n        sensory (int): number of connections for input nodes\n        inter (int): number of connections for decision nodes\n        command (int): number of connections for high-level decision nodes\n        motor (int): number of connections for output nodes\n    \"\"\"\n\n    sensory: int\n    inter: int\n    command: int\n    motor: int\n</code></pre> <p>As we've discussed, you likely won't ever need to use this. It's strictly used internally inside the <code>Wiring</code> class.</p> <p>To access it through a created <code>Wiring</code> class instance, we use the <code>n_connections</code> attribute:</p> Python<pre><code>from velora.utils import set_seed\nfrom velora.wiring import Wiring\n\nset_seed(64)\n\nwiring = Wiring(in_features=3, n_neurons=10, out_features=1)\nwiring.n_connections\n\n# SynapseCounts(sensory=3, inter=2, command=4, motor=2)\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/customize/wiring/#importing-dataclasses","title":"Importing Dataclasses","text":"<p>If you need to work with the dataclasses directly, you can manually import them from the <code>wiring</code> module:</p> Python<pre><code>from velora.wiring import LayerMasks, NeuronCounts, SynapseCounts\n\nn_sensory = 1\n\ncounts = NeuronCounts(sensory=n_sensory, inter=6, command=4, motor=2)\n\nmasks = LayerMasks(\n    inter=torch.zeros(\n        (n_sensory, counts.inter),\n        dtype=torch.int32,\n    ),\n    command=torch.zeros(\n        (counts.inter, counts.command),\n        dtype=torch.int32,\n    ),\n    motor=torch.zeros(\n        (counts.command, counts.motor),\n        dtype=torch.int32,\n    ),\n)\n\nconnections = SynapseCounts(sensory=1, inter=2, command=3, motor=2)\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/customize/wiring/#sparse-linear-layers","title":"Sparse Linear Layers","text":"API Docs <p><code>velora.models.lnn.SparseLinear(in_features, out_features, mask)</code></p> <p>We've seen how to use the <code>Wiring</code> class in <code>NCPLiquidCells</code> but what about in <code>Linear</code> layers?</p> <p>We've created our own implementation for this called a <code>SparseLinear</code> layer that applies the sparsity mask to the weights automatically.</p> <p>You can implement one like this:</p> Python<pre><code>from velora.models.lnn import SparseLinear\nfrom velora.wiring import Wiring\n\nimport torch\n\nwiring = Wiring(4, 10, 1)\nl1 = SparseLinear(4, 1, torch.abs(wiring.masks.motor.T))\n</code></pre> <p>This code should work 'as is'.</p> <p>Notice how we transpose (<code>T</code>) the mask and then take it's absolute value.</p> <p>The transpose operation is required to ensure our mask fits the weights correctly and take the absolute to ensure gradient stability (turning <code>-1</code> -&gt; <code>1</code>).</p> <p>We didn't have to do this in the <code>NCPLiquidCell</code> because they have their own separate mask processing! \ud83d\ude09</p> <p>Up next we will look at the methods available for building Liquid Networks. See you there! \ud83d\udc4b</p>"},{"location":"learn/customize/modules/","title":"Agent Modules - Crash Course","text":"API Docs <p><code>velora.models.nf.modules</code></p> <p>There might be a time where you want to experiment with specific <code>Actor</code> or <code>Critic</code> modules individually, without using an existing agent.</p> <p>While it's very uncommon, it's possible with our framework! You'll learn all about the different modules in this section. \ud83d\ude01</p> In a Hurry? <p>Looking to jump to something specific? Use the navigation menu on the left \ud83d\udc48!</p>"},{"location":"learn/customize/modules/#the-basics","title":"The Basics","text":"<p>Under-the-hood, Velora's agents are made up of different modules that utilise PyTorch's functionality.</p> <p>Each module has it's own <code>continuous</code> and <code>discrete</code> variant that is accessible from the API using:</p> Python<pre><code>velora.models.nf.modules import [module]\n</code></pre> Read the API Docs! <p>We highly recommend you refer to the API docs for complete details on each of the modules. They are far more extensive than the details in this chapter. API doc links are provided in the respective sections.</p> <p>This chapter is only a quick crash course for the methods the modules use and assumes that you already have a solid grounding in PyTorch basics.</p>"},{"location":"learn/customize/modules/#saving-and-loading-modules","title":"Saving and Loading Modules","text":"<p>Every module has it's own saving and loading mechanism. To save a modules state, we use the <code>state_dict</code> method:</p> Python<pre><code>sd = module.state_dict()\n</code></pre> <p>And to restore it into a new or existing module, we use <code>load_state_dict</code>:</p> Python<pre><code>module.load_state_dict(sd)\n</code></pre> <p>When you're ready, click the button \ud83d\udc47 to jump into our first set of modules - <code>ActorModules</code>! \ud83d\ude80</p>"},{"location":"learn/customize/modules/actor/","title":"Actor Modules","text":"<p>Actor modules follow the Actor part of the Actor-Critic architecture. In NF's case, we follow a SAC base with Liquid NCP Networks, so the <code>continuous</code> variant uses a Gaussian policy, and the <code>discrete</code> variant a Categorical one.</p> <p>The layout of the modules are identical but their underlying functionality differs to handle their respective use cases.</p> <p>The only differences are the required <code>init</code> parameters and the number of items returned by the <code>forward</code> method.</p> <p>Actor modules are a wrapper over the top of PyTorch functionality and are made up of the following components:</p> Attribute Description PyTorch Item <code>network</code> The Actor network. <code>torch.nn.Module</code> <code>optim</code> The Actor's optimizer. <code>torch.optim.Optimizer</code>"},{"location":"learn/customize/modules/actor/#discrete","title":"Discrete","text":"API Docs <p><code>velora.models.nf.modules.ActorModuleDiscrete</code></p> <p><code>velora.models.sac.SACActorDiscrete</code></p> <p>For <code>discrete</code> action spaces, we use the <code>ActorModuleDiscrete</code> class.</p> <p>This accepts the following parameters:</p> Parameter Description Default <code>state_dim</code> The dimension of the state space. - <code>n_neurons</code> The number of decision/hidden neurons. - <code>action_dim</code> The dimension of the action space. - <code>optim</code> The PyTorch optimizer. <code>torch.optim.Adam</code> <code>lr</code> The optimizer learning rate. <code>0.0003</code> <code>device</code> The device to perform computations on. E.g., <code>cpu</code> or <code>cuda:0</code>. <code>None</code>"},{"location":"learn/customize/modules/actor/#continuous","title":"Continuous","text":"API Docs <p><code>velora.models.nf.modules.ActorModule</code></p> <p><code>velora.models.sac.SACActor</code></p> <p>For <code>continuous</code> action spaces, we use the <code>ActorModule</code> class.</p> <p>This accepts the following parameters:</p> Parameter Description Default <code>state_dim</code> The dimension of the state space. - <code>n_neurons</code> The number of decision/hidden neurons. - <code>action_dim</code> The dimension of the action space. - <code>action_scale</code> The scale factor to map the normalized actions to the environment's action range. - <code>action_bias</code> The bias/offset to center the normalized actions to the environment's action range. - <code>log_std_min</code> The minimum log standard deviation of the action distribution. <code>-5</code> <code>log_std_max</code> The maximum log standard deviation of the action distribution. <code>2</code> <code>optim</code> The PyTorch optimizer. <code>torch.optim.Adam</code> <code>lr</code> The optimizer learning rate. <code>0.0003</code> <code>device</code> The device to perform computations on. E.g., <code>cpu</code> or <code>cuda:0</code>. <code>None</code> Computing Scale Factors <p>The scale factors are designed to go from normalized actions back to the normal environment's action range. This is fundamental for SAC's training stability.</p> <p>To calculate them, we use the following:</p> Python<pre><code>action_scale = torch.tensor(\n    env.action_space.high - env.action_space.low,\n    device=device,\n) / 2.0\n\naction_bias = torch.tensor(\n    env.action_space.high + env.action_space.low,\n    device=device,\n) / 2.0\n</code></pre>"},{"location":"learn/customize/modules/actor/#updating-gradients","title":"Updating Gradients","text":"<p>To perform a gradient update, we use the <code>gradient_step</code> method:</p> Python<pre><code>actor.gradient_step(loss)\n</code></pre>"},{"location":"learn/customize/modules/actor/#prediction","title":"Prediction","text":"<p>To make a prediction, we use the <code>predict</code> method:</p> Python<pre><code>action, hidden = actor.predict(obs, hidden)\n</code></pre>"},{"location":"learn/customize/modules/actor/#forward-pass","title":"Forward Pass","text":"<p>For a complete network forward pass used during training, we use the <code>forward</code> method:</p> Python<pre><code># ActorModuleDiscrete\nactions, probs, log_prob, hidden = actor.forward(obs, hidden)\n\n# ActorModule\nactions, log_prob, hidden = actor.forward(obs, hidden)\n</code></pre>"},{"location":"learn/customize/modules/actor/#training-vs-evaluation-mode","title":"Training vs. Evaluation Mode","text":"<p>To quickly swap between the networks training and evaluation mode we use the <code>train_mode</code> and <code>eval_mode</code> methods:</p> Python<pre><code># Evaluate mode active\nactor.eval_mode()\n\n# Training mode active\nactor.train_mode()\n</code></pre> <p>Next, we'll look at the <code>critic</code> modules! \ud83d\ude80</p>"},{"location":"learn/customize/modules/critic/","title":"Critic Modules","text":"<p>Critic modules follow the Critic part of the Actor-Critic architecture. In NF's case, we follow a SAC base with NCP Networks where both variants estimate Q-values using two target networks.</p> <p>The layout of the modules are identical but their underlying functionality differs to handle their respective use cases.</p> <p>The only differences are the required parameters for the <code>predict</code> and <code>target_predict</code> methods.</p> <p>Critic modules are a wrapper over the top of PyTorch functionality and are made up of the following components:</p> Attribute Description PyTorch Item <code>network1</code> The first Critic network. <code>torch.nn.Module</code> <code>network2</code> The second Critic network. <code>torch.nn.Module</code> <code>target1</code> The first Critic's target network. <code>torch.nn.Module</code> <code>target2</code> The second Critic's target network. <code>torch.nn.Module</code> <code>optim1</code> The first Critic network's optimizer. <code>torch.optim.Optimizer</code> <code>optim1</code> The second Critic network's optimizer. <code>torch.optim.Optimizer</code>"},{"location":"learn/customize/modules/critic/#discrete","title":"Discrete","text":"API Docs <p><code>velora.models.nf.modules.CriticModuleDiscrete</code></p> <p><code>velora.models.sac.SACCriticNCPDiscrete</code></p> <p>For <code>discrete</code> action spaces, we use the <code>CriticModuleDiscrete</code> class.</p> <p>This accepts the following parameters:</p> Parameter Description Default <code>state_dim</code> The dimension of the state space. - <code>n_neurons</code> The number of decision/hidden neurons. - <code>action_dim</code> The dimension of the action space. - <code>optim</code> The PyTorch optimizer. <code>torch.optim.Adam</code> <code>lr</code> The optimizer learning rate. <code>0.0003</code> <code>tau</code> The soft target network update factor. <code>0.0005</code> <code>device</code> The device to perform computations on. E.g., <code>cpu</code> or <code>cuda:0</code>. <code>None</code>"},{"location":"learn/customize/modules/critic/#continuous","title":"Continuous","text":"API Docs <p><code>velora.models.nf.modules.CriticModule</code></p> <p><code>velora.models.sac.SACCriticNCP</code></p> <p>For <code>continuous</code> action spaces, we use the <code>CriticModule</code> class.</p> <p>The parameters are the same as the <code>CriticModuleDiscrete</code> class.</p>"},{"location":"learn/customize/modules/critic/#target-updates","title":"Target Updates","text":"<p>To update the target networks we use the <code>update_targets</code> method:</p> Python<pre><code>critic.update_targets()\n</code></pre>"},{"location":"learn/customize/modules/critic/#updating-gradients","title":"Updating Gradients","text":"<p>To update the network gradients, we use the <code>gradient_step</code> method:</p> Python<pre><code>critic.gradient_step(c1_loss, c2_loss)\n</code></pre>"},{"location":"learn/customize/modules/critic/#prediction","title":"Prediction","text":"<p>To make a prediction with the Critic networks, we use the <code>predict</code> method:</p> Python<pre><code># CriticModuleDiscrete\nq1_pred, q2_pred = critic.predict(obs)\n\n# CriticModule\nq1_pred, q2_pred = critic.predict(obs, actions)\n</code></pre>"},{"location":"learn/customize/modules/critic/#target-prediction","title":"Target Prediction","text":"<p>To make a prediction with the target networks, we use the <code>target_predict</code> method:</p> Python<pre><code># CriticModuleDiscrete\nnext_q_min = critic.target_predict(obs)\n\n# CriticModule\nnext_q_min = critic.target_predict(obs, actions)\n</code></pre> <p>This gives us the smallest next Q-Value prediction between the two target networks (<code>torch.min(q_values1, q_values2)</code>).</p> <p>Next, we'll look at the <code>entropy</code> modules! \ud83d\ude80</p>"},{"location":"learn/customize/modules/entropy/","title":"Entropy Modules","text":"<p>Entropy modules are an extension to the SAC algorithm that are used for automatic tuning.</p> <p>The layout of the modules are identical but their underlying functionality differs to handle their respective use cases.</p> <p>The only method differences are the required parameters for the <code>compute_loss</code> method.</p> <p>Entropy modules are a wrapper over the top of PyTorch functionality and are made up of the following components:</p> Attribute Description PyTorch Item <code>target</code> The target entropy value. <code>float</code> or <code>torch.Tensor</code> <code>log_alpha</code> A tunable parameter. <code>torch.nn.Parameter</code> <code>alpha</code> The current entropy coefficient. <code>torch.Tensor</code> <code>optim</code> The entropy optimizer. <code>torch.optim.Optimizer</code>"},{"location":"learn/customize/modules/entropy/#discrete","title":"Discrete","text":"API Docs <p><code>velora.models.nf.modules.EntropyModuleDiscrete</code></p> <p>For <code>discrete</code> action spaces, we use the <code>EntropyModuleDiscrete</code> class.</p> <p>This accepts the following parameters:</p> Parameter Description Default <code>action_dim</code> The dimension of the action space. - <code>initial_alpha</code> The starting entropy coefficient value. <code>1.0</code> <code>optim</code> The PyTorch optimizer. <code>torch.optim.Adam</code> <code>lr</code> The optimizer learning rate. <code>0.0003</code> <code>device</code> The device to perform computations on. E.g., <code>cpu</code> or <code>cuda:0</code>. <code>None</code>"},{"location":"learn/customize/modules/entropy/#continuous","title":"Continuous","text":"API Docs <p><code>velora.models.nf.modules.EntropyModule</code></p> <p>For <code>continuous</code> action spaces, we use the <code>EntropyModule</code> class.</p> <p>The parameters are the same as the <code>EntropyModuleDiscrete</code> class.</p>"},{"location":"learn/customize/modules/entropy/#compute-loss","title":"Compute Loss","text":"<p>To compute the module loss, we use the <code>compute_loss</code> method:</p> Python<pre><code># EntropyModuleDiscrete\nentropy_loss = entropy.compute_loss(actor_probs, actor_log_probs)\n\n# EntropyModule\nentropy_loss = entropy.compute_loss(actor_log_probs)\n</code></pre>"},{"location":"learn/customize/modules/entropy/#updating-gradients","title":"Updating Gradients","text":"<p>To update the gradients, we use the <code>gradient_step</code> method:</p> Python<pre><code>entropy.gradient_step(entropy_loss)\n</code></pre>"},{"location":"learn/customize/modules/entropy/#config","title":"Config","text":"<p>To quickly get an overview of the modules parameters we can use the <code>config</code> method:</p> Python<pre><code>config = entropy.config()\n</code></pre> <p>This provides us with an <code>EntropyParameters</code> config model containing details about the module.</p> <p>Other modules have their own respective config models that are obtained using their attribute <code>module.config</code> instead.</p> <p>Next, we'll dive into working with static <code>backbones</code> that Velora offers \ud83d\udc4b.</p>"},{"location":"learn/reference/","title":"API Reference","text":"<p>Velora comes packed with its own methods and classes to really push your development game to the next level.</p> <p>In this section we focus solely on the API references of the framework. Here you'll find the classes, methods associated to them and all their attributes.</p> In a Hurry? <p>Looking to jump to something specific? Use the navigation menu on the left \ud83d\udc48!</p> <p>Usability is a big part of our codebase so everything should be easy to understand! \ud83d\ude09</p>"},{"location":"learn/reference/buffer/","title":"velora.buffer","text":"Documentation <p>Customization: Buffers</p> <p>Storage buffers for all algorithms.</p>"},{"location":"learn/reference/buffer/#velora.buffer.BatchExperience","title":"<code>BatchExperience</code>  <code>dataclass</code>","text":"<p>Storage container for a batch agent experiences.</p> <p>Attributes:</p> Name Type Description <code>states</code> <code>torch.Tensor</code> <p>a batch of environment observations</p> <code>actions</code> <code>torch.Tensor</code> <p>a batch of agent actions taken in the states</p> <code>rewards</code> <code>torch.Tensor</code> <p>a batch of rewards obtained for taking the actions</p> <code>next_states</code> <code>torch.Tensor</code> <p>a batch of newly generated environment observations following the actions taken</p> <code>dones</code> <code>torch.Tensor</code> <p>a batch of environment completion statuses</p> <code>hiddens</code> <code>torch.Tensor</code> <p>a batch of prediction network hidden states (e.g., Actor)</p> Source code in <code>velora/buffer/experience.py</code> Python<pre><code>@dataclass\nclass BatchExperience:\n    \"\"\"\n    Storage container for a batch agent experiences.\n\n    Attributes:\n        states (torch.Tensor): a batch of environment observations\n        actions (torch.Tensor): a batch of agent actions taken in the states\n        rewards (torch.Tensor): a batch of rewards obtained for taking the actions\n        next_states (torch.Tensor): a batch of newly generated environment\n            observations following the actions taken\n        dones (torch.Tensor): a batch of environment completion statuses\n        hiddens (torch.Tensor): a batch of prediction network hidden states\n            (e.g., Actor)\n    \"\"\"\n\n    states: torch.Tensor\n    actions: torch.Tensor\n    rewards: torch.Tensor\n    next_states: torch.Tensor\n    dones: torch.Tensor\n    hiddens: torch.Tensor\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase","title":"<code>BufferBase</code>","text":"<p>A base class for all buffers.</p> <p>Stores experiences <code>(states, actions, rewards, next_states, dones)</code> as individual items in tensors.</p> Source code in <code>velora/buffer/base.py</code> Python<pre><code>class BufferBase:\n    \"\"\"\n    A base class for all buffers.\n\n    Stores experiences `(states, actions, rewards, next_states, dones)` as\n    individual items in tensors.\n    \"\"\"\n\n    def __init__(\n        self,\n        capacity: int,\n        state_dim: int,\n        action_dim: int,\n        hidden_dim: int,\n        *,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            capacity (int): the total capacity of the buffer\n            state_dim (int): dimension of state observations\n            action_dim (int): dimension of actions\n            hidden_dim (int): dimension of hidden state\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        self.capacity = capacity\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.hidden_dim = hidden_dim\n        self.device = device\n\n        # Position indicators\n        self.position = 0\n        self.size = 0\n\n        # Pre-allocate storage\n        self.states = torch.zeros((capacity, state_dim), device=device)\n        self.actions = torch.zeros((capacity, action_dim), device=device)\n        self.rewards = torch.zeros((capacity, 1), device=device)\n        self.next_states = torch.zeros((capacity, state_dim), device=device)\n        self.dones = torch.zeros((capacity, 1), device=device)\n        self.hiddens = torch.zeros((capacity, hidden_dim), device=device)\n\n    def add(\n        self,\n        state: torch.Tensor,\n        action: torch.Tensor,\n        reward: float,\n        next_state: torch.Tensor,\n        done: bool,\n        hidden: torch.Tensor,\n    ) -&gt; None:\n        \"\"\"\n        Adds a single experience to the buffer.\n\n        Parameters:\n            state (torch.Tensor): current state observation\n            action (torch.Tensor): action taken\n            reward (float): reward received\n            next_state (torch.Tensor): next state observation\n            done (bool): whether the episode ended\n            hidden (torch.Tensor): Actor hidden state (prediction network)\n        \"\"\"\n        self.states[self.position] = state.to(torch.float32)\n        self.actions[self.position] = action\n        self.rewards[self.position] = reward\n        self.next_states[self.position] = next_state.to(torch.float32)\n        self.dones[self.position] = done\n        self.hiddens[self.position] = hidden\n\n        # Update position - deque style\n        self.position = (self.position + 1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n\n    def add_multi(\n        self,\n        states: torch.Tensor,\n        actions: torch.Tensor,\n        rewards: torch.Tensor,\n        next_states: torch.Tensor,\n        dones: torch.Tensor,\n        hiddens: torch.Tensor,\n    ) -&gt; None:\n        \"\"\"\n        Adds a set of experience to the buffer.\n\n        Parameters:\n            states (torch.Tensor): current state observations\n            actions (torch.Tensor): action takens\n            rewards (torch.Tensor): rewards received\n            next_states (torch.Tensor): next state observations\n            dones (torch.Tensor): whether the episode ended\n            hiddens (torch.Tensor): Actor hidden states (prediction network)\n        \"\"\"\n        batch_size = states.shape[0]\n\n        new_position = self.position + batch_size\n        indices = torch.arange(self.position, new_position) % self.capacity\n\n        dtype = torch.float32\n        rewards = rewards.unsqueeze(-1) if rewards.dim() == 1 else rewards\n        dones = dones.unsqueeze(-1) if dones.dim() == 1 else dones\n        actions = actions.unsqueeze(-1) if actions.dim() == 1 else actions\n\n        self.states[indices] = states.to(dtype)\n        self.actions[indices] = actions.to(dtype)\n        self.rewards[indices] = rewards.to(dtype)\n        self.next_states[indices] = next_states.to(dtype)\n        self.dones[indices] = dones.to(dtype)\n        self.hiddens[indices] = hiddens\n\n        # Update position - deque style\n        self.position = (new_position) % self.capacity\n        self.size = min(self.size + batch_size, self.capacity)\n\n    @abstractmethod\n    def sample(self) -&gt; BatchExperience:\n        \"\"\"\n        Samples experience from the buffer.\n\n        Returns:\n            batch (BatchExperience): an object of samples with the attributes (`states`, `actions`, `rewards`, `next_states`, `dones`, `hidden`).\n\n                All items have the same shape `(batch_size, features)`.\n        \"\"\"\n        pass  # pragma: no cover\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Gets the current size of the buffer.\n\n        Returns:\n            size (int): the current size of the buffer.\n        \"\"\"\n        return self.size\n\n    def metadata(self) -&gt; Dict[MetaDataKeys, Any]:\n        \"\"\"\n        Gets the metadata of the buffer.\n\n        Includes:\n\n        - `capacity` - the maximum capacity of the buffer.\n        - `state_dim` - state dimension.\n        - `action_dim` - action dimension.\n        - `hidden_dim` - hidden state dimension.\n        - `position` - current buffer position.\n        - `size` - current size of buffer.\n        - `device` - the device used for computations.\n\n        Returns:\n            metadata (Dict[str, Any]): the buffers metadata\n        \"\"\"\n        return {\n            \"capacity\": self.capacity,\n            \"state_dim\": self.state_dim,\n            \"action_dim\": self.action_dim,\n            \"hidden_dim\": self.hidden_dim,\n            \"position\": self.position,\n            \"size\": self.size,\n            \"device\": str(self.device) if self.device else None,\n        }\n\n    def state_dict(self) -&gt; Dict[BufferKeys, torch.Tensor]:\n        \"\"\"\n        Return a dictionary containing the buffers state.\n\n        Includes:\n\n        - `states` - tensor of states.\n        - `actions` - tensor of actions.\n        - `rewards` - tensor of rewards.\n        - `next_states` - tensor of next states.\n        - `dones` - tensor of dones.\n        - `hiddens` - tensor of Actor hidden states (prediction network).\n\n        Returns:\n            state_dict (Dict[str, torch.Tensor]): the current state of the buffer\n        \"\"\"\n        return {\n            \"states\": self.states,\n            \"actions\": self.actions,\n            \"rewards\": self.rewards,\n            \"next_states\": self.next_states,\n            \"dones\": self.dones,\n            \"hiddens\": self.hiddens,\n        }\n\n    def save(self, dirpath: str | Path, prefix: str = \"buffer_\") -&gt; None:\n        \"\"\"\n        Saves a buffers `state_dict()` to a `safetensors` file.\n\n        Includes:\n\n        - `&lt;prefix&gt;metadata.json` - the buffers metadata\n        - `&lt;prefix&gt;state.safetensors` - the buffer state\n\n        Parameters:\n            dirpath (str | Path): the folder path to save the buffer state\n            prefix (str, optional): a name prefix for the files\n        \"\"\"\n        save_path = Path(dirpath)\n        save_path.mkdir(parents=True, exist_ok=True)\n\n        metadata_path = Path(save_path, f\"{prefix}metadata\").with_suffix(\".json\")\n        buffer_path = Path(save_path, f\"{prefix}state\").with_suffix(\".safetensors\")\n\n        save_file(self.state_dict(), buffer_path)\n\n        with metadata_path.open(\"w\") as f:\n            f.write(json.dumps(self.metadata(), indent=2))\n\n    @classmethod\n    def load(cls, state_path: str | Path, metadata: Dict[MetaDataKeys, Any]) -&gt; Self:\n        \"\"\"\n        Restores the buffer from a saved state.\n\n        Parameters:\n            state_path (str | Path): the filepath to the buffer state\n            metadata (Dict[str, Any]): a dictionary of metadata already\n                loaded from a `metadata.json` file\n\n        Returns:\n            buffer (Self): a new buffer instance with the saved state restored\n        \"\"\"\n        buffer_path = Path(state_path).with_suffix(\".safetensors\")\n        device = metadata[\"device\"] or \"cpu\"\n\n        # Create new buffer instance\n        buffer = cls(\n            capacity=metadata[\"capacity\"],\n            state_dim=metadata[\"state_dim\"],\n            action_dim=metadata[\"action_dim\"],\n            hidden_dim=metadata[\"hidden_dim\"],\n            device=torch.device(device) if device else None,\n        )\n        buffer.position = metadata[\"position\"]\n        buffer.size = metadata[\"size\"]\n\n        # Load buffer state\n        data: Dict[BufferKeys, torch.Tensor] = load_file(buffer_path, device)\n\n        buffer.states = data[\"states\"]\n        buffer.actions = data[\"actions\"]\n        buffer.rewards = data[\"rewards\"]\n        buffer.next_states = data[\"next_states\"]\n        buffer.dones = data[\"dones\"]\n        buffer.hiddens = data[\"hiddens\"]\n\n        return buffer\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase.__init__","title":"<code>__init__(capacity, state_dim, action_dim, hidden_dim, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>capacity</code> <code>int</code> <p>the total capacity of the buffer</p> required <code>state_dim</code> <code>int</code> <p>dimension of state observations</p> required <code>action_dim</code> <code>int</code> <p>dimension of actions</p> required <code>hidden_dim</code> <code>int</code> <p>dimension of hidden state</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/buffer/base.py</code> Python<pre><code>def __init__(\n    self,\n    capacity: int,\n    state_dim: int,\n    action_dim: int,\n    hidden_dim: int,\n    *,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        capacity (int): the total capacity of the buffer\n        state_dim (int): dimension of state observations\n        action_dim (int): dimension of actions\n        hidden_dim (int): dimension of hidden state\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    self.capacity = capacity\n    self.state_dim = state_dim\n    self.action_dim = action_dim\n    self.hidden_dim = hidden_dim\n    self.device = device\n\n    # Position indicators\n    self.position = 0\n    self.size = 0\n\n    # Pre-allocate storage\n    self.states = torch.zeros((capacity, state_dim), device=device)\n    self.actions = torch.zeros((capacity, action_dim), device=device)\n    self.rewards = torch.zeros((capacity, 1), device=device)\n    self.next_states = torch.zeros((capacity, state_dim), device=device)\n    self.dones = torch.zeros((capacity, 1), device=device)\n    self.hiddens = torch.zeros((capacity, hidden_dim), device=device)\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase.__len__","title":"<code>__len__()</code>","text":"<p>Gets the current size of the buffer.</p> <p>Returns:</p> Name Type Description <code>size</code> <code>int</code> <p>the current size of the buffer.</p> Source code in <code>velora/buffer/base.py</code> Python<pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Gets the current size of the buffer.\n\n    Returns:\n        size (int): the current size of the buffer.\n    \"\"\"\n    return self.size\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase.add","title":"<code>add(state, action, reward, next_state, done, hidden)</code>","text":"<p>Adds a single experience to the buffer.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>torch.Tensor</code> <p>current state observation</p> required <code>action</code> <code>torch.Tensor</code> <p>action taken</p> required <code>reward</code> <code>float</code> <p>reward received</p> required <code>next_state</code> <code>torch.Tensor</code> <p>next state observation</p> required <code>done</code> <code>bool</code> <p>whether the episode ended</p> required <code>hidden</code> <code>torch.Tensor</code> <p>Actor hidden state (prediction network)</p> required Source code in <code>velora/buffer/base.py</code> Python<pre><code>def add(\n    self,\n    state: torch.Tensor,\n    action: torch.Tensor,\n    reward: float,\n    next_state: torch.Tensor,\n    done: bool,\n    hidden: torch.Tensor,\n) -&gt; None:\n    \"\"\"\n    Adds a single experience to the buffer.\n\n    Parameters:\n        state (torch.Tensor): current state observation\n        action (torch.Tensor): action taken\n        reward (float): reward received\n        next_state (torch.Tensor): next state observation\n        done (bool): whether the episode ended\n        hidden (torch.Tensor): Actor hidden state (prediction network)\n    \"\"\"\n    self.states[self.position] = state.to(torch.float32)\n    self.actions[self.position] = action\n    self.rewards[self.position] = reward\n    self.next_states[self.position] = next_state.to(torch.float32)\n    self.dones[self.position] = done\n    self.hiddens[self.position] = hidden\n\n    # Update position - deque style\n    self.position = (self.position + 1) % self.capacity\n    self.size = min(self.size + 1, self.capacity)\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase.add_multi","title":"<code>add_multi(states, actions, rewards, next_states, dones, hiddens)</code>","text":"<p>Adds a set of experience to the buffer.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>torch.Tensor</code> <p>current state observations</p> required <code>actions</code> <code>torch.Tensor</code> <p>action takens</p> required <code>rewards</code> <code>torch.Tensor</code> <p>rewards received</p> required <code>next_states</code> <code>torch.Tensor</code> <p>next state observations</p> required <code>dones</code> <code>torch.Tensor</code> <p>whether the episode ended</p> required <code>hiddens</code> <code>torch.Tensor</code> <p>Actor hidden states (prediction network)</p> required Source code in <code>velora/buffer/base.py</code> Python<pre><code>def add_multi(\n    self,\n    states: torch.Tensor,\n    actions: torch.Tensor,\n    rewards: torch.Tensor,\n    next_states: torch.Tensor,\n    dones: torch.Tensor,\n    hiddens: torch.Tensor,\n) -&gt; None:\n    \"\"\"\n    Adds a set of experience to the buffer.\n\n    Parameters:\n        states (torch.Tensor): current state observations\n        actions (torch.Tensor): action takens\n        rewards (torch.Tensor): rewards received\n        next_states (torch.Tensor): next state observations\n        dones (torch.Tensor): whether the episode ended\n        hiddens (torch.Tensor): Actor hidden states (prediction network)\n    \"\"\"\n    batch_size = states.shape[0]\n\n    new_position = self.position + batch_size\n    indices = torch.arange(self.position, new_position) % self.capacity\n\n    dtype = torch.float32\n    rewards = rewards.unsqueeze(-1) if rewards.dim() == 1 else rewards\n    dones = dones.unsqueeze(-1) if dones.dim() == 1 else dones\n    actions = actions.unsqueeze(-1) if actions.dim() == 1 else actions\n\n    self.states[indices] = states.to(dtype)\n    self.actions[indices] = actions.to(dtype)\n    self.rewards[indices] = rewards.to(dtype)\n    self.next_states[indices] = next_states.to(dtype)\n    self.dones[indices] = dones.to(dtype)\n    self.hiddens[indices] = hiddens\n\n    # Update position - deque style\n    self.position = (new_position) % self.capacity\n    self.size = min(self.size + batch_size, self.capacity)\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase.load","title":"<code>load(state_path, metadata)</code>  <code>classmethod</code>","text":"<p>Restores the buffer from a saved state.</p> <p>Parameters:</p> Name Type Description Default <code>state_path</code> <code>str | Path</code> <p>the filepath to the buffer state</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>a dictionary of metadata already loaded from a <code>metadata.json</code> file</p> required <p>Returns:</p> Name Type Description <code>buffer</code> <code>Self</code> <p>a new buffer instance with the saved state restored</p> Source code in <code>velora/buffer/base.py</code> Python<pre><code>@classmethod\ndef load(cls, state_path: str | Path, metadata: Dict[MetaDataKeys, Any]) -&gt; Self:\n    \"\"\"\n    Restores the buffer from a saved state.\n\n    Parameters:\n        state_path (str | Path): the filepath to the buffer state\n        metadata (Dict[str, Any]): a dictionary of metadata already\n            loaded from a `metadata.json` file\n\n    Returns:\n        buffer (Self): a new buffer instance with the saved state restored\n    \"\"\"\n    buffer_path = Path(state_path).with_suffix(\".safetensors\")\n    device = metadata[\"device\"] or \"cpu\"\n\n    # Create new buffer instance\n    buffer = cls(\n        capacity=metadata[\"capacity\"],\n        state_dim=metadata[\"state_dim\"],\n        action_dim=metadata[\"action_dim\"],\n        hidden_dim=metadata[\"hidden_dim\"],\n        device=torch.device(device) if device else None,\n    )\n    buffer.position = metadata[\"position\"]\n    buffer.size = metadata[\"size\"]\n\n    # Load buffer state\n    data: Dict[BufferKeys, torch.Tensor] = load_file(buffer_path, device)\n\n    buffer.states = data[\"states\"]\n    buffer.actions = data[\"actions\"]\n    buffer.rewards = data[\"rewards\"]\n    buffer.next_states = data[\"next_states\"]\n    buffer.dones = data[\"dones\"]\n    buffer.hiddens = data[\"hiddens\"]\n\n    return buffer\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase.metadata","title":"<code>metadata()</code>","text":"<p>Gets the metadata of the buffer.</p> <p>Includes:</p> <ul> <li><code>capacity</code> - the maximum capacity of the buffer.</li> <li><code>state_dim</code> - state dimension.</li> <li><code>action_dim</code> - action dimension.</li> <li><code>hidden_dim</code> - hidden state dimension.</li> <li><code>position</code> - current buffer position.</li> <li><code>size</code> - current size of buffer.</li> <li><code>device</code> - the device used for computations.</li> </ul> <p>Returns:</p> Name Type Description <code>metadata</code> <code>Dict[str, Any]</code> <p>the buffers metadata</p> Source code in <code>velora/buffer/base.py</code> Python<pre><code>def metadata(self) -&gt; Dict[MetaDataKeys, Any]:\n    \"\"\"\n    Gets the metadata of the buffer.\n\n    Includes:\n\n    - `capacity` - the maximum capacity of the buffer.\n    - `state_dim` - state dimension.\n    - `action_dim` - action dimension.\n    - `hidden_dim` - hidden state dimension.\n    - `position` - current buffer position.\n    - `size` - current size of buffer.\n    - `device` - the device used for computations.\n\n    Returns:\n        metadata (Dict[str, Any]): the buffers metadata\n    \"\"\"\n    return {\n        \"capacity\": self.capacity,\n        \"state_dim\": self.state_dim,\n        \"action_dim\": self.action_dim,\n        \"hidden_dim\": self.hidden_dim,\n        \"position\": self.position,\n        \"size\": self.size,\n        \"device\": str(self.device) if self.device else None,\n    }\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase.sample","title":"<code>sample()</code>  <code>abstractmethod</code>","text":"<p>Samples experience from the buffer.</p> <p>Returns:</p> Name Type Description <code>batch</code> <code>BatchExperience</code> <p>an object of samples with the attributes (<code>states</code>, <code>actions</code>, <code>rewards</code>, <code>next_states</code>, <code>dones</code>, <code>hidden</code>).</p> <p>All items have the same shape <code>(batch_size, features)</code>.</p> Source code in <code>velora/buffer/base.py</code> Python<pre><code>@abstractmethod\ndef sample(self) -&gt; BatchExperience:\n    \"\"\"\n    Samples experience from the buffer.\n\n    Returns:\n        batch (BatchExperience): an object of samples with the attributes (`states`, `actions`, `rewards`, `next_states`, `dones`, `hidden`).\n\n            All items have the same shape `(batch_size, features)`.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase.save","title":"<code>save(dirpath, prefix='buffer_')</code>","text":"<p>Saves a buffers <code>state_dict()</code> to a <code>safetensors</code> file.</p> <p>Includes:</p> <ul> <li><code>&lt;prefix&gt;metadata.json</code> - the buffers metadata</li> <li><code>&lt;prefix&gt;state.safetensors</code> - the buffer state</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dirpath</code> <code>str | Path</code> <p>the folder path to save the buffer state</p> required <code>prefix</code> <code>str</code> <p>a name prefix for the files</p> <code>'buffer_'</code> Source code in <code>velora/buffer/base.py</code> Python<pre><code>def save(self, dirpath: str | Path, prefix: str = \"buffer_\") -&gt; None:\n    \"\"\"\n    Saves a buffers `state_dict()` to a `safetensors` file.\n\n    Includes:\n\n    - `&lt;prefix&gt;metadata.json` - the buffers metadata\n    - `&lt;prefix&gt;state.safetensors` - the buffer state\n\n    Parameters:\n        dirpath (str | Path): the folder path to save the buffer state\n        prefix (str, optional): a name prefix for the files\n    \"\"\"\n    save_path = Path(dirpath)\n    save_path.mkdir(parents=True, exist_ok=True)\n\n    metadata_path = Path(save_path, f\"{prefix}metadata\").with_suffix(\".json\")\n    buffer_path = Path(save_path, f\"{prefix}state\").with_suffix(\".safetensors\")\n\n    save_file(self.state_dict(), buffer_path)\n\n    with metadata_path.open(\"w\") as f:\n        f.write(json.dumps(self.metadata(), indent=2))\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.BufferBase.state_dict","title":"<code>state_dict()</code>","text":"<p>Return a dictionary containing the buffers state.</p> <p>Includes:</p> <ul> <li><code>states</code> - tensor of states.</li> <li><code>actions</code> - tensor of actions.</li> <li><code>rewards</code> - tensor of rewards.</li> <li><code>next_states</code> - tensor of next states.</li> <li><code>dones</code> - tensor of dones.</li> <li><code>hiddens</code> - tensor of Actor hidden states (prediction network).</li> </ul> <p>Returns:</p> Name Type Description <code>state_dict</code> <code>Dict[str, torch.Tensor]</code> <p>the current state of the buffer</p> Source code in <code>velora/buffer/base.py</code> Python<pre><code>def state_dict(self) -&gt; Dict[BufferKeys, torch.Tensor]:\n    \"\"\"\n    Return a dictionary containing the buffers state.\n\n    Includes:\n\n    - `states` - tensor of states.\n    - `actions` - tensor of actions.\n    - `rewards` - tensor of rewards.\n    - `next_states` - tensor of next states.\n    - `dones` - tensor of dones.\n    - `hiddens` - tensor of Actor hidden states (prediction network).\n\n    Returns:\n        state_dict (Dict[str, torch.Tensor]): the current state of the buffer\n    \"\"\"\n    return {\n        \"states\": self.states,\n        \"actions\": self.actions,\n        \"rewards\": self.rewards,\n        \"next_states\": self.next_states,\n        \"dones\": self.dones,\n        \"hiddens\": self.hiddens,\n    }\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.ReplayBuffer","title":"<code>ReplayBuffer</code>","text":"<p>               Bases: <code>BufferBase</code></p> <p>A Buffer for storing agent experiences. Used for Off-Policy agents.</p> <p>First introduced in Deep RL in the Deep Q-Network paper: Player Atari with Deep Reinforcement Learning.</p> Source code in <code>velora/buffer/replay.py</code> Python<pre><code>class ReplayBuffer(BufferBase):\n    \"\"\"\n    A Buffer for storing agent experiences. Used for Off-Policy agents.\n\n    First introduced in Deep RL in the Deep Q-Network paper:\n    [Player Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602).\n    \"\"\"\n\n    def __init__(\n        self,\n        capacity: int,\n        state_dim: int,\n        action_dim: int,\n        hidden_dim: int,\n        *,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            capacity (int): the total capacity of the buffer\n            state_dim (int): dimension of state observations\n            action_dim (int): dimension of actions\n            hidden_dim (int): dimension of hidden state\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__(capacity, state_dim, action_dim, hidden_dim, device=device)\n\n    def config(self) -&gt; BufferConfig:\n        \"\"\"\n        Creates a buffer config model.\n\n        Returns:\n            config (BufferConfig): a config model with buffer details.\n        \"\"\"\n        return BufferConfig(\n            type=\"ReplayBuffer\",\n            capacity=self.capacity,\n            state_dim=self.state_dim,\n            action_dim=self.action_dim,\n            hidden_dim=self.hidden_dim,\n        )\n\n    @override\n    def sample(self, batch_size: int) -&gt; BatchExperience:\n        \"\"\"\n        Samples a random batch of experiences from the buffer.\n\n        Parameters:\n            batch_size (int): the number of items to sample\n\n        Returns:\n            batch (BatchExperience): an object of samples with the attributes (`states`, `actions`, `rewards`, `next_states`, `dones`, `hidden`).\n\n                All items have the same shape `(batch_size, features)`.\n        \"\"\"\n        if len(self) &lt; batch_size:\n            raise ValueError(\n                f\"Buffer does not contain enough experiences. Available: {len(self)}, Requested: {batch_size}\"\n            )\n\n        indices = torch.randint(0, self.size, (batch_size,), device=self.device)\n\n        return BatchExperience(\n            states=self.states[indices],\n            actions=self.actions[indices],\n            rewards=self.rewards[indices],\n            next_states=self.next_states[indices],\n            dones=self.dones[indices],\n            hiddens=self.hiddens[indices],\n        )\n\n    def warm(self, agent: \"RLModuleAgent\", n_samples: int, num_envs: int = 8) -&gt; None:\n        \"\"\"\n        Warms the buffer to fill it to a number of samples by generating them\n        from an agent using a `vectorized` copy of the environment.\n\n        Parameters:\n            agent (Any): the agent to generate samples with\n            n_samples (int): the maximum number of samples to generate\n            num_envs (int, optional): number of vectorized environments. Cannot\n                be smaller than `2`\n        \"\"\"\n        if num_envs &lt; 2:\n            raise ValueError(f\"'{num_envs=}' cannot be smaller than 2.\")\n\n        envs = gym.make_vec(\n            agent.env.spec.id,\n            num_envs=num_envs,\n            vectorization_mode=\"sync\",\n        )\n        envs: gym.vector.SyncVectorEnv = gym.wrappers.vector.NumpyToTorch(\n            envs, agent.device\n        )\n\n        hidden = None\n        states, _ = envs.reset()\n\n        while not len(self) &gt;= n_samples:\n            actions, hidden = agent.predict(states, hidden, train_mode=True)\n            next_states, rewards, terminated, truncated, _ = envs.step(actions)\n            dones = terminated | truncated\n\n            self.add_multi(states, actions, rewards, next_states, dones, hidden)\n\n            states = next_states\n\n        envs.close()\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.ReplayBuffer.__init__","title":"<code>__init__(capacity, state_dim, action_dim, hidden_dim, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>capacity</code> <code>int</code> <p>the total capacity of the buffer</p> required <code>state_dim</code> <code>int</code> <p>dimension of state observations</p> required <code>action_dim</code> <code>int</code> <p>dimension of actions</p> required <code>hidden_dim</code> <code>int</code> <p>dimension of hidden state</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/buffer/replay.py</code> Python<pre><code>def __init__(\n    self,\n    capacity: int,\n    state_dim: int,\n    action_dim: int,\n    hidden_dim: int,\n    *,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        capacity (int): the total capacity of the buffer\n        state_dim (int): dimension of state observations\n        action_dim (int): dimension of actions\n        hidden_dim (int): dimension of hidden state\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__(capacity, state_dim, action_dim, hidden_dim, device=device)\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.ReplayBuffer.config","title":"<code>config()</code>","text":"<p>Creates a buffer config model.</p> <p>Returns:</p> Name Type Description <code>config</code> <code>BufferConfig</code> <p>a config model with buffer details.</p> Source code in <code>velora/buffer/replay.py</code> Python<pre><code>def config(self) -&gt; BufferConfig:\n    \"\"\"\n    Creates a buffer config model.\n\n    Returns:\n        config (BufferConfig): a config model with buffer details.\n    \"\"\"\n    return BufferConfig(\n        type=\"ReplayBuffer\",\n        capacity=self.capacity,\n        state_dim=self.state_dim,\n        action_dim=self.action_dim,\n        hidden_dim=self.hidden_dim,\n    )\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.ReplayBuffer.sample","title":"<code>sample(batch_size)</code>","text":"<p>Samples a random batch of experiences from the buffer.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of items to sample</p> required <p>Returns:</p> Name Type Description <code>batch</code> <code>BatchExperience</code> <p>an object of samples with the attributes (<code>states</code>, <code>actions</code>, <code>rewards</code>, <code>next_states</code>, <code>dones</code>, <code>hidden</code>).</p> <p>All items have the same shape <code>(batch_size, features)</code>.</p> Source code in <code>velora/buffer/replay.py</code> Python<pre><code>@override\ndef sample(self, batch_size: int) -&gt; BatchExperience:\n    \"\"\"\n    Samples a random batch of experiences from the buffer.\n\n    Parameters:\n        batch_size (int): the number of items to sample\n\n    Returns:\n        batch (BatchExperience): an object of samples with the attributes (`states`, `actions`, `rewards`, `next_states`, `dones`, `hidden`).\n\n            All items have the same shape `(batch_size, features)`.\n    \"\"\"\n    if len(self) &lt; batch_size:\n        raise ValueError(\n            f\"Buffer does not contain enough experiences. Available: {len(self)}, Requested: {batch_size}\"\n        )\n\n    indices = torch.randint(0, self.size, (batch_size,), device=self.device)\n\n    return BatchExperience(\n        states=self.states[indices],\n        actions=self.actions[indices],\n        rewards=self.rewards[indices],\n        next_states=self.next_states[indices],\n        dones=self.dones[indices],\n        hiddens=self.hiddens[indices],\n    )\n</code></pre>"},{"location":"learn/reference/buffer/#velora.buffer.ReplayBuffer.warm","title":"<code>warm(agent, n_samples, num_envs=8)</code>","text":"<p>Warms the buffer to fill it to a number of samples by generating them from an agent using a <code>vectorized</code> copy of the environment.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Any</code> <p>the agent to generate samples with</p> required <code>n_samples</code> <code>int</code> <p>the maximum number of samples to generate</p> required <code>num_envs</code> <code>int</code> <p>number of vectorized environments. Cannot be smaller than <code>2</code></p> <code>8</code> Source code in <code>velora/buffer/replay.py</code> Python<pre><code>def warm(self, agent: \"RLModuleAgent\", n_samples: int, num_envs: int = 8) -&gt; None:\n    \"\"\"\n    Warms the buffer to fill it to a number of samples by generating them\n    from an agent using a `vectorized` copy of the environment.\n\n    Parameters:\n        agent (Any): the agent to generate samples with\n        n_samples (int): the maximum number of samples to generate\n        num_envs (int, optional): number of vectorized environments. Cannot\n            be smaller than `2`\n    \"\"\"\n    if num_envs &lt; 2:\n        raise ValueError(f\"'{num_envs=}' cannot be smaller than 2.\")\n\n    envs = gym.make_vec(\n        agent.env.spec.id,\n        num_envs=num_envs,\n        vectorization_mode=\"sync\",\n    )\n    envs: gym.vector.SyncVectorEnv = gym.wrappers.vector.NumpyToTorch(\n        envs, agent.device\n    )\n\n    hidden = None\n    states, _ = envs.reset()\n\n    while not len(self) &gt;= n_samples:\n        actions, hidden = agent.predict(states, hidden, train_mode=True)\n        next_states, rewards, terminated, truncated, _ = envs.step(actions)\n        dones = terminated | truncated\n\n        self.add_multi(states, actions, rewards, next_states, dones, hidden)\n\n        states = next_states\n\n    envs.close()\n</code></pre>"},{"location":"learn/reference/callbacks/","title":"velora.callbacks","text":"Documentation <p>User Guide - Tutorials: Callbacks</p> <p>Methods that provide extra functionality for agent training.</p>"},{"location":"learn/reference/callbacks/#velora.callbacks.CometAnalytics","title":"<code>CometAnalytics</code>","text":"<p>               Bases: <code>TrainCallback</code></p> <p>A callback that enables <code>comet-ml</code> cloud-based analytics tracking.</p> <p>Requires Comet ML API key set using the <code>COMET_API_KEY</code> environment variable.</p> <p>Features:</p> <ul> <li>Upload agent configuration objects</li> <li>Tracks episodic training metrics</li> <li>Uploads video recordings (if <code>RecordVideos</code> callback applied)</li> </ul> Source code in <code>velora/callbacks.py</code> Python<pre><code>class CometAnalytics(TrainCallback):\n    \"\"\"\n    A callback that enables [`comet-ml`](https://www.comet.com/site/) cloud-based\n    analytics tracking.\n\n    Requires Comet ML API key set using the `COMET_API_KEY` environment variable.\n\n    Features:\n\n    - Upload agent configuration objects\n    - Tracks episodic training metrics\n    - Uploads video recordings (if `RecordVideos` callback applied)\n    \"\"\"\n\n    @override\n    def __init__(\n        self,\n        project_name: str,\n        experiment_name: str | None = None,\n        *,\n        tags: List[str] | None = None,\n        experiment_key: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            project_name (str): the name of the Comet ML project to add this\n                experiment to\n            experiment_name (str, optional): the name of this experiment run.\n                If `None`, automatically creates the name using the format\n                `&lt;agent_classname&gt;_&lt;env_name&gt;_&lt;n_episodes&gt;ep`. Ignored when\n                using `experiment_key`\n            tags (List[str], optional): a list of tags associated with the\n                experiment. If `None` adds the `agent_classname` and `env_name`\n                by default. Ignored when using `experiment_key`\n            experiment_key (str, optional): an existing Comet ML experiment key.\n                Used for continuing an experiment\n        \"\"\"\n        try:\n            from comet_ml import Experiment\n        except ImportError:\n            raise ImportError(\n                \"Failed to load the 'comet_ml' package. Have you installed it using 'pip install velora[comet]'?\"\n            )\n\n        api_key = os.getenv(\"COMET_API_KEY\", None)\n        if api_key is None or api_key == \"\":\n            raise ValueError(\n                \"Missing 'api_key'! Store it as a 'COMET_API_KEY' environment variable.\"\n            )\n\n        self.experiment: Experiment | None = None\n        self.experiment_key = experiment_key\n\n        self.state = AnalyticsState(\n            project_name=project_name,\n            experiment_name=experiment_name,\n            tags=tags,\n        )\n\n        self.project_name = project_name\n        self.experiment_name = experiment_name if experiment_name else \"auto\"\n        self.tags = tags if tags else \"auto\"\n\n    def __call__(self, state: \"TrainState\") -&gt; \"TrainState\":\n        # Setup experiment\n        if state.status == \"start\":\n            # Update comet training state\n            state.analytics_state = self.state\n            state.analytics_update()\n\n            self.init_experiment(state)\n\n            # Log config\n            self.experiment.log_parameters(state.agent.config.model_dump())\n\n        # Send episodic metrics\n        if state.status == \"logging\":\n            if state.logging_type == \"episode\":\n                self.log_episode_data(state)\n\n        # Finalize training\n        if state.status == \"complete\":\n            # Log video recordings\n            if state.record_state is not None:\n                for video in state.record_state.dirpath.iterdir():\n                    self.experiment.log_video(str(video), format=\"mp4\")\n\n            self.experiment.end()\n\n        return state\n\n    def log_episode_data(self, state: \"TrainState\") -&gt; None:\n        \"\"\"\n        Logs episodic data to the experiment.\n\n        Parameters:\n            state (TrainState): the current training state\n        \"\"\"\n        results = get_current_episode(\n            state.session,\n            state.experiment_id,\n            state.current_ep,\n        )\n\n        for item in results:\n            reward_low = item.reward_moving_avg - item.reward_moving_std\n            reward_high = item.reward_moving_avg + item.reward_moving_std\n\n            self.experiment.log_metrics(\n                {\n                    \"reward/moving_lower\": reward_low,\n                    \"reward/moving_avg\": item.reward_moving_avg,\n                    \"reward/moving_upper\": reward_high,\n                    \"episode/return\": item.reward,\n                    \"episode/length\": item.length,\n                    \"losses/actor\": item.actor_loss,\n                    \"losses/critic\": item.critic_loss,\n                    \"losses/entropy\": item.entropy_loss,\n                },\n                epoch=state.current_ep,\n            )\n\n    def init_experiment(self, state: \"TrainState\") -&gt; None:\n        \"\"\"Setups up a comet experiment and stores it locally.\n\n        Parameters:\n            state (TrainState): the current training state\n        \"\"\"\n        from comet_ml import ExistingExperiment, Experiment\n\n        if self.experiment_key is not None:\n            # Continue existing experiment\n            self.experiment = ExistingExperiment(\n                api_key=os.getenv(\"COMET_API_KEY\", None),\n                project_name=state.analytics_state.project_name,\n                experiment_key=self.experiment_key,\n                disabled=bool(os.getenv(\"VELORA_TEST_MODE\", \"\").lower())\n                in (\"true\", \"1\"),\n                log_env_cpu=True,\n                log_env_gpu=True,\n                log_env_details=True,\n            )\n        else:\n            # Start new experiment\n            self.experiment = Experiment(\n                api_key=os.getenv(\"COMET_API_KEY\", None),\n                project_name=state.analytics_state.project_name,\n                auto_param_logging=False,\n                auto_metric_logging=False,\n                auto_output_logging=False,\n                log_graph=False,\n                display_summary_level=0,\n                disabled=bool(os.getenv(\"VELORA_TEST_MODE\", \"\").lower())\n                in (\"true\", \"1\"),\n            )\n\n            self.experiment.set_name(state.analytics_state.experiment_name)\n            self.experiment.add_tags(state.analytics_state.tags)\n\n    def config(self) -&gt; Tuple[str, Dict[str, Any]]:\n        return self.__class__.__name__, {\n            \"project_name\": self.project_name,\n            \"experiment_name\": self.experiment_name,\n            \"tags\": \",\".join(self.tags) if isinstance(self.tags, list) else self.tags,\n            \"experiment_key\": self.experiment_key,\n        }\n\n    def info(self) -&gt; str:\n        return f\"'{self.__class__.__name__}' enabled with 'project_name={self.project_name}', 'experiment_name={self.experiment_name}' and 'tags={self.tags}'.\"\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.CometAnalytics.__init__","title":"<code>__init__(project_name, experiment_name=None, *, tags=None, experiment_key=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>the name of the Comet ML project to add this experiment to</p> required <code>experiment_name</code> <code>str</code> <p>the name of this experiment run. If <code>None</code>, automatically creates the name using the format <code>&lt;agent_classname&gt;_&lt;env_name&gt;_&lt;n_episodes&gt;ep</code>. Ignored when using <code>experiment_key</code></p> <code>None</code> <code>tags</code> <code>List[str]</code> <p>a list of tags associated with the experiment. If <code>None</code> adds the <code>agent_classname</code> and <code>env_name</code> by default. Ignored when using <code>experiment_key</code></p> <code>None</code> <code>experiment_key</code> <code>str</code> <p>an existing Comet ML experiment key. Used for continuing an experiment</p> <code>None</code> Source code in <code>velora/callbacks.py</code> Python<pre><code>@override\ndef __init__(\n    self,\n    project_name: str,\n    experiment_name: str | None = None,\n    *,\n    tags: List[str] | None = None,\n    experiment_key: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        project_name (str): the name of the Comet ML project to add this\n            experiment to\n        experiment_name (str, optional): the name of this experiment run.\n            If `None`, automatically creates the name using the format\n            `&lt;agent_classname&gt;_&lt;env_name&gt;_&lt;n_episodes&gt;ep`. Ignored when\n            using `experiment_key`\n        tags (List[str], optional): a list of tags associated with the\n            experiment. If `None` adds the `agent_classname` and `env_name`\n            by default. Ignored when using `experiment_key`\n        experiment_key (str, optional): an existing Comet ML experiment key.\n            Used for continuing an experiment\n    \"\"\"\n    try:\n        from comet_ml import Experiment\n    except ImportError:\n        raise ImportError(\n            \"Failed to load the 'comet_ml' package. Have you installed it using 'pip install velora[comet]'?\"\n        )\n\n    api_key = os.getenv(\"COMET_API_KEY\", None)\n    if api_key is None or api_key == \"\":\n        raise ValueError(\n            \"Missing 'api_key'! Store it as a 'COMET_API_KEY' environment variable.\"\n        )\n\n    self.experiment: Experiment | None = None\n    self.experiment_key = experiment_key\n\n    self.state = AnalyticsState(\n        project_name=project_name,\n        experiment_name=experiment_name,\n        tags=tags,\n    )\n\n    self.project_name = project_name\n    self.experiment_name = experiment_name if experiment_name else \"auto\"\n    self.tags = tags if tags else \"auto\"\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.CometAnalytics.init_experiment","title":"<code>init_experiment(state)</code>","text":"<p>Setups up a comet experiment and stores it locally.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainState</code> <p>the current training state</p> required Source code in <code>velora/callbacks.py</code> Python<pre><code>def init_experiment(self, state: \"TrainState\") -&gt; None:\n    \"\"\"Setups up a comet experiment and stores it locally.\n\n    Parameters:\n        state (TrainState): the current training state\n    \"\"\"\n    from comet_ml import ExistingExperiment, Experiment\n\n    if self.experiment_key is not None:\n        # Continue existing experiment\n        self.experiment = ExistingExperiment(\n            api_key=os.getenv(\"COMET_API_KEY\", None),\n            project_name=state.analytics_state.project_name,\n            experiment_key=self.experiment_key,\n            disabled=bool(os.getenv(\"VELORA_TEST_MODE\", \"\").lower())\n            in (\"true\", \"1\"),\n            log_env_cpu=True,\n            log_env_gpu=True,\n            log_env_details=True,\n        )\n    else:\n        # Start new experiment\n        self.experiment = Experiment(\n            api_key=os.getenv(\"COMET_API_KEY\", None),\n            project_name=state.analytics_state.project_name,\n            auto_param_logging=False,\n            auto_metric_logging=False,\n            auto_output_logging=False,\n            log_graph=False,\n            display_summary_level=0,\n            disabled=bool(os.getenv(\"VELORA_TEST_MODE\", \"\").lower())\n            in (\"true\", \"1\"),\n        )\n\n        self.experiment.set_name(state.analytics_state.experiment_name)\n        self.experiment.add_tags(state.analytics_state.tags)\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.CometAnalytics.log_episode_data","title":"<code>log_episode_data(state)</code>","text":"<p>Logs episodic data to the experiment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainState</code> <p>the current training state</p> required Source code in <code>velora/callbacks.py</code> Python<pre><code>def log_episode_data(self, state: \"TrainState\") -&gt; None:\n    \"\"\"\n    Logs episodic data to the experiment.\n\n    Parameters:\n        state (TrainState): the current training state\n    \"\"\"\n    results = get_current_episode(\n        state.session,\n        state.experiment_id,\n        state.current_ep,\n    )\n\n    for item in results:\n        reward_low = item.reward_moving_avg - item.reward_moving_std\n        reward_high = item.reward_moving_avg + item.reward_moving_std\n\n        self.experiment.log_metrics(\n            {\n                \"reward/moving_lower\": reward_low,\n                \"reward/moving_avg\": item.reward_moving_avg,\n                \"reward/moving_upper\": reward_high,\n                \"episode/return\": item.reward,\n                \"episode/length\": item.length,\n                \"losses/actor\": item.actor_loss,\n                \"losses/critic\": item.critic_loss,\n                \"losses/entropy\": item.entropy_loss,\n            },\n            epoch=state.current_ep,\n        )\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.EarlyStopping","title":"<code>EarlyStopping</code>","text":"<p>               Bases: <code>TrainCallback</code></p> <p>A callback that applies early stopping to the training process.</p> Source code in <code>velora/callbacks.py</code> Python<pre><code>class EarlyStopping(TrainCallback):\n    \"\"\"\n    A callback that applies early stopping to the training process.\n    \"\"\"\n\n    @override\n    def __init__(self, target: float, *, patience: int = 3) -&gt; None:\n        \"\"\"\n        Parameters:\n            target (float): episode reward target to achieve\n            patience (int, optional): number of times the threshold needs\n                to be met to terminate training\n        \"\"\"\n        self.target = target\n        self.patience = patience\n\n        self.count = 0\n\n    def __call__(self, state: \"TrainState\") -&gt; \"TrainState\":\n        if state.stop_training:\n            return state\n\n        if state.status == \"episode\":\n            if state.ep_reward &gt;= self.target:\n                self.count += 1\n\n                if self.count &gt;= self.patience:\n                    state.stop_training = True\n            else:\n                self.count = 0\n\n        return state\n\n    def config(self) -&gt; Tuple[str, Dict[str, Any]]:\n        return self.__class__.__name__, {\n            \"target\": self.target,\n            \"patience\": self.patience,\n        }\n\n    def info(self) -&gt; str:\n        return f\"'{self.__class__.__name__}' enabled with 'target={self.target}' and 'patience={self.patience}'.\"\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.EarlyStopping.__init__","title":"<code>__init__(target, *, patience=3)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>target</code> <code>float</code> <p>episode reward target to achieve</p> required <code>patience</code> <code>int</code> <p>number of times the threshold needs to be met to terminate training</p> <code>3</code> Source code in <code>velora/callbacks.py</code> Python<pre><code>@override\ndef __init__(self, target: float, *, patience: int = 3) -&gt; None:\n    \"\"\"\n    Parameters:\n        target (float): episode reward target to achieve\n        patience (int, optional): number of times the threshold needs\n            to be met to terminate training\n    \"\"\"\n    self.target = target\n    self.patience = patience\n\n    self.count = 0\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.RecordVideos","title":"<code>RecordVideos</code>","text":"<p>               Bases: <code>TrainCallback</code></p> <p>A callback to enable intermittent environment video recording to visualize the agent training progress.</p> <p>Requires environment with <code>render_mode=\"rgb_array\"</code>.</p> Source code in <code>velora/callbacks.py</code> Python<pre><code>class RecordVideos(TrainCallback):\n    \"\"\"\n    A callback to enable intermittent environment video recording to visualize\n    the agent training progress.\n\n    Requires environment with `render_mode=\"rgb_array\"`.\n    \"\"\"\n\n    @override\n    def __init__(\n        self,\n        dirname: str,\n        *,\n        method: RecordMethodLiteral = \"episode\",\n        frequency: int = 100,\n        force: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            dirname (str): the model directory name to store the videos.\n                Automatically created in `checkpoints` directory as\n                `checkpoints/&lt;dirname&gt;/videos`.\n\n                Compliments `TrainCallback.SaveCheckpoints` callback.\n\n            method (Literal[\"episode\", \"step\"], optional): the recording method.\n                When `episode` records episodically. When `step` records during\n                training steps.\n            frequency (int, optional): the `episode` or `step` record frequency\n            force (bool, optional): enables file overwriting, ignoring existing\n                video files. Useful for continuing model training\n        \"\"\"\n        if method not in get_args(RecordMethodLiteral):\n            raise ValueError(\n                f\"'{method=}' is not supported. Choices: '{get_args(RecordMethodLiteral)}'\"\n            )\n\n        self.dirname = dirname\n        self.method = method\n        self.frequency = frequency\n        self.force = force\n\n        self.dirpath = Path(\"checkpoints\", dirname, \"videos\")\n\n        if not force and self.dirpath.exists():\n            raise FileExistsError(\n                f\"Files already exist in the '{self.dirpath.parent}' directory! Either change the 'dirname', delete/move the folder and its contents, or use 'force=True' to allow overwriting.\"\n            )\n\n        def trigger(t: int) -&gt; bool:\n            # Skip first item\n            if t == 0:\n                return False\n\n            return t % frequency == 0\n\n        self.details = RecordState(\n            dirpath=self.dirpath,\n            method=method,\n            episode_trigger=trigger if method == \"episode\" else None,\n            step_trigger=trigger if method == \"step\" else None,\n        )\n\n    def __call__(self, state: \"TrainState\") -&gt; \"TrainState\":\n        # 'start': Set the recording state\n        if state.status == \"start\":\n            state.record_state = self.details\n\n            state.env = gym.wrappers.RecordVideo(\n                state.env,\n                name_prefix=state.env.spec.name,\n                **state.record_state.to_wrapper(),\n            )\n\n        # Ignore other events\n        return state\n\n    def config(self) -&gt; Tuple[str, Dict[str, Any]]:\n        return self.__class__.__name__, {\n            \"dirname\": self.dirname,\n            \"method\": self.method,\n            \"frequency\": self.frequency,\n            \"force\": self.force,\n        }\n\n    def info(self) -&gt; str:\n        return (\n            f\"'{self.__class__.__name__}' enabled with 'method={str(self.method)}' and 'frequency={self.frequency}'.\\n\"\n            f\"    Files will be saved in the 'checkpoints/{self.dirname}/videos' directory.\"\n        )\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.RecordVideos.__init__","title":"<code>__init__(dirname, *, method='episode', frequency=100, force=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dirname</code> <code>str</code> <p>the model directory name to store the videos. Automatically created in <code>checkpoints</code> directory as <code>checkpoints/&lt;dirname&gt;/videos</code>.</p> <p>Compliments <code>TrainCallback.SaveCheckpoints</code> callback.</p> required <code>method</code> <code>Literal[episode, step]</code> <p>the recording method. When <code>episode</code> records episodically. When <code>step</code> records during training steps.</p> <code>'episode'</code> <code>frequency</code> <code>int</code> <p>the <code>episode</code> or <code>step</code> record frequency</p> <code>100</code> <code>force</code> <code>bool</code> <p>enables file overwriting, ignoring existing video files. Useful for continuing model training</p> <code>False</code> Source code in <code>velora/callbacks.py</code> Python<pre><code>@override\ndef __init__(\n    self,\n    dirname: str,\n    *,\n    method: RecordMethodLiteral = \"episode\",\n    frequency: int = 100,\n    force: bool = False,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        dirname (str): the model directory name to store the videos.\n            Automatically created in `checkpoints` directory as\n            `checkpoints/&lt;dirname&gt;/videos`.\n\n            Compliments `TrainCallback.SaveCheckpoints` callback.\n\n        method (Literal[\"episode\", \"step\"], optional): the recording method.\n            When `episode` records episodically. When `step` records during\n            training steps.\n        frequency (int, optional): the `episode` or `step` record frequency\n        force (bool, optional): enables file overwriting, ignoring existing\n            video files. Useful for continuing model training\n    \"\"\"\n    if method not in get_args(RecordMethodLiteral):\n        raise ValueError(\n            f\"'{method=}' is not supported. Choices: '{get_args(RecordMethodLiteral)}'\"\n        )\n\n    self.dirname = dirname\n    self.method = method\n    self.frequency = frequency\n    self.force = force\n\n    self.dirpath = Path(\"checkpoints\", dirname, \"videos\")\n\n    if not force and self.dirpath.exists():\n        raise FileExistsError(\n            f\"Files already exist in the '{self.dirpath.parent}' directory! Either change the 'dirname', delete/move the folder and its contents, or use 'force=True' to allow overwriting.\"\n        )\n\n    def trigger(t: int) -&gt; bool:\n        # Skip first item\n        if t == 0:\n            return False\n\n        return t % frequency == 0\n\n    self.details = RecordState(\n        dirpath=self.dirpath,\n        method=method,\n        episode_trigger=trigger if method == \"episode\" else None,\n        step_trigger=trigger if method == \"step\" else None,\n    )\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.SaveCheckpoints","title":"<code>SaveCheckpoints</code>","text":"<p>               Bases: <code>TrainCallback</code></p> <p>A callback that applies model state saving checkpoints to the training process.</p> Source code in <code>velora/callbacks.py</code> Python<pre><code>class SaveCheckpoints(TrainCallback):\n    \"\"\"\n    A callback that applies model state saving checkpoints to the training process.\n    \"\"\"\n\n    @override\n    def __init__(\n        self,\n        dirname: str,\n        *,\n        frequency: int = 100,\n        buffer: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            dirname (str): the model directory name to save checkpoints.\n                Automatically created inside `checkpoints` directory as\n                `checkpoints/&lt;dirname&gt;/saves`.\n\n                Compliments `TrainCallback.RecordVideos` callback.\n\n            frequency (int, optional): save frequency (in episodes or steps)\n            buffer (bool, optional): whether to save the buffer state\n        \"\"\"\n        self.dirname = dirname\n        self.frequency = frequency\n        self.buffer = buffer\n\n        self.filepath = Path(\"checkpoints\", self.dirname, \"saves\")\n\n        if self.filepath.exists():\n            raise FileExistsError(\n                f\"Items already exist in the '{self.filepath.parent}' directory! Either change the 'dirname' or delete the folders contents.\"\n            )\n\n    def __call__(self, state: \"TrainState\") -&gt; \"TrainState\":\n        if state.status == \"start\":\n            state.saving_enabled = True\n            state.checkpoint_dir = self.filepath\n\n        # Only perform checkpoint operations on episode and complete events\n        if state.status != \"episode\" and state.status != \"complete\":\n            return state\n\n        # Create directory if it doesn't exist on first call\n        self.filepath.mkdir(parents=True, exist_ok=True)\n\n        ep_idx = state.current_ep\n\n        should_save = False\n        filename = f\"{state.env.spec.name}_\"\n\n        # Save checkpoint at specified frequency\n        if state.status == \"episode\" and ep_idx != state.total_episodes:\n            if ep_idx % self.frequency == 0:\n                should_save = True\n                filename += f\"{number_to_short(ep_idx)}\"\n\n        # Perform final checkpoint save\n        elif state.status == \"complete\":\n            should_save = True\n            filename += \"final\"\n\n        if should_save:\n            self.save_checkpoint(state.agent, filename)\n\n        return state\n\n    def save_checkpoint(self, agent: \"RLModuleAgent\", dirname: str) -&gt; None:\n        \"\"\"\n        Saves a checkpoint at a given episode with the given suffix.\n\n        Parameters:\n            agent (RLModuleAgent): the agent being trained\n            dirname (str): the checkpoint directory name\n        \"\"\"\n        checkpoint_path = Path(self.filepath, dirname)\n        agent.save(checkpoint_path, buffer=self.buffer, config=True)\n\n    def config(self) -&gt; Tuple[str, Dict[str, Any]]:\n        return self.__class__.__name__, {\n            \"dirname\": self.dirname,\n            \"frequency\": self.frequency,\n            \"buffer\": self.buffer,\n        }\n\n    def info(self) -&gt; str:\n        return (\n            f\"'{self.__class__.__name__}' enabled with 'frequency={self.frequency}' and 'buffer={self.buffer}'.\\n\"\n            f\"    Files will be saved in the 'checkpoints/{self.dirname}/saves' directory.\"\n        )\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.SaveCheckpoints.__init__","title":"<code>__init__(dirname, *, frequency=100, buffer=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dirname</code> <code>str</code> <p>the model directory name to save checkpoints. Automatically created inside <code>checkpoints</code> directory as <code>checkpoints/&lt;dirname&gt;/saves</code>.</p> <p>Compliments <code>TrainCallback.RecordVideos</code> callback.</p> required <code>frequency</code> <code>int</code> <p>save frequency (in episodes or steps)</p> <code>100</code> <code>buffer</code> <code>bool</code> <p>whether to save the buffer state</p> <code>False</code> Source code in <code>velora/callbacks.py</code> Python<pre><code>@override\ndef __init__(\n    self,\n    dirname: str,\n    *,\n    frequency: int = 100,\n    buffer: bool = False,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        dirname (str): the model directory name to save checkpoints.\n            Automatically created inside `checkpoints` directory as\n            `checkpoints/&lt;dirname&gt;/saves`.\n\n            Compliments `TrainCallback.RecordVideos` callback.\n\n        frequency (int, optional): save frequency (in episodes or steps)\n        buffer (bool, optional): whether to save the buffer state\n    \"\"\"\n    self.dirname = dirname\n    self.frequency = frequency\n    self.buffer = buffer\n\n    self.filepath = Path(\"checkpoints\", self.dirname, \"saves\")\n\n    if self.filepath.exists():\n        raise FileExistsError(\n            f\"Items already exist in the '{self.filepath.parent}' directory! Either change the 'dirname' or delete the folders contents.\"\n        )\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.SaveCheckpoints.save_checkpoint","title":"<code>save_checkpoint(agent, dirname)</code>","text":"<p>Saves a checkpoint at a given episode with the given suffix.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>RLModuleAgent</code> <p>the agent being trained</p> required <code>dirname</code> <code>str</code> <p>the checkpoint directory name</p> required Source code in <code>velora/callbacks.py</code> Python<pre><code>def save_checkpoint(self, agent: \"RLModuleAgent\", dirname: str) -&gt; None:\n    \"\"\"\n    Saves a checkpoint at a given episode with the given suffix.\n\n    Parameters:\n        agent (RLModuleAgent): the agent being trained\n        dirname (str): the checkpoint directory name\n    \"\"\"\n    checkpoint_path = Path(self.filepath, dirname)\n    agent.save(checkpoint_path, buffer=self.buffer, config=True)\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.TrainCallback","title":"<code>TrainCallback</code>","text":"<p>Abstract base class for all training callbacks.</p> Source code in <code>velora/callbacks.py</code> Python<pre><code>class TrainCallback:\n    \"\"\"\n    Abstract base class for all training callbacks.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, *args, **kwargs) -&gt; None:\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def __call__(self, state: \"TrainState\") -&gt; \"TrainState\":\n        \"\"\"\n        The callback function that gets called during training.\n\n        Parameters:\n            state (TrainState): the current training state\n\n        Returns:\n            state (TrainState): the current training state.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def config(self) -&gt; Tuple[str, Dict[str, Any]]:\n        \"\"\"\n        Retrieves callback details in the form: `(name, values)`.\n\n        Returns:\n            name (str): callback name.\n            values (Dict[str, Any]): a dictionary containing callback settings.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def info(self) -&gt; str:\n        \"\"\"\n        Provides details with basic information about the callback initialization.\n\n        Returns:\n            details (str): a string of information.\n        \"\"\"\n        pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.TrainCallback.__call__","title":"<code>__call__(state)</code>  <code>abstractmethod</code>","text":"<p>The callback function that gets called during training.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainState</code> <p>the current training state</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>TrainState</code> <p>the current training state.</p> Source code in <code>velora/callbacks.py</code> Python<pre><code>@abstractmethod\ndef __call__(self, state: \"TrainState\") -&gt; \"TrainState\":\n    \"\"\"\n    The callback function that gets called during training.\n\n    Parameters:\n        state (TrainState): the current training state\n\n    Returns:\n        state (TrainState): the current training state.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.TrainCallback.config","title":"<code>config()</code>  <code>abstractmethod</code>","text":"<p>Retrieves callback details in the form: <code>(name, values)</code>.</p> <p>Returns:</p> Name Type Description <code>name</code> <code>str</code> <p>callback name.</p> <code>values</code> <code>Dict[str, Any]</code> <p>a dictionary containing callback settings.</p> Source code in <code>velora/callbacks.py</code> Python<pre><code>@abstractmethod\ndef config(self) -&gt; Tuple[str, Dict[str, Any]]:\n    \"\"\"\n    Retrieves callback details in the form: `(name, values)`.\n\n    Returns:\n        name (str): callback name.\n        values (Dict[str, Any]): a dictionary containing callback settings.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/callbacks/#velora.callbacks.TrainCallback.info","title":"<code>info()</code>  <code>abstractmethod</code>","text":"<p>Provides details with basic information about the callback initialization.</p> <p>Returns:</p> Name Type Description <code>details</code> <code>str</code> <p>a string of information.</p> Source code in <code>velora/callbacks.py</code> Python<pre><code>@abstractmethod\ndef info(self) -&gt; str:\n    \"\"\"\n    Provides details with basic information about the callback initialization.\n\n    Returns:\n        details (str): a string of information.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/gym/","title":"velora.gym","text":"Documentation <p>User Guide - Tutorials: Gymnasium</p> <p>Dedicated Gymnasium [] utility methods to simplify working with the Gymnasium API.</p>"},{"location":"learn/reference/gym/#velora.gym.EnvResult","title":"<code>EnvResult</code>  <code>dataclass</code>","text":"<p>Container for environment search results.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the environment</p> required <code>type</code> <code>str</code> <p>the environment type, either <code>discrete</code> or <code>continuous</code></p> required Source code in <code>velora/gym/search.py</code> Python<pre><code>@dataclass\nclass EnvResult:\n    \"\"\"\n    Container for environment search results.\n\n    Parameters:\n        name (str): the name of the environment\n        type (str): the environment type, either `discrete` or `continuous`\n    \"\"\"\n\n    name: str\n    type: str\n</code></pre>"},{"location":"learn/reference/gym/#velora.gym.EnvSearch","title":"<code>EnvSearch</code>","text":"<p>A utility class for searching for Gymnasium environments.</p> Source code in <code>velora/gym/search.py</code> Python<pre><code>class EnvSearch:\n    \"\"\"\n    A utility class for searching for [Gymnasium](https://gymnasium.farama.org/)\n    environments.\n    \"\"\"\n\n    _result_cache: List[EnvResult] = []\n    _helper = SearchHelper()\n\n    @classmethod\n    def _build_name_cache(cls) -&gt; None:\n        \"\"\"\n        Builds the environment name cache storing the latest\n        [Gymnasium](https://gymnasium.farama.org/) environment names.\n        \"\"\"\n        if cls._result_cache:\n            return\n\n        names = cls._helper.get_latest_env_names()\n        print(f\"Caching {len(names)} environments...\", end=\"\")\n        results = []\n\n        for env in names:\n            env_type = cls._helper.get_env_type(env)\n            results.append(EnvResult(name=env, type=env_type))\n\n        cls._result_cache = results\n        print(\"Complete.\")\n\n    @classmethod\n    def find(cls, query: str) -&gt; List[EnvResult]:\n        \"\"\"\n        Find a [Gymnasium](https://gymnasium.farama.org/) environment that contains `query`.\n\n        Parameters:\n            query (str): partial or complete name of an environment\n                (e.g., `Lunar` or `Pendulum`)\n\n        Returns:\n            result (List[EnvResult]): a list of environment results matching the query.\n        \"\"\"\n        cls._build_name_cache()\n\n        matches = [env for env in cls._result_cache if query in env.name]\n        print(f\"{len(matches)} environments found.\")\n        return matches\n\n    @classmethod\n    def discrete(cls) -&gt; List[EnvResult]:\n        \"\"\"\n        Get all available discrete [Gymnasium](https://gymnasium.farama.org/) environments.\n\n        Returns:\n            names (List[EnvResult]): a list of available discrete environments.\n        \"\"\"\n        cls._build_name_cache()\n\n        matches = [env for env in cls._result_cache if env.type == \"discrete\"]\n        print(f\"{len(matches)} environments found.\")\n        return matches\n\n    @classmethod\n    def continuous(cls) -&gt; List[EnvResult]:\n        \"\"\"\n        Get all available continuous [Gymnasium](https://gymnasium.farama.org/) environments.\n\n        Returns:\n            names (List[EnvResult]): a list of available continuous environments.\n        \"\"\"\n        cls._build_name_cache()\n\n        matches = [env for env in cls._result_cache if env.type == \"continuous\"]\n        print(f\"{len(matches)} environments found.\")\n        return matches\n</code></pre>"},{"location":"learn/reference/gym/#velora.gym.EnvSearch.continuous","title":"<code>continuous()</code>  <code>classmethod</code>","text":"<p>Get all available continuous Gymnasium environments.</p> <p>Returns:</p> Name Type Description <code>names</code> <code>List[EnvResult]</code> <p>a list of available continuous environments.</p> Source code in <code>velora/gym/search.py</code> Python<pre><code>@classmethod\ndef continuous(cls) -&gt; List[EnvResult]:\n    \"\"\"\n    Get all available continuous [Gymnasium](https://gymnasium.farama.org/) environments.\n\n    Returns:\n        names (List[EnvResult]): a list of available continuous environments.\n    \"\"\"\n    cls._build_name_cache()\n\n    matches = [env for env in cls._result_cache if env.type == \"continuous\"]\n    print(f\"{len(matches)} environments found.\")\n    return matches\n</code></pre>"},{"location":"learn/reference/gym/#velora.gym.EnvSearch.discrete","title":"<code>discrete()</code>  <code>classmethod</code>","text":"<p>Get all available discrete Gymnasium environments.</p> <p>Returns:</p> Name Type Description <code>names</code> <code>List[EnvResult]</code> <p>a list of available discrete environments.</p> Source code in <code>velora/gym/search.py</code> Python<pre><code>@classmethod\ndef discrete(cls) -&gt; List[EnvResult]:\n    \"\"\"\n    Get all available discrete [Gymnasium](https://gymnasium.farama.org/) environments.\n\n    Returns:\n        names (List[EnvResult]): a list of available discrete environments.\n    \"\"\"\n    cls._build_name_cache()\n\n    matches = [env for env in cls._result_cache if env.type == \"discrete\"]\n    print(f\"{len(matches)} environments found.\")\n    return matches\n</code></pre>"},{"location":"learn/reference/gym/#velora.gym.EnvSearch.find","title":"<code>find(query)</code>  <code>classmethod</code>","text":"<p>Find a Gymnasium environment that contains <code>query</code>.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>partial or complete name of an environment (e.g., <code>Lunar</code> or <code>Pendulum</code>)</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>List[EnvResult]</code> <p>a list of environment results matching the query.</p> Source code in <code>velora/gym/search.py</code> Python<pre><code>@classmethod\ndef find(cls, query: str) -&gt; List[EnvResult]:\n    \"\"\"\n    Find a [Gymnasium](https://gymnasium.farama.org/) environment that contains `query`.\n\n    Parameters:\n        query (str): partial or complete name of an environment\n            (e.g., `Lunar` or `Pendulum`)\n\n    Returns:\n        result (List[EnvResult]): a list of environment results matching the query.\n    \"\"\"\n    cls._build_name_cache()\n\n    matches = [env for env in cls._result_cache if query in env.name]\n    print(f\"{len(matches)} environments found.\")\n    return matches\n</code></pre>"},{"location":"learn/reference/gym/#velora.gym.SearchHelper","title":"<code>SearchHelper</code>","text":"<p>A helper class containing methods for creating the <code>EnvSearch</code> cache.</p> Source code in <code>velora/gym/search.py</code> Python<pre><code>class SearchHelper:\n    \"\"\"\n    A helper class containing methods for creating the `EnvSearch` cache.\n    \"\"\"\n\n    @staticmethod\n    def get_env_type(env_name: str) -&gt; Literal[\"discrete\", \"continuous\"] | None:\n        \"\"\"\n        Determines if an environment has a discrete or continuous action space.\n\n        Parameters:\n            env_name (str): name of the environment\n\n        Returns:\n            env_type (str | None): one of three values -\n\n            - `discrete` for Discrete action space.\n            - `continuous` for Box action space.\n            - `None`, otherwise.\n        \"\"\"\n        try:\n            env = gym.make(env_name)\n\n            if isinstance(env.action_space, gym.spaces.Discrete):\n                return \"discrete\"\n            elif isinstance(env.action_space, gym.spaces.Box):\n                return \"continuous\"\n            return None\n\n        except Exception as e:\n            raise e\n        finally:\n            if \"env\" in locals():\n                env.close()\n\n    @staticmethod\n    def get_latest_env_names() -&gt; List[str]:\n        \"\"\"\n        Builds a list of the latest [Gymnasium](https://gymnasium.farama.org/) environment names.\n\n        Returns:\n            names (List[str]): a list of names for all latest env versions.\n        \"\"\"\n        env_dict: Dict[str, int] = {}\n\n        for env_name in gym.envs.registry.keys():\n            # Skip envs with paths (JAX-based) or old Gym versions\n            if \"/\" in env_name or env_name.startswith(\"GymV\"):\n                continue\n\n            match = re.match(r\"(.+)-v(\\d+)$\", env_name)\n\n            if match:\n                base_name, version = match.groups()\n                version = int(version)\n\n            # Keep only the latest version\n            if base_name not in env_dict or version &gt; env_dict[base_name]:\n                env_dict[base_name] = version\n\n        return [f\"{name}-v{version}\" for name, version in env_dict.items()]\n</code></pre>"},{"location":"learn/reference/gym/#velora.gym.SearchHelper.get_env_type","title":"<code>get_env_type(env_name)</code>  <code>staticmethod</code>","text":"<p>Determines if an environment has a discrete or continuous action space.</p> <p>Parameters:</p> Name Type Description Default <code>env_name</code> <code>str</code> <p>name of the environment</p> required <p>Returns:</p> Name Type Description <code>env_type</code> <code>str | None</code> <p>one of three values -</p> <code>Literal['discrete', 'continuous'] | None</code> <ul> <li><code>discrete</code> for Discrete action space.</li> </ul> <code>Literal['discrete', 'continuous'] | None</code> <ul> <li><code>continuous</code> for Box action space.</li> </ul> <code>Literal['discrete', 'continuous'] | None</code> <ul> <li><code>None</code>, otherwise.</li> </ul> Source code in <code>velora/gym/search.py</code> Python<pre><code>@staticmethod\ndef get_env_type(env_name: str) -&gt; Literal[\"discrete\", \"continuous\"] | None:\n    \"\"\"\n    Determines if an environment has a discrete or continuous action space.\n\n    Parameters:\n        env_name (str): name of the environment\n\n    Returns:\n        env_type (str | None): one of three values -\n\n        - `discrete` for Discrete action space.\n        - `continuous` for Box action space.\n        - `None`, otherwise.\n    \"\"\"\n    try:\n        env = gym.make(env_name)\n\n        if isinstance(env.action_space, gym.spaces.Discrete):\n            return \"discrete\"\n        elif isinstance(env.action_space, gym.spaces.Box):\n            return \"continuous\"\n        return None\n\n    except Exception as e:\n        raise e\n    finally:\n        if \"env\" in locals():\n            env.close()\n</code></pre>"},{"location":"learn/reference/gym/#velora.gym.SearchHelper.get_latest_env_names","title":"<code>get_latest_env_names()</code>  <code>staticmethod</code>","text":"<p>Builds a list of the latest Gymnasium environment names.</p> <p>Returns:</p> Name Type Description <code>names</code> <code>List[str]</code> <p>a list of names for all latest env versions.</p> Source code in <code>velora/gym/search.py</code> Python<pre><code>@staticmethod\ndef get_latest_env_names() -&gt; List[str]:\n    \"\"\"\n    Builds a list of the latest [Gymnasium](https://gymnasium.farama.org/) environment names.\n\n    Returns:\n        names (List[str]): a list of names for all latest env versions.\n    \"\"\"\n    env_dict: Dict[str, int] = {}\n\n    for env_name in gym.envs.registry.keys():\n        # Skip envs with paths (JAX-based) or old Gym versions\n        if \"/\" in env_name or env_name.startswith(\"GymV\"):\n            continue\n\n        match = re.match(r\"(.+)-v(\\d+)$\", env_name)\n\n        if match:\n            base_name, version = match.groups()\n            version = int(version)\n\n        # Keep only the latest version\n        if base_name not in env_dict or version &gt; env_dict[base_name]:\n            env_dict[base_name] = version\n\n    return [f\"{name}-v{version}\" for name, version in env_dict.items()]\n</code></pre>"},{"location":"learn/reference/gym/#velora.gym.add_core_env_wrappers","title":"<code>add_core_env_wrappers(env, device)</code>","text":"<p>Wraps a Gymnasium environment with the following (in order) if not already applied:</p> <ul> <li>RecordEpisodeStatistics - for easily retrieving episode statistics.</li> <li>NumpyToTorch - for turning environment feedback into <code>PyTorch</code> tensors.</li> </ul> <p>Used in all pre-built algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>gym.Env</code> <p>the Gymnasium environment</p> required <code>device</code> <code>torch.device</code> <p>the PyTorch device to perform computations on</p> required <p>Returns:</p> Name Type Description <code>env</code> <code>gym.Env</code> <p>an updated Gymnasium environment</p> Source code in <code>velora/gym/wrap.py</code> Python<pre><code>def add_core_env_wrappers(env: gym.Env, device: torch.device) -&gt; gym.Env:\n    \"\"\"\n    Wraps a [Gymnasium](https://gymnasium.farama.org/) environment with the following (in order) if not already applied:\n\n    - [RecordEpisodeStatistics](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RecordEpisodeStatistics) - for easily retrieving episode statistics.\n    - [NumpyToTorch](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.NumpyToTorch) - for turning environment feedback into `PyTorch` tensors.\n\n    Used in all pre-built algorithms.\n\n    Parameters:\n        env (gym.Env): the Gymnasium environment\n        device (torch.device): the PyTorch device to perform computations on\n\n    Returns:\n        env (gym.Env): an updated Gymnasium environment\n    \"\"\"\n    if isinstance(env, gym.Env):\n        from gymnasium.wrappers import RecordEpisodeStatistics\n        from gymnasium.wrappers.numpy_to_torch import NumpyToTorch\n    else:\n        raise ValueError(f\"{type(env)}' is not supported.\")\n\n    has_stats = False\n    has_torch = False\n\n    current_env = env\n    while hasattr(current_env, \"env\"):\n        if isinstance(current_env, RecordEpisodeStatistics):\n            has_stats = True\n        if isinstance(current_env, NumpyToTorch):\n            has_torch = True\n\n        current_env = current_env.env\n\n    if not has_stats:\n        env = RecordEpisodeStatistics(env)\n\n    if not has_torch:\n        env = NumpyToTorch(env, device=device)\n\n    return env\n</code></pre>"},{"location":"learn/reference/gym/#velora.gym.wrap_gym_env","title":"<code>wrap_gym_env(env, wrappers)</code>","text":"<p>Creates a new Gymnasium environment with one or more gymnasium.Wrappers applied.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>gymnasium.Env | str</code> <p>a name of a Gymnasium environment or the environment itself to wrap</p> required <code>wrappers</code> <code>List[gym.Wrapper | gym.vector.VectorWrapper | functools.partial]</code> <p>a list of wrapper classes or partially applied wrapper functions</p> required <p>Examples:</p> <p>A Gymnasium environment with normalization and reward clipping: Python<pre><code>from functools import partial\n\nfrom gymnasium.wrappers import (\n    NormalizeObservation,\n    NormalizeReward,\n    ClipReward,\n)\n\nenv = wrap_gym_env(\"InvertedPendulum-v5\", [\n    partial(NormalizeObservation, epsilon=1e-8),\n    partial(NormalizeReward, gamma=0.99, epsilon=1e-8),\n    partial(ClipReward, max_reward=10.0)\n])\n</code></pre></p> <p>Returns:</p> Name Type Description <code>env</code> <code>gymnasium.Env</code> <p>The wrapped environment</p> Source code in <code>velora/gym/wrap.py</code> Python<pre><code>def wrap_gym_env(\n    env: gym.Env | str | DiscreteGymNames | ContinuousGymNames,\n    wrappers: List[gym.Wrapper | gym.vector.VectorWrapper | Callable],\n) -&gt; gym.Env:\n    \"\"\"\n    Creates a new [Gymnasium](https://gymnasium.farama.org/) environment with\n    one or more [gymnasium.Wrappers](https://gymnasium.farama.org/api/wrappers/table/) applied.\n\n    Parameters:\n        env (gymnasium.Env | str): a name of a Gymnasium environment or the\n            environment itself to wrap\n        wrappers (List[gym.Wrapper | gym.vector.VectorWrapper | functools.partial]): a list of wrapper classes or partially applied wrapper functions\n\n    Examples:\n        A Gymnasium environment with normalization and reward clipping:\n        ```python\n        from functools import partial\n\n        from gymnasium.wrappers import (\n            NormalizeObservation,\n            NormalizeReward,\n            ClipReward,\n        )\n\n        env = wrap_gym_env(\"InvertedPendulum-v5\", [\n            partial(NormalizeObservation, epsilon=1e-8),\n            partial(NormalizeReward, gamma=0.99, epsilon=1e-8),\n            partial(ClipReward, max_reward=10.0)\n        ])\n        ```\n\n    Returns:\n        env (gymnasium.Env): The wrapped environment\n    \"\"\"\n    if isinstance(env, str):\n        env = gym.make(env, render_mode=\"rgb_array\")\n\n    def apply_wrapper(env: gym.Env, wrapper: WrapperType) -&gt; gym.Env:\n        return wrapper(env)\n\n    return reduce(apply_wrapper, wrappers, env)\n</code></pre>"},{"location":"learn/reference/metrics/","title":"velora.metrics","text":"Documentation <p>User Guide - Tutorials: Training Metrics</p> <p>Classes and methods dedicated to offline metric storage.</p>"},{"location":"learn/reference/metrics/#velora.metrics.Episode","title":"<code>Episode</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Episode-level metrics tracking reward, length, and agent performance.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>a unique identifier for the episode</p> <code>experiment_id</code> <code>int</code> <p>the experiment ID associated to the episode</p> <code>episode_num</code> <code>int</code> <p>the episode index</p> <code>reward</code> <code>float</code> <p>the episodic reward (return)</p> <code>length</code> <code>int</code> <p>the number of timesteps performed to terminate the episode</p> <code>reward_moving_avg</code> <code>float</code> <p>the episodes reward moving average based on a window size</p> <code>reward_moving_std</code> <code>float</code> <p>the episodes reward moving standard deviation based on a window size</p> <code>actor_loss</code> <code>float</code> <p>the average Actor loss for the episode</p> <code>critic_loss</code> <code>float</code> <p>the average Critic loss for the episode</p> <code>entropy_loss</code> <code>float</code> <p>the average Entropy loss for the episode</p> <code>created_at</code> <code>datetime</code> <p>the date and time when the the entry was created</p> Source code in <code>velora/metrics/models.py</code> Python<pre><code>class Episode(SQLModel, table=True):\n    \"\"\"\n    Episode-level metrics tracking reward, length, and agent performance.\n\n    Attributes:\n        id (int): a unique identifier for the episode\n        experiment_id (int): the experiment ID associated to the episode\n        episode_num (int): the episode index\n        reward (float): the episodic reward (return)\n        length (int): the number of timesteps performed to terminate the episode\n        reward_moving_avg (float): the episodes reward moving average based on a window size\n        reward_moving_std (float): the episodes reward moving standard deviation based on a\n            window size\n        actor_loss (float): the average Actor loss for the episode\n        critic_loss (float): the average Critic loss for the episode\n        entropy_loss (float): the average Entropy loss for the episode\n        created_at (datetime): the date and time when the the entry was created\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    experiment_id: int = Field(foreign_key=\"experiment.id\", index=True)\n    episode_num: int = Field(index=True)\n\n    # Core metrics\n    reward: float\n    length: int\n\n    # Statistical metrics\n    reward_moving_avg: float\n    reward_moving_std: float\n\n    # Loss metrics\n    actor_loss: float\n    critic_loss: float\n    entropy_loss: float = Field(default=0.0)\n\n    # Timestamps\n    created_at: datetime = Field(default_factory=datetime.now)\n\n    # Relationships\n    experiment: Experiment = Relationship(back_populates=\"episodes\")\n</code></pre>"},{"location":"learn/reference/metrics/#velora.metrics.Experiment","title":"<code>Experiment</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Experiment information tracking agent, environment, and metadata.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>unique identifier for the experiment</p> <code>agent</code> <code>str</code> <p>the name of the agent used in the experiment</p> <code>env</code> <code>str</code> <p>the name of the environment used in the experiment</p> <code>config</code> <code>str</code> <p>a JSON string containing the agent's configuration details</p> <code>created_at</code> <code>datetime</code> <p>the date and time the experiment was created</p> Source code in <code>velora/metrics/models.py</code> Python<pre><code>class Experiment(SQLModel, table=True):\n    \"\"\"\n    Experiment information tracking agent, environment, and metadata.\n\n    Attributes:\n        id (int): unique identifier for the experiment\n        agent (str): the name of the agent used in the experiment\n        env (str): the name of the environment used in the experiment\n        config (str): a JSON string containing the agent's configuration details\n        created_at (datetime): the date and time the experiment was created\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    agent: str = Field(index=True)\n    env: str = Field(index=True)\n    config: str  # JSON object\n    created_at: datetime = Field(default_factory=datetime.now)\n\n    # Relationships\n    episodes: List[\"Episode\"] = Relationship(back_populates=\"experiment\")\n</code></pre>"},{"location":"learn/reference/metrics/#velora.metrics.get_current_episode","title":"<code>get_current_episode(session, experiment_id, current_ep)</code>","text":"<p>Queries a database session to retrieve the current episode for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sqlmodel.Session</code> <p>a metric database session</p> required <code>experiment_id</code> <code>int</code> <p>the current experiment's unique ID</p> required <code>current_ep</code> <code>int</code> <p>the current episode index</p> required <p>Returns:</p> Name Type Description <code>results</code> <code>ScalarResult[Episode]</code> <p>a iterable set of matching episodes.</p> Source code in <code>velora/metrics/db.py</code> Python<pre><code>def get_current_episode(\n    session: Session,\n    experiment_id: int,\n    current_ep: int,\n) -&gt; ScalarResult[Episode]:\n    \"\"\"\n    Queries a database session to retrieve the current episode for an experiment.\n\n    Parameters:\n        session (sqlmodel.Session): a metric database session\n        experiment_id (int): the current experiment's unique ID\n        current_ep (int): the current episode index\n\n    Returns:\n        results (ScalarResult[Episode]): a iterable set of matching episodes.\n    \"\"\"\n    statement = select(Episode).where(\n        Episode.experiment_id == experiment_id,\n        Episode.episode_num == current_ep,\n    )\n    return session.exec(statement)\n</code></pre>"},{"location":"learn/reference/metrics/#velora.metrics.get_db_engine","title":"<code>get_db_engine()</code>","text":"<p>Starts the metric database engine and returns it.</p> <p>Returns:</p> Name Type Description <code>engine</code> <code>sqlalchemy.Engine</code> <p>a database engine instance</p> Source code in <code>velora/metrics/db.py</code> Python<pre><code>def get_db_engine() -&gt; Engine:\n    \"\"\"\n    Starts the metric database engine and returns it.\n\n    Returns:\n        engine (sqlalchemy.Engine): a database engine instance\n    \"\"\"\n    from velora.metrics.models import Episode, Experiment  # pragma: no cover\n\n    if os.getenv(\"VELORA_TEST_MODE\", \"\").lower() in (\"true\", \"1\"):\n        # Use in-memory database for testing\n        engine = create_engine(\"sqlite:///:memory:\")\n    else:\n        # Regular file-based database\n        engine = create_engine(\"sqlite:///metrics.db\")\n\n    SQLModel.metadata.create_all(engine)\n    return engine\n</code></pre>"},{"location":"learn/reference/state/","title":"velora.state","text":"Documentation <p>User Guide - Tutorials: Callbacks</p> <p>Dataclasses for storing states used during callbacks and agent training.</p>"},{"location":"learn/reference/state/#velora.state.AnalyticsState","title":"<code>AnalyticsState</code>  <code>dataclass</code>","text":"<p>A storage container for the details of a Comet or Weights and Biases analytics experiment.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>the name of the project to add this experiment to</p> required <code>experiment_name</code> <code>str</code> <p>the name of the experiment</p> <code>None</code> <code>tags</code> <code>List[str]</code> <p>a list of tags associated with the experiment</p> <code>None</code> Source code in <code>velora/state.py</code> Python<pre><code>@dataclass\nclass AnalyticsState:\n    \"\"\"\n    A storage container for the details of a [Comet](https://www.comet.com/) or\n    [Weights and Biases](https://wandb.ai/) analytics experiment.\n\n    Parameters:\n        project_name (str): the name of the project to add this experiment to\n        experiment_name (str, optional): the name of the experiment\n        tags (List[str], optional): a list of tags associated with the experiment\n    \"\"\"\n\n    project_name: str\n    experiment_name: str | None = None\n    tags: List[str] | None = None\n</code></pre>"},{"location":"learn/reference/state/#velora.state.RecordState","title":"<code>RecordState</code>  <code>dataclass</code>","text":"<p>A storage container for the video recording state.</p> <p>Parameters:</p> Name Type Description Default <code>dirpath</code> <code>Path</code> <p>the video directory path to store the videos</p> required <code>method</code> <code>Literal['episode', 'step']</code> <p>the recording method</p> required <code>episode_trigger</code> <code>Callable[[int], bool]</code> <p>the <code>episode</code> recording trigger</p> <code>None</code> <code>step_trigger</code> <code>Callable[[int], bool]</code> <p>the <code>step</code> recording trigger</p> <code>None</code> Source code in <code>velora/state.py</code> Python<pre><code>@dataclass\nclass RecordState:\n    \"\"\"\n    A storage container for the video recording state.\n\n    Parameters:\n        dirpath (Path): the video directory path to store the videos\n        method (Literal[\"episode\", \"step\"]): the recording method\n        episode_trigger (Callable[[int], bool], optional): the `episode` recording\n            trigger\n        step_trigger (Callable[[int], bool], optional): the `step` recording trigger\n    \"\"\"\n\n    dirpath: Path\n    method: RecordMethodLiteral\n    episode_trigger: Callable[[int], bool] | None = None\n    step_trigger: Callable[[int], bool] | None = None\n\n    def to_wrapper(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Converts the state into wrapper parameters.\n\n        Returns:\n            params (Dict[str, Any]): values as parameters for [Gymnasium's RecordVideo](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RecordVideo) wrapper.\n\n            Includes the following keys - `[video_folder, episode_trigger, step_trigger]`.\n        \"\"\"\n        return {\n            \"video_folder\": self.dirpath,\n            \"episode_trigger\": self.episode_trigger,\n            \"step_trigger\": self.step_trigger,\n        }\n</code></pre>"},{"location":"learn/reference/state/#velora.state.RecordState.to_wrapper","title":"<code>to_wrapper()</code>","text":"<p>Converts the state into wrapper parameters.</p> <p>Returns:</p> Name Type Description <code>params</code> <code>Dict[str, Any]</code> <p>values as parameters for Gymnasium's RecordVideo wrapper.</p> <code>Dict[str, Any]</code> <p>Includes the following keys - <code>[video_folder, episode_trigger, step_trigger]</code>.</p> Source code in <code>velora/state.py</code> Python<pre><code>def to_wrapper(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Converts the state into wrapper parameters.\n\n    Returns:\n        params (Dict[str, Any]): values as parameters for [Gymnasium's RecordVideo](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RecordVideo) wrapper.\n\n        Includes the following keys - `[video_folder, episode_trigger, step_trigger]`.\n    \"\"\"\n    return {\n        \"video_folder\": self.dirpath,\n        \"episode_trigger\": self.episode_trigger,\n        \"step_trigger\": self.step_trigger,\n    }\n</code></pre>"},{"location":"learn/reference/state/#velora.state.TrainState","title":"<code>TrainState</code>  <code>dataclass</code>","text":"<p>A storage container for the current state of model training.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>RLModuleAgent</code> <p>the agent being trained</p> required <code>env</code> <code>gymnasium.Env</code> <p>a single training or evaluation environment</p> required <code>session</code> <code>sqlmodel.Session</code> <p>the current metric database session</p> required <code>experiment_id</code> <code>int</code> <p>the current experiment's unique ID</p> required <code>total_episodes</code> <code>int</code> <p>total number of training episodes</p> <code>0</code> <code>total_steps</code> <code>int</code> <p>total number of training steps</p> <code>0</code> <code>status</code> <code>Literal['start', 'episode', 'logging', 'step', 'complete']</code> <p>the current stage of training.</p> <ul> <li><code>start</code> - before training starts.</li> <li><code>episode</code> - inside the episode loop.</li> <li><code>logging</code> - metric logging.</li> <li><code>step</code> - inside the timestep loop.</li> <li><code>complete</code> - completed training.</li> </ul> <code>'start'</code> <code>logging_type</code> <code>Literal['episode', 'step']</code> <p>the logging type</p> <code>'episode'</code> <code>current_ep</code> <code>int</code> <p>the current episode index</p> <code>0</code> <code>current_step</code> <code>int</code> <p>the current training timestep</p> <code>0</code> <code>ep_reward</code> <code>float</code> <p>the current episode reward</p> <code>0.0</code> <code>stop_training</code> <code>bool</code> <p>a flag to declare training termination</p> <code>False</code> <code>saving_enabled</code> <code>bool</code> <p>a flag for checkpoint saving</p> <code>False</code> <code>checkpoint_dir</code> <code>Path</code> <p>the checkpoint directory path when <code>saving_enabled=True</code></p> <code>None</code> <code>record_state</code> <code>RecordState</code> <p>the video recording state</p> <code>None</code> <code>analytics_state</code> <code>AnalyticsState</code> <p>the analytics state</p> <code>None</code> Source code in <code>velora/state.py</code> Python<pre><code>@dataclass\nclass TrainState:\n    \"\"\"\n    A storage container for the current state of model training.\n\n    Parameters:\n        agent (RLModuleAgent): the agent being trained\n        env (gymnasium.Env): a single training or evaluation environment\n        session (sqlmodel.Session): the current metric database session\n        experiment_id (int): the current experiment's unique ID\n        total_episodes (int, optional): total number of training episodes\n        total_steps (int, optional): total number of training steps\n        status (Literal[\"start\", \"episode\", \"logging\", \"step\", \"complete\"], optional): the current stage of training.\n\n            - `start` - before training starts.\n            - `episode` - inside the episode loop.\n            - `logging` - metric logging.\n            - `step` - inside the timestep loop.\n            - `complete` - completed training.\n\n        logging_type (Literal[\"episode\", \"step\"], optional): the logging type\n        current_ep (int, optional): the current episode index\n        current_step (int, optional): the current training timestep\n        ep_reward (float, optional): the current episode reward\n        stop_training (bool, optional): a flag to declare training termination\n        saving_enabled (bool, optional): a flag for checkpoint saving\n        checkpoint_dir (Path, optional): the checkpoint directory path when\n            `saving_enabled=True`\n        record_state (RecordState, optional): the video recording state\n        analytics_state (AnalyticsState, optional): the analytics state\n    \"\"\"\n\n    agent: RLModuleAgent\n    env: gym.Env\n    session: Session\n    experiment_id: int\n    total_episodes: int = 0\n    total_steps: int = 0\n    status: StatusLiteral = \"start\"\n    logging_type: Literal[\"episode\", \"step\"] = \"episode\"\n    current_ep: int = 0\n    current_step: int = 0\n    ep_reward: float = 0.0\n    stop_training: bool = False\n    saving_enabled: bool = False\n    checkpoint_dir: Path | None = None\n    record_state: RecordState | None = None\n    analytics_state: AnalyticsState | None = None\n\n    def update(\n        self,\n        *,\n        status: StatusLiteral | None = None,\n        current_ep: int | None = None,\n        current_step: int | None = None,\n        ep_reward: int | None = None,\n        logging_type: Literal[\"episode\", \"step\"] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Updates the training state. When any input is `None`, uses existing value.\n\n        Parameters:\n            status (Literal[\"start\", \"episode\", \"logging\", \"step\", \"complete\"], optional): the current stage of training.\n\n                - `start` - before training start.\n                - `episode` - inside the episode loop.\n                - `logging` - metric logging.\n                - `step` - inside the timestep loop.\n                - `complete` - completed training.\n\n            current_ep (int, optional): the current episode index\n            current_step (int, optional): the current training timestep\n            ep_reward (float, optional): the current episode or rollout update reward\n            logging_type (Literal[\"episode\", \"step\"], optional): the logging type\n        \"\"\"\n        self.status = status if status else self.status\n        self.current_ep = current_ep if current_ep else self.current_ep\n        self.current_step = current_step if current_step else self.current_step\n        self.ep_reward = ep_reward if ep_reward else self.ep_reward\n        self.logging_type = logging_type if logging_type else self.logging_type\n\n    def analytics_update(self) -&gt; None:\n        \"\"\"\n        Updates the analytics state details that are `None` dynamically, using\n        the current training state.\n        \"\"\"\n        agent_name = self.agent.__class__.__name__\n        env_name = self.env.spec.name\n\n        new_state = self.analytics_state\n\n        new_state.experiment_name = (\n            new_state.experiment_name\n            if new_state.experiment_name\n            else f\"{agent_name}_{env_name}_{self.total_episodes}ep\"\n        )\n\n        new_state.tags = new_state.tags if new_state.tags else [agent_name, env_name]\n\n        # Update state\n        self.analytics_state = new_state\n</code></pre>"},{"location":"learn/reference/state/#velora.state.TrainState.analytics_update","title":"<code>analytics_update()</code>","text":"<p>Updates the analytics state details that are <code>None</code> dynamically, using the current training state.</p> Source code in <code>velora/state.py</code> Python<pre><code>def analytics_update(self) -&gt; None:\n    \"\"\"\n    Updates the analytics state details that are `None` dynamically, using\n    the current training state.\n    \"\"\"\n    agent_name = self.agent.__class__.__name__\n    env_name = self.env.spec.name\n\n    new_state = self.analytics_state\n\n    new_state.experiment_name = (\n        new_state.experiment_name\n        if new_state.experiment_name\n        else f\"{agent_name}_{env_name}_{self.total_episodes}ep\"\n    )\n\n    new_state.tags = new_state.tags if new_state.tags else [agent_name, env_name]\n\n    # Update state\n    self.analytics_state = new_state\n</code></pre>"},{"location":"learn/reference/state/#velora.state.TrainState.update","title":"<code>update(*, status=None, current_ep=None, current_step=None, ep_reward=None, logging_type=None)</code>","text":"<p>Updates the training state. When any input is <code>None</code>, uses existing value.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>Literal['start', 'episode', 'logging', 'step', 'complete']</code> <p>the current stage of training.</p> <ul> <li><code>start</code> - before training start.</li> <li><code>episode</code> - inside the episode loop.</li> <li><code>logging</code> - metric logging.</li> <li><code>step</code> - inside the timestep loop.</li> <li><code>complete</code> - completed training.</li> </ul> <code>None</code> <code>current_ep</code> <code>int</code> <p>the current episode index</p> <code>None</code> <code>current_step</code> <code>int</code> <p>the current training timestep</p> <code>None</code> <code>ep_reward</code> <code>float</code> <p>the current episode or rollout update reward</p> <code>None</code> <code>logging_type</code> <code>Literal['episode', 'step']</code> <p>the logging type</p> <code>None</code> Source code in <code>velora/state.py</code> Python<pre><code>def update(\n    self,\n    *,\n    status: StatusLiteral | None = None,\n    current_ep: int | None = None,\n    current_step: int | None = None,\n    ep_reward: int | None = None,\n    logging_type: Literal[\"episode\", \"step\"] | None = None,\n) -&gt; None:\n    \"\"\"\n    Updates the training state. When any input is `None`, uses existing value.\n\n    Parameters:\n        status (Literal[\"start\", \"episode\", \"logging\", \"step\", \"complete\"], optional): the current stage of training.\n\n            - `start` - before training start.\n            - `episode` - inside the episode loop.\n            - `logging` - metric logging.\n            - `step` - inside the timestep loop.\n            - `complete` - completed training.\n\n        current_ep (int, optional): the current episode index\n        current_step (int, optional): the current training timestep\n        ep_reward (float, optional): the current episode or rollout update reward\n        logging_type (Literal[\"episode\", \"step\"], optional): the logging type\n    \"\"\"\n    self.status = status if status else self.status\n    self.current_ep = current_ep if current_ep else self.current_ep\n    self.current_step = current_step if current_step else self.current_step\n    self.ep_reward = ep_reward if ep_reward else self.ep_reward\n    self.logging_type = logging_type if logging_type else self.logging_type\n</code></pre>"},{"location":"learn/reference/training/","title":"velora.training","text":"<p>Methods and classes dedicated to handling agent training.</p>"},{"location":"learn/reference/training/#velora.training.MovingMetric","title":"<code>MovingMetric</code>","text":"<p>Tracks a metric with a moving window for statistics.</p> <p>Attributes:</p> Name Type Description <code>window</code> <code>torch.Tensor</code> <p>a list of values for the statistics</p> <code>window_size</code> <code>int</code> <p>the window size of the moving statistics</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>class MovingMetric:\n    \"\"\"\n    Tracks a metric with a moving window for statistics.\n\n    Attributes:\n        window (torch.Tensor): a list of values for the statistics\n        window_size (int): the window size of the moving statistics\n    \"\"\"\n\n    def __init__(self, window_size: int, *, device: torch.device | None = None) -&gt; None:\n        \"\"\"\n        Parameters:\n            window_size (int): the size of the moving window\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        self.window_size = window_size\n        self.device = device\n\n        # Position indicators\n        self.position = 0\n        self.size = 0\n\n        # Pre-allocated storage\n        self.window = torch.zeros((window_size), device=device)\n\n    @property\n    def latest(self) -&gt; torch.Tensor:\n        \"\"\"Gets the latest value.\"\"\"\n        latest_pos = (self.position - 1) % self.window_size\n        return self.window[latest_pos]\n\n    def add(self, value: torch.Tensor) -&gt; None:\n        \"\"\"\n        Adds a value and updates the window.\n\n        Parameters:\n            value (torch.Tensor): value to add\n        \"\"\"\n        self.window[self.position] = value.to(self.device)\n\n        # Update position - deque style\n        self.position = (self.position + 1) % self.window_size\n        self.size = min(self.size + 1, self.window_size)\n\n    def mean(self) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the mean of values or the current window.\n\n        Returns:\n            avg (torch.Tensor): the calculated mean.\n        \"\"\"\n        return self.window.mean()\n\n    def std(self) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the standard deviation of values or the current window.\n\n        Returns:\n            std (torch.Tensor): the calculated standard deviation.\n        \"\"\"\n        return (\n            self.window.std()\n            if self.window.size(dim=0) &gt; 1\n            else torch.tensor(0.0, device=self.device)\n        )\n\n    def max(self) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the maximum value of a set of values or the current window.\n\n        Returns:\n            max (torch.Tensor): the maximum value.\n        \"\"\"\n        return self.window.max()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of items in the values array.\"\"\"\n        return self.size\n</code></pre>"},{"location":"learn/reference/training/#velora.training.MovingMetric.latest","title":"<code>latest</code>  <code>property</code>","text":"<p>Gets the latest value.</p>"},{"location":"learn/reference/training/#velora.training.MovingMetric.__init__","title":"<code>__init__(window_size, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>the size of the moving window</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def __init__(self, window_size: int, *, device: torch.device | None = None) -&gt; None:\n    \"\"\"\n    Parameters:\n        window_size (int): the size of the moving window\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    self.window_size = window_size\n    self.device = device\n\n    # Position indicators\n    self.position = 0\n    self.size = 0\n\n    # Pre-allocated storage\n    self.window = torch.zeros((window_size), device=device)\n</code></pre>"},{"location":"learn/reference/training/#velora.training.MovingMetric.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of items in the values array.</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of items in the values array.\"\"\"\n    return self.size\n</code></pre>"},{"location":"learn/reference/training/#velora.training.MovingMetric.add","title":"<code>add(value)</code>","text":"<p>Adds a value and updates the window.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>torch.Tensor</code> <p>value to add</p> required Source code in <code>velora/training/metrics.py</code> Python<pre><code>def add(self, value: torch.Tensor) -&gt; None:\n    \"\"\"\n    Adds a value and updates the window.\n\n    Parameters:\n        value (torch.Tensor): value to add\n    \"\"\"\n    self.window[self.position] = value.to(self.device)\n\n    # Update position - deque style\n    self.position = (self.position + 1) % self.window_size\n    self.size = min(self.size + 1, self.window_size)\n</code></pre>"},{"location":"learn/reference/training/#velora.training.MovingMetric.max","title":"<code>max()</code>","text":"<p>Calculates the maximum value of a set of values or the current window.</p> <p>Returns:</p> Name Type Description <code>max</code> <code>torch.Tensor</code> <p>the maximum value.</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def max(self) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the maximum value of a set of values or the current window.\n\n    Returns:\n        max (torch.Tensor): the maximum value.\n    \"\"\"\n    return self.window.max()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.MovingMetric.mean","title":"<code>mean()</code>","text":"<p>Calculates the mean of values or the current window.</p> <p>Returns:</p> Name Type Description <code>avg</code> <code>torch.Tensor</code> <p>the calculated mean.</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def mean(self) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the mean of values or the current window.\n\n    Returns:\n        avg (torch.Tensor): the calculated mean.\n    \"\"\"\n    return self.window.mean()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.MovingMetric.std","title":"<code>std()</code>","text":"<p>Calculates the standard deviation of values or the current window.</p> <p>Returns:</p> Name Type Description <code>std</code> <code>torch.Tensor</code> <p>the calculated standard deviation.</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def std(self) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the standard deviation of values or the current window.\n\n    Returns:\n        std (torch.Tensor): the calculated standard deviation.\n    \"\"\"\n    return (\n        self.window.std()\n        if self.window.size(dim=0) &gt; 1\n        else torch.tensor(0.0, device=self.device)\n    )\n</code></pre>"},{"location":"learn/reference/training/#velora.training.StepStorage","title":"<code>StepStorage</code>","text":"<p>A storage container for step metrics.</p> <p>Useful for calculating the episodic average values to store in <code>MetricStorage</code>.</p> <p>Attributes:</p> Name Type Description <code>critic_losses</code> <code>torch.Tensor</code> <p>a tensor of agent Critic loss values</p> <code>actor_losses</code> <code>torch.Tensor</code> <p>a tensor of agent Actor loss values</p> <code>entropy_losses</code> <code>torch.Tensor</code> <p>a tensor of agent Entropy loss values</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>class StepStorage:\n    \"\"\"\n    A storage container for step metrics.\n\n    Useful for calculating the episodic average values to store in `MetricStorage`.\n\n    Attributes:\n        critic_losses (torch.Tensor): a tensor of agent Critic loss values\n        actor_losses (torch.Tensor): a tensor of agent Actor loss values\n        entropy_losses (torch.Tensor): a tensor of agent Entropy loss values\n    \"\"\"\n\n    def __init__(self, capacity: int, *, device: torch.device | None = None) -&gt; None:\n        \"\"\"\n        Parameters:\n            capacity (int): storage capacity for each tensor\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        self.capacity = capacity\n        self.device = device\n\n        # Position indicators\n        self.position = 0\n        self.size = 0\n\n        self.critic_losses = torch.zeros((capacity), device=device)\n        self.actor_losses = torch.zeros((capacity), device=device)\n        self.entropy_losses = torch.zeros((capacity), device=device)\n\n    def critic_avg(self, ep_length: int) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the critic loss average. Useful for computing episodic averages.\n\n        Parameters:\n            ep_length (int): size of the episode\n\n        Returns:\n            avg (torch.Tensor): critic loss step average\n        \"\"\"\n        return self.critic_losses[:ep_length].mean()\n\n    def actor_avg(self, ep_length: int) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the actor loss average. Useful for computing episodic averages.\n\n        Parameters:\n            ep_length (int): size of the episode\n\n        Returns:\n            avg (torch.Tensor): actor loss step average\n        \"\"\"\n        return self.actor_losses[:ep_length].mean()\n\n    def entropy_avg(self, ep_length: int) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the entropy loss average. Useful for computing episodic averages.\n\n        Parameters:\n            ep_length (int): size of the episode\n\n        Returns:\n            avg (torch.Tensor): entropy loss step average\n        \"\"\"\n        return self.entropy_losses[:ep_length].mean()\n\n    def add(\n        self,\n        critic: torch.Tensor,\n        actor: torch.Tensor,\n        entropy: torch.Tensor,\n    ) -&gt; None:\n        \"\"\"\n        Adds one of each metric into storage.\n\n        Parameters:\n            critic (torch.Tensor): critic loss\n            actor (torch.Tensor): actor loss\n            entropy (torch.Tensor): entropy loss\n        \"\"\"\n        self.critic_losses[self.position] = critic\n        self.actor_losses[self.position] = actor\n        self.entropy_losses[self.position] = entropy\n\n        # Update position\n        self.position = (self.position + 1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty storage.\"\"\"\n        self.critic_losses.zero_()\n        self.actor_losses.zero_()\n        self.entropy_losses.zero_()\n\n        self.position = 0\n        self.size = 0\n</code></pre>"},{"location":"learn/reference/training/#velora.training.StepStorage.__init__","title":"<code>__init__(capacity, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>capacity</code> <code>int</code> <p>storage capacity for each tensor</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def __init__(self, capacity: int, *, device: torch.device | None = None) -&gt; None:\n    \"\"\"\n    Parameters:\n        capacity (int): storage capacity for each tensor\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    self.capacity = capacity\n    self.device = device\n\n    # Position indicators\n    self.position = 0\n    self.size = 0\n\n    self.critic_losses = torch.zeros((capacity), device=device)\n    self.actor_losses = torch.zeros((capacity), device=device)\n    self.entropy_losses = torch.zeros((capacity), device=device)\n</code></pre>"},{"location":"learn/reference/training/#velora.training.StepStorage.actor_avg","title":"<code>actor_avg(ep_length)</code>","text":"<p>Computes the actor loss average. Useful for computing episodic averages.</p> <p>Parameters:</p> Name Type Description Default <code>ep_length</code> <code>int</code> <p>size of the episode</p> required <p>Returns:</p> Name Type Description <code>avg</code> <code>torch.Tensor</code> <p>actor loss step average</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def actor_avg(self, ep_length: int) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the actor loss average. Useful for computing episodic averages.\n\n    Parameters:\n        ep_length (int): size of the episode\n\n    Returns:\n        avg (torch.Tensor): actor loss step average\n    \"\"\"\n    return self.actor_losses[:ep_length].mean()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.StepStorage.add","title":"<code>add(critic, actor, entropy)</code>","text":"<p>Adds one of each metric into storage.</p> <p>Parameters:</p> Name Type Description Default <code>critic</code> <code>torch.Tensor</code> <p>critic loss</p> required <code>actor</code> <code>torch.Tensor</code> <p>actor loss</p> required <code>entropy</code> <code>torch.Tensor</code> <p>entropy loss</p> required Source code in <code>velora/training/metrics.py</code> Python<pre><code>def add(\n    self,\n    critic: torch.Tensor,\n    actor: torch.Tensor,\n    entropy: torch.Tensor,\n) -&gt; None:\n    \"\"\"\n    Adds one of each metric into storage.\n\n    Parameters:\n        critic (torch.Tensor): critic loss\n        actor (torch.Tensor): actor loss\n        entropy (torch.Tensor): entropy loss\n    \"\"\"\n    self.critic_losses[self.position] = critic\n    self.actor_losses[self.position] = actor\n    self.entropy_losses[self.position] = entropy\n\n    # Update position\n    self.position = (self.position + 1) % self.capacity\n    self.size = min(self.size + 1, self.capacity)\n</code></pre>"},{"location":"learn/reference/training/#velora.training.StepStorage.critic_avg","title":"<code>critic_avg(ep_length)</code>","text":"<p>Computes the critic loss average. Useful for computing episodic averages.</p> <p>Parameters:</p> Name Type Description Default <code>ep_length</code> <code>int</code> <p>size of the episode</p> required <p>Returns:</p> Name Type Description <code>avg</code> <code>torch.Tensor</code> <p>critic loss step average</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def critic_avg(self, ep_length: int) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the critic loss average. Useful for computing episodic averages.\n\n    Parameters:\n        ep_length (int): size of the episode\n\n    Returns:\n        avg (torch.Tensor): critic loss step average\n    \"\"\"\n    return self.critic_losses[:ep_length].mean()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.StepStorage.empty","title":"<code>empty()</code>","text":"<p>Empty storage.</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty storage.\"\"\"\n    self.critic_losses.zero_()\n    self.actor_losses.zero_()\n    self.entropy_losses.zero_()\n\n    self.position = 0\n    self.size = 0\n</code></pre>"},{"location":"learn/reference/training/#velora.training.StepStorage.entropy_avg","title":"<code>entropy_avg(ep_length)</code>","text":"<p>Computes the entropy loss average. Useful for computing episodic averages.</p> <p>Parameters:</p> Name Type Description Default <code>ep_length</code> <code>int</code> <p>size of the episode</p> required <p>Returns:</p> Name Type Description <code>avg</code> <code>torch.Tensor</code> <p>entropy loss step average</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def entropy_avg(self, ep_length: int) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the entropy loss average. Useful for computing episodic averages.\n\n    Parameters:\n        ep_length (int): size of the episode\n\n    Returns:\n        avg (torch.Tensor): entropy loss step average\n    \"\"\"\n    return self.entropy_losses[:ep_length].mean()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainHandler","title":"<code>TrainHandler</code>","text":"<p>               Bases: <code>TrainHandlerBase</code></p> <p>A context manager for handling an agents training state. Compatible with single environments.</p> Source code in <code>velora/training/handler.py</code> Python<pre><code>class TrainHandler(TrainHandlerBase):\n    \"\"\"\n    A context manager for handling an agents training state. Compatible with single\n    environments.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: RLModuleAgent,\n        n_episodes: int,\n        max_steps: int,\n        log_freq: int,\n        window_size: int,\n        callbacks: List[\"TrainCallback\"] | None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            agent (RLModuleAgent): the agent being trained\n            n_episodes (int): the total number of training episodes\n            max_steps (int): maximum number of steps in an episode\n            log_freq (int): metric logging frequency (in episodes)\n            window_size (int): episode window size rate\n            callbacks (List[TrainCallback] | None): a list of training callbacks.\n                If `None` sets to an empty list\n        \"\"\"\n        super().__init__(agent, window_size, callbacks)\n\n        self.log_freq = log_freq\n        self.n_episodes = n_episodes\n        self.max_steps = max_steps\n\n    @property\n    def metrics(self) -&gt; TrainMetrics:\n        \"\"\"\n        Training metric class instance.\n\n        Returns:\n            metrics (TrainMetrics): current training metric state.\n        \"\"\"\n        return self._metrics\n\n    def __enter__(self) -&gt; Self:\n        \"\"\"\n        Setup the training context, initializing the environment.\n\n        Returns:\n            self (Self): the initialized context.\n        \"\"\"\n        self.session = Session(self.engine)\n        self._metrics = TrainMetrics(\n            self.session,\n            self.window_size,\n            self.n_episodes,\n            self.max_steps,\n            device=self.device,\n        )\n        self._metrics.start_experiment(self.agent.config)\n\n        self.state = TrainState(\n            agent=self.agent,\n            env=self.env,\n            session=self.session,\n            total_episodes=self.n_episodes,\n            experiment_id=self._metrics.experiment_id,\n        )\n\n        return super().__enter__()\n\n    def __exit__(\n        self,\n        exc_type: Type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ):\n        super().__exit__(exc_type, exc_val, exc_tb)\n\n        if self.state.saving_enabled:\n            self.save_completed()\n\n    def start(self) -&gt; None:\n        super().start()\n\n        # Update environment with callback wrappers\n        self.env = self.state.env\n\n    def step(self, current_step: int) -&gt; None:\n        \"\"\"\n        Performs `step` callback event.\n\n        Parameters:\n            current_step (int): the current training timestep index\n        \"\"\"\n        self.state.update(status=\"step\", current_step=current_step)\n        self._run_callbacks()\n\n    def log(self, idx: int, log_type: Literal[\"episode\", \"step\"]) -&gt; None:\n        \"\"\"\n        Performs `logging` callback event.\n\n        Parameters:\n            idx (int): the current training step or episode index\n            log_type (str): the type of logging method\n        \"\"\"\n        if log_type == \"episode\":\n            self.state.update(status=\"logging\", current_ep=idx, logging_type=log_type)\n        else:\n            self.state.update(status=\"logging\", current_step=idx, logging_type=log_type)\n\n        self._run_callbacks()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainHandler.metrics","title":"<code>metrics</code>  <code>property</code>","text":"<p>Training metric class instance.</p> <p>Returns:</p> Name Type Description <code>metrics</code> <code>TrainMetrics</code> <p>current training metric state.</p>"},{"location":"learn/reference/training/#velora.training.TrainHandler.__enter__","title":"<code>__enter__()</code>","text":"<p>Setup the training context, initializing the environment.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>the initialized context.</p> Source code in <code>velora/training/handler.py</code> Python<pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"\n    Setup the training context, initializing the environment.\n\n    Returns:\n        self (Self): the initialized context.\n    \"\"\"\n    self.session = Session(self.engine)\n    self._metrics = TrainMetrics(\n        self.session,\n        self.window_size,\n        self.n_episodes,\n        self.max_steps,\n        device=self.device,\n    )\n    self._metrics.start_experiment(self.agent.config)\n\n    self.state = TrainState(\n        agent=self.agent,\n        env=self.env,\n        session=self.session,\n        total_episodes=self.n_episodes,\n        experiment_id=self._metrics.experiment_id,\n    )\n\n    return super().__enter__()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainHandler.__init__","title":"<code>__init__(agent, n_episodes, max_steps, log_freq, window_size, callbacks)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>agent</code> <code>RLModuleAgent</code> <p>the agent being trained</p> required <code>n_episodes</code> <code>int</code> <p>the total number of training episodes</p> required <code>max_steps</code> <code>int</code> <p>maximum number of steps in an episode</p> required <code>log_freq</code> <code>int</code> <p>metric logging frequency (in episodes)</p> required <code>window_size</code> <code>int</code> <p>episode window size rate</p> required <code>callbacks</code> <code>List[TrainCallback] | None</code> <p>a list of training callbacks. If <code>None</code> sets to an empty list</p> required Source code in <code>velora/training/handler.py</code> Python<pre><code>def __init__(\n    self,\n    agent: RLModuleAgent,\n    n_episodes: int,\n    max_steps: int,\n    log_freq: int,\n    window_size: int,\n    callbacks: List[\"TrainCallback\"] | None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        agent (RLModuleAgent): the agent being trained\n        n_episodes (int): the total number of training episodes\n        max_steps (int): maximum number of steps in an episode\n        log_freq (int): metric logging frequency (in episodes)\n        window_size (int): episode window size rate\n        callbacks (List[TrainCallback] | None): a list of training callbacks.\n            If `None` sets to an empty list\n    \"\"\"\n    super().__init__(agent, window_size, callbacks)\n\n    self.log_freq = log_freq\n    self.n_episodes = n_episodes\n    self.max_steps = max_steps\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainHandler.log","title":"<code>log(idx, log_type)</code>","text":"<p>Performs <code>logging</code> callback event.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the current training step or episode index</p> required <code>log_type</code> <code>str</code> <p>the type of logging method</p> required Source code in <code>velora/training/handler.py</code> Python<pre><code>def log(self, idx: int, log_type: Literal[\"episode\", \"step\"]) -&gt; None:\n    \"\"\"\n    Performs `logging` callback event.\n\n    Parameters:\n        idx (int): the current training step or episode index\n        log_type (str): the type of logging method\n    \"\"\"\n    if log_type == \"episode\":\n        self.state.update(status=\"logging\", current_ep=idx, logging_type=log_type)\n    else:\n        self.state.update(status=\"logging\", current_step=idx, logging_type=log_type)\n\n    self._run_callbacks()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainHandler.step","title":"<code>step(current_step)</code>","text":"<p>Performs <code>step</code> callback event.</p> <p>Parameters:</p> Name Type Description Default <code>current_step</code> <code>int</code> <p>the current training timestep index</p> required Source code in <code>velora/training/handler.py</code> Python<pre><code>def step(self, current_step: int) -&gt; None:\n    \"\"\"\n    Performs `step` callback event.\n\n    Parameters:\n        current_step (int): the current training timestep index\n    \"\"\"\n    self.state.update(status=\"step\", current_step=current_step)\n    self._run_callbacks()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainMetrics","title":"<code>TrainMetrics</code>","text":"<p>               Bases: <code>TrainMetricsBase</code></p> <p>A utility class for working with and storing episodic training metrics for monitoring an agents training performance.</p> Source code in <code>velora/training/metrics.py</code> Python<pre><code>class TrainMetrics(TrainMetricsBase):\n    \"\"\"\n    A utility class for working with and storing episodic training metrics for\n    monitoring an agents training performance.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        window_size: int,\n        n_episodes: int,\n        max_steps: int,\n        *,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            session (sqlmodel.Session): current metric database session\n            window_size (int): moving average window size\n            n_episodes (int): total number of training episodes\n            max_steps (int): maximum number of steps per episode\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__(session, window_size, device=device)\n\n        self.n_episodes = n_episodes\n        self.max_steps = max_steps\n\n        self._current_losses = StepStorage(max_steps, device=device)\n\n    def add_step(\n        self,\n        critic: torch.Tensor,\n        actor: torch.Tensor,\n        entropy: torch.Tensor,\n    ) -&gt; None:\n        \"\"\"\n        Add timestep metrics to local storage.\n\n        Parameters:\n            critic (torch.Tensor): critic step loss\n            actor (torch.Tensor): actor step loss\n            entropy (torch.Tensor): entropy step loss\n        \"\"\"\n        self._exp_created_check()\n\n        self._current_losses.add(critic, actor, entropy)\n\n    def add_episode(\n        self,\n        ep_idx: int,\n        reward: torch.Tensor,\n        ep_length: torch.Tensor,\n    ) -&gt; None:\n        \"\"\"\n        Add episode metrics to the metric database and reset step accumulators.\n\n        Parameters:\n            ep_idx (int): the current episode index\n            reward (torch.Tensor): episode reward\n            ep_length (torch.Tensor): number of steps after episode done\n        \"\"\"\n        self._exp_created_check()\n\n        self._ep_rewards.add(reward.to(self.device))\n        self._ep_lengths.add(ep_length.to(self.device))\n\n        self._actor_loss = self._current_losses.actor_avg(ep_length.item())\n        self._critic_loss = self._current_losses.critic_avg(ep_length.item())\n        self._entropy_loss = self._current_losses.entropy_avg(ep_length.item())\n        self.step_total += ep_length\n\n        moving_avg = self.reward_moving_avg()\n        moving_std = self.reward_moving_std()\n\n        ep = Episode(\n            experiment_id=self.experiment_id,\n            episode_num=ep_idx,\n            reward=reward.item(),\n            length=ep_length.item(),\n            reward_moving_avg=moving_avg,\n            reward_moving_std=moving_std,\n            actor_loss=self._actor_loss.item(),\n            critic_loss=self._critic_loss.item(),\n            entropy_loss=self._entropy_loss.item(),\n        )\n        self.session.add(ep)\n        self.session.commit()\n\n        # Reset step storage\n        self._current_losses.empty()\n\n    def info(self, current_ep: int) -&gt; None:\n        \"\"\"\n        Outputs basic information to the console.\n\n        Parameters:\n            current_ep (int): the current episode index\n        \"\"\"\n        ep = number_to_short(current_ep)\n        max_eps = number_to_short(self.n_episodes)\n\n        ep_length = number_to_short(int(self._ep_lengths.latest))\n        step_total = number_to_short(self.step_total.item())\n\n        max_length = number_to_short(int(self._ep_lengths.max().item()))\n        max_steps = number_to_short(self.max_steps)\n\n        print(\n            f\"Episode: {ep}/{max_eps}, \"\n            f\"Steps: {ep_length}/{step_total}, \"\n            f\"Max Length: {max_length}/{max_steps}, \"\n            f\"Reward Avg: {self.reward_moving_avg():.2f}, \"\n            f\"Reward Max: {self.reward_moving_max():.2f}, \"\n            f\"Actor Loss: {self._actor_loss.item():.2f}, \"\n            f\"Critic Loss: {self._critic_loss.item():.2f}, \"\n            f\"Entropy Loss: {self._entropy_loss.item():.2f}\"\n        )\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainMetrics.__init__","title":"<code>__init__(session, window_size, n_episodes, max_steps, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>session</code> <code>sqlmodel.Session</code> <p>current metric database session</p> required <code>window_size</code> <code>int</code> <p>moving average window size</p> required <code>n_episodes</code> <code>int</code> <p>total number of training episodes</p> required <code>max_steps</code> <code>int</code> <p>maximum number of steps per episode</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/training/metrics.py</code> Python<pre><code>def __init__(\n    self,\n    session: Session,\n    window_size: int,\n    n_episodes: int,\n    max_steps: int,\n    *,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        session (sqlmodel.Session): current metric database session\n        window_size (int): moving average window size\n        n_episodes (int): total number of training episodes\n        max_steps (int): maximum number of steps per episode\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__(session, window_size, device=device)\n\n    self.n_episodes = n_episodes\n    self.max_steps = max_steps\n\n    self._current_losses = StepStorage(max_steps, device=device)\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainMetrics.add_episode","title":"<code>add_episode(ep_idx, reward, ep_length)</code>","text":"<p>Add episode metrics to the metric database and reset step accumulators.</p> <p>Parameters:</p> Name Type Description Default <code>ep_idx</code> <code>int</code> <p>the current episode index</p> required <code>reward</code> <code>torch.Tensor</code> <p>episode reward</p> required <code>ep_length</code> <code>torch.Tensor</code> <p>number of steps after episode done</p> required Source code in <code>velora/training/metrics.py</code> Python<pre><code>def add_episode(\n    self,\n    ep_idx: int,\n    reward: torch.Tensor,\n    ep_length: torch.Tensor,\n) -&gt; None:\n    \"\"\"\n    Add episode metrics to the metric database and reset step accumulators.\n\n    Parameters:\n        ep_idx (int): the current episode index\n        reward (torch.Tensor): episode reward\n        ep_length (torch.Tensor): number of steps after episode done\n    \"\"\"\n    self._exp_created_check()\n\n    self._ep_rewards.add(reward.to(self.device))\n    self._ep_lengths.add(ep_length.to(self.device))\n\n    self._actor_loss = self._current_losses.actor_avg(ep_length.item())\n    self._critic_loss = self._current_losses.critic_avg(ep_length.item())\n    self._entropy_loss = self._current_losses.entropy_avg(ep_length.item())\n    self.step_total += ep_length\n\n    moving_avg = self.reward_moving_avg()\n    moving_std = self.reward_moving_std()\n\n    ep = Episode(\n        experiment_id=self.experiment_id,\n        episode_num=ep_idx,\n        reward=reward.item(),\n        length=ep_length.item(),\n        reward_moving_avg=moving_avg,\n        reward_moving_std=moving_std,\n        actor_loss=self._actor_loss.item(),\n        critic_loss=self._critic_loss.item(),\n        entropy_loss=self._entropy_loss.item(),\n    )\n    self.session.add(ep)\n    self.session.commit()\n\n    # Reset step storage\n    self._current_losses.empty()\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainMetrics.add_step","title":"<code>add_step(critic, actor, entropy)</code>","text":"<p>Add timestep metrics to local storage.</p> <p>Parameters:</p> Name Type Description Default <code>critic</code> <code>torch.Tensor</code> <p>critic step loss</p> required <code>actor</code> <code>torch.Tensor</code> <p>actor step loss</p> required <code>entropy</code> <code>torch.Tensor</code> <p>entropy step loss</p> required Source code in <code>velora/training/metrics.py</code> Python<pre><code>def add_step(\n    self,\n    critic: torch.Tensor,\n    actor: torch.Tensor,\n    entropy: torch.Tensor,\n) -&gt; None:\n    \"\"\"\n    Add timestep metrics to local storage.\n\n    Parameters:\n        critic (torch.Tensor): critic step loss\n        actor (torch.Tensor): actor step loss\n        entropy (torch.Tensor): entropy step loss\n    \"\"\"\n    self._exp_created_check()\n\n    self._current_losses.add(critic, actor, entropy)\n</code></pre>"},{"location":"learn/reference/training/#velora.training.TrainMetrics.info","title":"<code>info(current_ep)</code>","text":"<p>Outputs basic information to the console.</p> <p>Parameters:</p> Name Type Description Default <code>current_ep</code> <code>int</code> <p>the current episode index</p> required Source code in <code>velora/training/metrics.py</code> Python<pre><code>def info(self, current_ep: int) -&gt; None:\n    \"\"\"\n    Outputs basic information to the console.\n\n    Parameters:\n        current_ep (int): the current episode index\n    \"\"\"\n    ep = number_to_short(current_ep)\n    max_eps = number_to_short(self.n_episodes)\n\n    ep_length = number_to_short(int(self._ep_lengths.latest))\n    step_total = number_to_short(self.step_total.item())\n\n    max_length = number_to_short(int(self._ep_lengths.max().item()))\n    max_steps = number_to_short(self.max_steps)\n\n    print(\n        f\"Episode: {ep}/{max_eps}, \"\n        f\"Steps: {ep_length}/{step_total}, \"\n        f\"Max Length: {max_length}/{max_steps}, \"\n        f\"Reward Avg: {self.reward_moving_avg():.2f}, \"\n        f\"Reward Max: {self.reward_moving_max():.2f}, \"\n        f\"Actor Loss: {self._actor_loss.item():.2f}, \"\n        f\"Critic Loss: {self._critic_loss.item():.2f}, \"\n        f\"Entropy Loss: {self._entropy_loss.item():.2f}\"\n    )\n</code></pre>"},{"location":"learn/reference/utils/","title":"velora.utils","text":"Documentation <p>User Guide - Tutorials: Utilities</p> <p>Generic utility methods usable in any experiment.</p>"},{"location":"learn/reference/utils/#velora.utils.core.set_device","title":"<code>set_device(device='auto')</code>","text":"<p>Sets the <code>PyTorch</code> device dynamically.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>the name of the device to perform computations on.</p> <p>When <code>auto</code>:</p> <ul> <li>Set to <code>cuda:0</code>, if available.</li> <li>Else, <code>cpu</code>.</li> </ul> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>device</code> <code>torch.device</code> <p>the <code>PyTorch</code> device.</p> Source code in <code>velora/utils/core.py</code> Python<pre><code>def set_device(device: str = \"auto\") -&gt; torch.device:\n    \"\"\"\n    Sets the `PyTorch` device dynamically.\n\n    Parameters:\n        device (str, optional): the name of the device to perform computations on.\n\n            When `auto`:\n\n            - Set to `cuda:0`, if available.\n            - Else, `cpu`.\n\n    Returns:\n        device (torch.device): the `PyTorch` device.\n    \"\"\"\n    if device == \"auto\":\n        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n    return torch.device(device)\n</code></pre>"},{"location":"learn/reference/utils/#velora.utils.core.set_seed","title":"<code>set_seed(value=None)</code>","text":"<p>Sets the random seed for <code>Python</code>, <code>PyTorch</code> and <code>NumPy</code>. When <code>None</code> will create a new one automatically.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>the seed value</p> <code>None</code> <p>Returns:</p> Name Type Description <code>seed</code> <code>int</code> <p>the used seed value</p> Source code in <code>velora/utils/core.py</code> Python<pre><code>def set_seed(value: int | None = None) -&gt; int:\n    \"\"\"\n    Sets the random seed for `Python`, `PyTorch` and `NumPy`.\n    When `None` will create a new one automatically.\n\n    Parameters:\n        value (int, optional): the seed value\n\n    Returns:\n        seed (int): the used seed value\n    \"\"\"\n    if value is None:\n        value = random.randint(0, 2**32 - 1)\n\n    random.seed(value)\n    torch.manual_seed(value)\n    np.random.seed(value)\n\n    return value\n</code></pre>"},{"location":"learn/reference/utils/#velora.utils.torch.active_parameters","title":"<code>active_parameters(model)</code>","text":"<p>Calculates the active number of parameters used in a PyTorch <code>nn.Module</code>. Filters out parameters that are <code>0</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>a PyTorch module with parameters</p> required <p>Returns:</p> Name Type Description <code>count</code> <code>int</code> <p>the total active number of parameters.</p> Source code in <code>velora/utils/torch.py</code> Python<pre><code>@torch.jit.ignore\ndef active_parameters(model: nn.Module) -&gt; int:\n    \"\"\"\n    Calculates the active number of parameters used in a PyTorch `nn.Module`.\n    Filters out parameters that are `0`.\n\n    Parameters:\n        model (nn.Module): a PyTorch module with parameters\n\n    Returns:\n        count (int): the total active number of parameters.\n    \"\"\"\n    return sum((p != 0).sum().item() for p in model.parameters() if p.requires_grad)\n</code></pre>"},{"location":"learn/reference/utils/#velora.utils.torch.hard_update","title":"<code>hard_update(source, target)</code>","text":"<p>Performs a hard parameter update between two PyTorch Networks.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>nn.Module</code> <p>the source network</p> required <code>target</code> <code>nn.Module</code> <p>the target network</p> required Source code in <code>velora/utils/torch.py</code> Python<pre><code>def hard_update(source: nn.Module, target: nn.Module) -&gt; None:\n    \"\"\"\n    Performs a hard parameter update between two PyTorch Networks.\n\n    Parameters:\n        source (nn.Module): the source network\n        target (nn.Module): the target network\n    \"\"\"\n    for target_param, param in zip(target.parameters(), source.parameters()):\n        target_param.data.copy_(param.data)\n</code></pre>"},{"location":"learn/reference/utils/#velora.utils.torch.soft_update","title":"<code>soft_update(source, target, *, tau=0.005)</code>","text":"<p>Performs a soft parameter update between two PyTorch Networks.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>nn.Module</code> <p>the source network</p> required <code>target</code> <code>nn.Module</code> <p>the target network</p> required <code>tau</code> <code>float</code> <p>the soft update factor used to slowly update the target network</p> <code>0.005</code> Source code in <code>velora/utils/torch.py</code> Python<pre><code>def soft_update(source: nn.Module, target: nn.Module, *, tau: float = 0.005) -&gt; None:\n    \"\"\"\n    Performs a soft parameter update between two PyTorch Networks.\n\n    Parameters:\n        source (nn.Module): the source network\n        target (nn.Module): the target network\n        tau (float, optional): the soft update factor used to slowly update\n            the target network\n    \"\"\"\n    for target_param, param in zip(target.parameters(), source.parameters()):\n        target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n</code></pre>"},{"location":"learn/reference/utils/#velora.utils.torch.stack_tensor","title":"<code>stack_tensor(items, *, dtype=torch.float32, device=None)</code>","text":"<p>Stacks a list of tensors together, then:</p> <ol> <li>Converts it to a specific <code>dtype</code></li> <li>Loads it onto <code>device</code></li> </ol> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[torch.Tensor]</code> <p>a list of torch.Tensors full of items</p> required <code>dtype</code> <code>torch.dtype</code> <p>the data type for the tensor</p> <code>torch.float32</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tensor</code> <code>torch.Tensor</code> <p>the updated <code>torch.Tensor</code>.</p> Source code in <code>velora/utils/torch.py</code> Python<pre><code>def stack_tensor(\n    items: List[torch.Tensor],\n    *,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Stacks a list of tensors together, then:\n\n    1. Converts it to a specific `dtype`\n    2. Loads it onto `device`\n\n    Parameters:\n        items (List[torch.Tensor]): a list of torch.Tensors full of items\n        dtype (torch.dtype, optional): the data type for the tensor\n        device (torch.device, optional): the device to perform computations on\n\n    Returns:\n        tensor (torch.Tensor): the updated `torch.Tensor`.\n    \"\"\"\n    return torch.stack(items).to(dtype=dtype, device=device)\n</code></pre>"},{"location":"learn/reference/utils/#velora.utils.torch.summary","title":"<code>summary(module)</code>","text":"<p>Outputs a summary of a module and all it's sub-modules as a dictionary.</p> <p>Returns:</p> Name Type Description <code>summary</code> <code>Dict[str, str]</code> <p>key-value pairs for the network layout.</p> Source code in <code>velora/utils/torch.py</code> Python<pre><code>@torch.jit.ignore\ndef summary(module: nn.Module) -&gt; Dict[str, str]:\n    \"\"\"\n    Outputs a summary of a module and all it's sub-modules as a dictionary.\n\n    Returns:\n        summary (Dict[str, str]): key-value pairs for the network layout.\n    \"\"\"\n    model_dict = {}\n\n    for name, mod in module.named_children():\n        if len(list(mod.children())) &gt; 0:\n            # If the module has submodules, recurse\n            model_dict[name] = summary(mod)\n        else:\n            # If it's a leaf module, store its string representation\n            model_dict[name] = str(mod)\n\n    return model_dict\n</code></pre>"},{"location":"learn/reference/utils/#velora.utils.torch.to_tensor","title":"<code>to_tensor(items, *, dtype=torch.float32, device=None)</code>","text":"<p>Converts a list of items to a Tensor, then:</p> <ol> <li>Converts it to a specific <code>dtype</code></li> <li>Loads it onto <code>device</code></li> </ol> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[Any]</code> <p>a list of items of any type</p> required <code>dtype</code> <code>torch.dtype</code> <p>the data type for the tensor</p> <code>torch.float32</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tensor</code> <code>torch.Tensor</code> <p>the updated <code>torch.Tensor</code>.</p> Source code in <code>velora/utils/torch.py</code> Python<pre><code>def to_tensor(\n    items: List[Any],\n    *,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Converts a list of items to a Tensor, then:\n\n    1. Converts it to a specific `dtype`\n    2. Loads it onto `device`\n\n    Parameters:\n        items (List[Any]): a list of items of any type\n        dtype (torch.dtype, optional): the data type for the tensor\n        device (torch.device, optional): the device to perform computations on\n\n    Returns:\n        tensor (torch.Tensor): the updated `torch.Tensor`.\n    \"\"\"\n    return torch.tensor(items).to(dtype=dtype, device=device)\n</code></pre>"},{"location":"learn/reference/utils/#velora.utils.torch.total_parameters","title":"<code>total_parameters(model)</code>","text":"<p>Calculates the total number of parameters used in a PyTorch <code>nn.Module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>a PyTorch module with parameters</p> required <p>Returns:</p> Name Type Description <code>count</code> <code>int</code> <p>the total number of parameters.</p> Source code in <code>velora/utils/torch.py</code> Python<pre><code>@torch.jit.ignore\ndef total_parameters(model: nn.Module) -&gt; int:\n    \"\"\"\n    Calculates the total number of parameters used in a PyTorch `nn.Module`.\n\n    Parameters:\n        model (nn.Module): a PyTorch module with parameters\n\n    Returns:\n        count (int): the total number of parameters.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n</code></pre>"},{"location":"learn/reference/wiring/","title":"velora.wiring","text":"Documentation <p>Customization: Wiring</p> <p>The secret sauce to the sparse neuron connections.</p>"},{"location":"learn/reference/wiring/#velora.wiring.LayerMasks","title":"<code>LayerMasks</code>  <code>dataclass</code>","text":"<p>Storage container for layer masks.</p> <p>Parameters:</p> Name Type Description Default <code>inter</code> <code>torch.Tensor</code> <p>sparse weight mask for input layer</p> required <code>command</code> <code>torch.Tensor</code> <p>sparse weight mask for hidden layer</p> required <code>motor</code> <code>torch.Tensor</code> <p>sparse weight mask for output layer</p> required <code>recurrent</code> <code>torch.Tensor</code> <p>sparse weight mask for recurrent connections</p> required Source code in <code>velora/wiring.py</code> Python<pre><code>@dataclass\nclass LayerMasks:\n    \"\"\"\n    Storage container for layer masks.\n\n    Parameters:\n        inter (torch.Tensor): sparse weight mask for input layer\n        command (torch.Tensor): sparse weight mask for hidden layer\n        motor (torch.Tensor): sparse weight mask for output layer\n        recurrent (torch.Tensor): sparse weight mask for recurrent connections\n    \"\"\"\n\n    inter: torch.Tensor\n    command: torch.Tensor\n    motor: torch.Tensor\n    recurrent: torch.Tensor\n</code></pre>"},{"location":"learn/reference/wiring/#velora.wiring.NeuronCounts","title":"<code>NeuronCounts</code>  <code>dataclass</code>","text":"<p>Storage container for NCP neuron category counts.</p> <p>Parameters:</p> Name Type Description Default <code>sensory</code> <code>int</code> <p>number of input nodes</p> required <code>inter</code> <code>int</code> <p>number of decision nodes</p> required <code>command</code> <code>int</code> <p>number of high-level decision nodes</p> required <code>motor</code> <code>int</code> <p>number of output nodes</p> required Source code in <code>velora/wiring.py</code> Python<pre><code>@dataclass\nclass NeuronCounts:\n    \"\"\"\n    Storage container for NCP neuron category counts.\n\n    Parameters:\n        sensory (int): number of input nodes\n        inter (int): number of decision nodes\n        command (int): number of high-level decision nodes\n        motor (int): number of output nodes\n    \"\"\"\n\n    sensory: int\n    inter: int\n    command: int\n    motor: int\n</code></pre>"},{"location":"learn/reference/wiring/#velora.wiring.SynapseCounts","title":"<code>SynapseCounts</code>  <code>dataclass</code>","text":"<p>Storage container for NCP neuron synapse connection counts.</p> <p>Parameters:</p> Name Type Description Default <code>sensory</code> <code>int</code> <p>number of connections for input nodes</p> required <code>inter</code> <code>int</code> <p>number of connections for decision nodes</p> required <code>command</code> <code>int</code> <p>number of connections for high-level decision nodes</p> required <code>motor</code> <code>int</code> <p>number of connections for output nodes</p> required Source code in <code>velora/wiring.py</code> Python<pre><code>@dataclass\nclass SynapseCounts:\n    \"\"\"\n    Storage container for NCP neuron synapse connection counts.\n\n    Parameters:\n        sensory (int): number of connections for input nodes\n        inter (int): number of connections for decision nodes\n        command (int): number of connections for high-level decision nodes\n        motor (int): number of connections for output nodes\n    \"\"\"\n\n    sensory: int\n    inter: int\n    command: int\n    motor: int\n</code></pre>"},{"location":"learn/reference/wiring/#velora.wiring.Wiring","title":"<code>Wiring</code>","text":"<p>Creates sparse wiring masks for Neural Circuit Policy (NCP) Networks.</p> <p>Note</p> <p>NCPs have three layers:</p> <ol> <li>Inter (input)</li> <li>Command (hidden)</li> <li>Motor (output)</li> </ol> Source code in <code>velora/wiring.py</code> Python<pre><code>class Wiring:\n    \"\"\"\n    Creates sparse wiring masks for Neural Circuit Policy (NCP) Networks.\n\n    !!! note\n\n        NCPs have three layers:\n\n        1. Inter (input)\n        2. Command (hidden)\n        3. Motor (output)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        n_neurons: int,\n        out_features: int,\n        *,\n        sparsity_level: float = 0.5,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            in_features (int): number of inputs (sensory nodes)\n            n_neurons (int): number of decision nodes (inter and command nodes)\n            out_features (int): number of outputs (motor nodes)\n            sparsity_level (float, optional): controls the connection sparsity between neurons.\n\n                Must be a value between `[0.1, 0.9]` -\n\n                - When `0.1` neurons are very dense.\n                - When `0.9` neurons are very sparse.\n        \"\"\"\n        if sparsity_level &lt; 0.1 or sparsity_level &gt; 0.9:\n            raise ValueError(f\"'{sparsity_level=}' must be between '[0.1, 0.9]'.\")\n\n        self.density_level = 1.0 - sparsity_level\n\n        self.n_command = max(int(0.4 * n_neurons), 1)\n        self.n_inter = n_neurons - self.n_command\n\n        self.counts, self._n_connections = self._set_counts(\n            in_features,\n            out_features,\n        )\n        self.masks = self._init_masks(in_features)\n\n        self.build()\n\n    @property\n    def n_connections(self) -&gt; SynapseCounts:\n        \"\"\"\n        Neuron connection counts.\n\n        Returns:\n            connections (SynapseCounts): object containing neuron connection counts.\n        \"\"\"\n        return self._n_connections\n\n    def _init_masks(self, n_inputs: int) -&gt; LayerMasks:\n        \"\"\"\n        Helper method. Initializes all layer masks with zeros\n        and stores them in a container.\n\n        Parameters:\n            n_inputs (int): the number of input nodes in the layer\n\n        Returns:\n            masks (LayerMasks): initialized layer masks.\n        \"\"\"\n        return LayerMasks(\n            inter=torch.zeros(\n                (n_inputs, self.counts.inter),\n                dtype=torch.int32,\n            ),\n            command=torch.zeros(\n                (self.counts.inter, self.counts.command),\n                dtype=torch.int32,\n            ),\n            motor=torch.zeros(\n                (self.counts.command, self.counts.motor),\n                dtype=torch.int32,\n            ),\n            recurrent=torch.zeros(\n                (self.counts.command, self.counts.command),\n                dtype=torch.int32,\n            ),\n        )\n\n    def _synapse_count(self, count: int, scale: int = 1) -&gt; int:\n        \"\"\"\n        Helper method. Computes the synapse count for a single layer.\n\n        Parameters:\n            count (int): the number of neurons\n            scale (int, optional): a scale factor\n\n        Returns:\n            count (int): synapse count.\n        \"\"\"\n        return max(int(count * self.density_level * scale), 1)\n\n    def _set_counts(\n        self, in_features: int, out_features: int\n    ) -&gt; Tuple[NeuronCounts, SynapseCounts]:\n        \"\"\"\n        Helper method. Computes the node layer and connection counts.\n\n        Parameters:\n            in_features (int): number of network input nodes\n            out_features (int): number of network output nodes\n\n        Returns:\n            neuron_counts (NeuronCounts): object with neuron counts.\n            synapse_counts (SynapseCounts): object with synapse connection counts.\n        \"\"\"\n        counts = NeuronCounts(\n            sensory=in_features,\n            inter=self.n_inter,\n            command=self.n_command,\n            motor=out_features,\n        )\n\n        connections = SynapseCounts(\n            sensory=self._synapse_count(self.n_inter),\n            inter=self._synapse_count(self.n_command),\n            command=self._synapse_count(self.n_command, scale=2),\n            motor=self._synapse_count(self.n_command),\n        )\n\n        return counts, connections\n\n    @staticmethod\n    def polarity(shape: Tuple[int, ...] = (1,)) -&gt; torch.IntTensor:\n        \"\"\"\n        Utility method. Randomly selects a polarity of `-1` or `1`, `n` times\n        based on shape.\n\n        Parameters:\n            shape (Tuple[int, ...]): size of the polarity matrix to generate.\n\n        Returns:\n            matrix (torch.Tensor): a polarity matrix filled with `-1` and `1`.\n        \"\"\"\n        return torch.IntTensor(np.random.choice([-1, 1], shape))\n\n    def _build_connections(self, mask: torch.Tensor, count: int) -&gt; torch.Tensor:\n        \"\"\"\n        Helper method. Randomly assigns connections to a set of nodes by populating\n        its mask.\n\n        !!! note \"Performs two operations\"\n\n            1. Applies minimum connections (count) to all nodes.\n            2. Checks all nodes have at least 1 connection.\n                If not, adds a connection to 'missing' nodes.\n\n        Parameters:\n            mask (torch.Tensor): the initialized mask\n            count (int): the number of connections per node\n\n        Examples:\n            Given 2 sensory (input) nodes and 5 inter neurons, we can define\n            our first layer (inter) mask as:\n\n            ```python\n            import torch\n\n            inter_mask = torch.zeros((2, 5), dtype=torch.int32)\n            n_connections = 2\n\n            inter_mask = wiring._build_connections(inter_mask, n_connections)\n\n            # tensor([[-1,  1,  0,  0,  1],\n            #         [ 0,  0, -1, -1,  0]], dtype=torch.int32)\n            ```\n\n        Returns:\n            mask (torch.Tensor): updated layer sparsity mask.\n        \"\"\"\n        num_nodes, num_cols = mask.shape\n\n        # Add required connection count\n        col_indices = torch.IntTensor(\n            np.random.choice(num_cols, (num_nodes, count)),\n        )\n        polarities = self.polarity(col_indices.shape)\n        row_indices = torch.arange(num_nodes).unsqueeze(1)\n\n        mask[row_indices, col_indices] = polarities\n\n        # Add missing node connections (if applicable)\n        # -&gt; Every node in 'num_cols' must have at least 1 connection\n        # -&gt; Column with all 0s = non-connected node\n        is_col_all_zero = (mask == 0).all(dim=0)\n        col_zero_indices = torch.nonzero(is_col_all_zero, as_tuple=True)[0]\n        zero_count = col_zero_indices.numel()\n\n        if zero_count &gt; 0:\n            # For each missing connection, randomly select a node and add connection\n            # -&gt; row = node\n            row_indices = torch.randint(0, num_nodes, (zero_count,))\n            random_polarities = self.polarity((zero_count,))\n            mask[row_indices, col_zero_indices] = random_polarities\n\n        return mask\n\n    def _build_recurrent_connections(\n        self, array: torch.Tensor, count: int\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Utility method. Adds recurrent connections to a set of nodes.\n\n        Used to simulate bidirectional connections between command neurons. Strictly\n        used for visualization purposes.\n\n        Parameters:\n            array (torch.Tensor): an initialized matrix to update\n            count (int): total number of connections to add\n\n        Returns:\n            matrix (torch.Tensor): the updated matrix.\n        \"\"\"\n        n_nodes = array.shape[0]\n\n        src = np.random.choice(n_nodes, count)\n        dest = np.random.choice(n_nodes, count)\n        polarities = self.polarity((count,))\n\n        array[src, dest] = polarities\n        return array\n\n    def build(self) -&gt; None:\n        \"\"\"\n        Builds the mask wiring for each layer.\n\n        !!! note \"Layer format\"\n\n            Follows a three layer format, each with separate masks:\n\n            1. Sensory -&gt; inter\n            2. Inter -&gt; command\n            3. Command -&gt; motor\n\n        Plus, command recurrent connections for ODE solvers.\n        \"\"\"\n        # Sensory -&gt; inter\n        self.masks.inter = self._build_connections(\n            self.masks.inter,\n            self._n_connections.sensory,\n        )\n        # Inter -&gt; command\n        self.masks.command = self._build_connections(\n            self.masks.command,\n            self._n_connections.inter,\n        )\n        # Command -&gt; motor\n        self.masks.motor = self._build_connections(\n            self.masks.motor,\n            self._n_connections.command,\n        )\n\n        # Command -&gt; command\n        self.masks.recurrent = self._build_recurrent_connections(\n            self.masks.recurrent,\n            self._n_connections.command,\n        )\n\n    def data(self) -&gt; Tuple[LayerMasks, NeuronCounts]:\n        \"\"\"\n        Retrieves wiring storage containers for layer masks and node counts.\n\n        Returns:\n            masks (LayerMasks): the object containing layer masks.\n            counts (NeuronCounts): the object containing node counts.\n        \"\"\"\n        return self.masks, self.counts\n</code></pre>"},{"location":"learn/reference/wiring/#velora.wiring.Wiring.n_connections","title":"<code>n_connections</code>  <code>property</code>","text":"<p>Neuron connection counts.</p> <p>Returns:</p> Name Type Description <code>connections</code> <code>SynapseCounts</code> <p>object containing neuron connection counts.</p>"},{"location":"learn/reference/wiring/#velora.wiring.Wiring.__init__","title":"<code>__init__(in_features, n_neurons, out_features, *, sparsity_level=0.5)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>number of inputs (sensory nodes)</p> required <code>n_neurons</code> <code>int</code> <p>number of decision nodes (inter and command nodes)</p> required <code>out_features</code> <code>int</code> <p>number of outputs (motor nodes)</p> required <code>sparsity_level</code> <code>float</code> <p>controls the connection sparsity between neurons.</p> <p>Must be a value between <code>[0.1, 0.9]</code> -</p> <ul> <li>When <code>0.1</code> neurons are very dense.</li> <li>When <code>0.9</code> neurons are very sparse.</li> </ul> <code>0.5</code> Source code in <code>velora/wiring.py</code> Python<pre><code>def __init__(\n    self,\n    in_features: int,\n    n_neurons: int,\n    out_features: int,\n    *,\n    sparsity_level: float = 0.5,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        in_features (int): number of inputs (sensory nodes)\n        n_neurons (int): number of decision nodes (inter and command nodes)\n        out_features (int): number of outputs (motor nodes)\n        sparsity_level (float, optional): controls the connection sparsity between neurons.\n\n            Must be a value between `[0.1, 0.9]` -\n\n            - When `0.1` neurons are very dense.\n            - When `0.9` neurons are very sparse.\n    \"\"\"\n    if sparsity_level &lt; 0.1 or sparsity_level &gt; 0.9:\n        raise ValueError(f\"'{sparsity_level=}' must be between '[0.1, 0.9]'.\")\n\n    self.density_level = 1.0 - sparsity_level\n\n    self.n_command = max(int(0.4 * n_neurons), 1)\n    self.n_inter = n_neurons - self.n_command\n\n    self.counts, self._n_connections = self._set_counts(\n        in_features,\n        out_features,\n    )\n    self.masks = self._init_masks(in_features)\n\n    self.build()\n</code></pre>"},{"location":"learn/reference/wiring/#velora.wiring.Wiring.build","title":"<code>build()</code>","text":"<p>Builds the mask wiring for each layer.</p> <p>Layer format</p> <p>Follows a three layer format, each with separate masks:</p> <ol> <li>Sensory -&gt; inter</li> <li>Inter -&gt; command</li> <li>Command -&gt; motor</li> </ol> <p>Plus, command recurrent connections for ODE solvers.</p> Source code in <code>velora/wiring.py</code> Python<pre><code>def build(self) -&gt; None:\n    \"\"\"\n    Builds the mask wiring for each layer.\n\n    !!! note \"Layer format\"\n\n        Follows a three layer format, each with separate masks:\n\n        1. Sensory -&gt; inter\n        2. Inter -&gt; command\n        3. Command -&gt; motor\n\n    Plus, command recurrent connections for ODE solvers.\n    \"\"\"\n    # Sensory -&gt; inter\n    self.masks.inter = self._build_connections(\n        self.masks.inter,\n        self._n_connections.sensory,\n    )\n    # Inter -&gt; command\n    self.masks.command = self._build_connections(\n        self.masks.command,\n        self._n_connections.inter,\n    )\n    # Command -&gt; motor\n    self.masks.motor = self._build_connections(\n        self.masks.motor,\n        self._n_connections.command,\n    )\n\n    # Command -&gt; command\n    self.masks.recurrent = self._build_recurrent_connections(\n        self.masks.recurrent,\n        self._n_connections.command,\n    )\n</code></pre>"},{"location":"learn/reference/wiring/#velora.wiring.Wiring.data","title":"<code>data()</code>","text":"<p>Retrieves wiring storage containers for layer masks and node counts.</p> <p>Returns:</p> Name Type Description <code>masks</code> <code>LayerMasks</code> <p>the object containing layer masks.</p> <code>counts</code> <code>NeuronCounts</code> <p>the object containing node counts.</p> Source code in <code>velora/wiring.py</code> Python<pre><code>def data(self) -&gt; Tuple[LayerMasks, NeuronCounts]:\n    \"\"\"\n    Retrieves wiring storage containers for layer masks and node counts.\n\n    Returns:\n        masks (LayerMasks): the object containing layer masks.\n        counts (NeuronCounts): the object containing node counts.\n    \"\"\"\n    return self.masks, self.counts\n</code></pre>"},{"location":"learn/reference/wiring/#velora.wiring.Wiring.polarity","title":"<code>polarity(shape=(1,))</code>  <code>staticmethod</code>","text":"<p>Utility method. Randomly selects a polarity of <code>-1</code> or <code>1</code>, <code>n</code> times based on shape.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple[int, ...]</code> <p>size of the polarity matrix to generate.</p> <code>(1,)</code> <p>Returns:</p> Name Type Description <code>matrix</code> <code>torch.Tensor</code> <p>a polarity matrix filled with <code>-1</code> and <code>1</code>.</p> Source code in <code>velora/wiring.py</code> Python<pre><code>@staticmethod\ndef polarity(shape: Tuple[int, ...] = (1,)) -&gt; torch.IntTensor:\n    \"\"\"\n    Utility method. Randomly selects a polarity of `-1` or `1`, `n` times\n    based on shape.\n\n    Parameters:\n        shape (Tuple[int, ...]): size of the polarity matrix to generate.\n\n    Returns:\n        matrix (torch.Tensor): a polarity matrix filled with `-1` and `1`.\n    \"\"\"\n    return torch.IntTensor(np.random.choice([-1, 1], shape))\n</code></pre>"},{"location":"learn/reference/models/activation/","title":"velora.models.activation","text":"<p>Utility methods for activation functions.</p>"},{"location":"learn/reference/models/activation/#velora.models.activation.ActivationEnum","title":"<code>ActivationEnum</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An Enum for PyTorch activation functions.</p> <p>Useful for getting activation functions dynamically using a <code>string</code> name. Refer to the <code>get()</code> method for more details.</p> Source code in <code>velora/models/activation.py</code> Python<pre><code>class ActivationEnum(Enum):\n    \"\"\"\n    An Enum for PyTorch activation functions.\n\n    Useful for getting activation functions dynamically using a `string` name.\n    Refer to the `get()` method for more details.\n    \"\"\"\n\n    RELU = nn.ReLU()\n    TANH = nn.Tanh()\n    ELU = nn.ELU()\n    LEAKY_RELU = nn.LeakyReLU()\n    PRELU = nn.PReLU()\n    SELU = nn.SELU()\n    SILU = nn.GELU()\n    SOFTSIGN = nn.Softsign()\n    SIGMOID = nn.Sigmoid()\n    HARDSIGMOID = nn.Hardsigmoid()\n    LECUN_TANH = LeCunTanh()\n\n    @classmethod\n    def get(cls, name: ActivationTypeLiteral) -&gt; nn.Module:\n        \"\"\"\n        Get the `torch.nn` activation function.\n\n        Parameters:\n            name (Literal[\"relu\", \"tanh\", \"elu\", \"leaky_relu\", \"prelu\", \"selu\", \"silu\", \"softsign\", \"sigmoid\", \"hardsigmoid\", \"lecun_tanh\"]):\n                the name of the activation function.\n\n        Returns:\n            activation (nn.Module): the PyTorch activation module.\n        \"\"\"\n        try:\n            return cls[name.upper()].value\n        except KeyError:\n            raise ValueError(f\"Unsupported activation function: {name}\")\n</code></pre>"},{"location":"learn/reference/models/activation/#velora.models.activation.ActivationEnum.get","title":"<code>get(name)</code>  <code>classmethod</code>","text":"<p>Get the <code>torch.nn</code> activation function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Literal['relu', 'tanh', 'elu', 'leaky_relu', 'prelu', 'selu', 'silu', 'softsign', 'sigmoid', 'hardsigmoid', 'lecun_tanh']</code> <p>the name of the activation function.</p> required <p>Returns:</p> Name Type Description <code>activation</code> <code>nn.Module</code> <p>the PyTorch activation module.</p> Source code in <code>velora/models/activation.py</code> Python<pre><code>@classmethod\ndef get(cls, name: ActivationTypeLiteral) -&gt; nn.Module:\n    \"\"\"\n    Get the `torch.nn` activation function.\n\n    Parameters:\n        name (Literal[\"relu\", \"tanh\", \"elu\", \"leaky_relu\", \"prelu\", \"selu\", \"silu\", \"softsign\", \"sigmoid\", \"hardsigmoid\", \"lecun_tanh\"]):\n            the name of the activation function.\n\n    Returns:\n        activation (nn.Module): the PyTorch activation module.\n    \"\"\"\n    try:\n        return cls[name.upper()].value\n    except KeyError:\n        raise ValueError(f\"Unsupported activation function: {name}\")\n</code></pre>"},{"location":"learn/reference/models/activation/#velora.models.activation.LeCunTanh","title":"<code>LeCunTanh</code>","text":"<p>               Bases: <code>nn.Module</code></p> <p>Implements LeCun's Tanh activation function. $$ f(x) = 1.7159 \\tanh (\\frac{2}{3} x) $$ Constants are applied to keep the variance of the output close to <code>1</code>.</p> Source code in <code>velora/models/activation.py</code> Python<pre><code>class LeCunTanh(nn.Module):\n    \"\"\"\n    Implements LeCun's Tanh activation function.\n    $$\n    f(x) = 1.7159 \\\\tanh (\\\\frac{2}{3} x)\n    $$\n    Constants are applied to keep the variance of the output close to `1`.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n        self.tanh = nn.Tanh()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return 1.7159 * self.tanh(0.666 * x)\n</code></pre>"},{"location":"learn/reference/models/backbone/","title":"velora.models.backbone","text":"Documentation <p>Customization: Backbones</p> <p>Architecture backbones for feature extraction.</p>"},{"location":"learn/reference/models/backbone/#velora.models.backbone.BasicCNN","title":"<code>BasicCNN</code>","text":"<p>               Bases: <code>nn.Module</code></p> <p>A CNN backbone from the DQN Nature paper: Human-level control through deep reinforcement learning [].</p> <p>Only contains the Convolutional Network with a flattened output.</p> <p>Useful for connecting with other architectures such as the <code>LiquidNCPNetwork</code>.</p> Source code in <code>velora/models/backbone.py</code> Python<pre><code>class BasicCNN(nn.Module):\n    \"\"\"\n    A CNN backbone from the DQN Nature paper: [Human-level control through deep reinforcement learning [:material-arrow-right-bottom:]](https://www.nature.com/articles/nature14236).\n\n    Only contains the Convolutional Network with a flattened output.\n\n    Useful for connecting with other architectures such as the\n    `LiquidNCPNetwork`.\n    \"\"\"\n\n    def __init__(self, in_channels: int) -&gt; None:\n        \"\"\"\n        Parameters:\n            in_channels (int): the number of channels in the input image. E.g.,\n\n                - `3` for RGB\n                - `1` for grayscale\n        \"\"\"\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n    def out_size(self, dim: Tuple[int, int]) -&gt; int:\n        \"\"\"\n        Calculates the size of the convolution output using a dummy input.\n\n        Often used as the `in_features` to linear layers.\n\n        Parameters:\n            dim (Tuple[int, int]): the `(height, width)` of a single image\n\n        Returns:\n            output_size (int): the number of feature maps.\n        \"\"\"\n        if len(dim) != 2:\n            raise ValueError(f\"Invalid '{dim=}'. Should be '(height, width)'.\")\n\n        with torch.no_grad():\n            x = torch.zeros(1, self.in_channels, *dim)\n            return self.conv(x).size(1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the network.\n\n        Parameters:\n            x (torch.Tensor): a batch of images in the shape\n                `(batch_size, in_channels, height, width)`.\n\n                - `batch_size` - number of images in the batch\n                - `in_channels` - number of colour channels\n                - `height` - height of the images\n                - `width` - width of the images\n\n        Returns:\n            y_pred (torch.Tensor): the flattened predicted feature maps.\n        \"\"\"\n        if x.dim() != 4:\n            raise ValueError(\n                f\"Invalid '{x.shape=}'. Should be `(batch_size, in_channels, height, width)`.\"\n            )\n\n        return self.conv(x)\n</code></pre>"},{"location":"learn/reference/models/backbone/#velora.models.backbone.BasicCNN.__init__","title":"<code>__init__(in_channels)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>the number of channels in the input image. E.g.,</p> <ul> <li><code>3</code> for RGB</li> <li><code>1</code> for grayscale</li> </ul> required Source code in <code>velora/models/backbone.py</code> Python<pre><code>def __init__(self, in_channels: int) -&gt; None:\n    \"\"\"\n    Parameters:\n        in_channels (int): the number of channels in the input image. E.g.,\n\n            - `3` for RGB\n            - `1` for grayscale\n    \"\"\"\n    super().__init__()\n    self.in_channels = in_channels\n\n    self.conv = nn.Sequential(\n        nn.Conv2d(in_channels, 32, kernel_size=8, stride=4, padding=0),\n        nn.ReLU(),\n        nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n        nn.ReLU(),\n        nn.Flatten(),\n    )\n</code></pre>"},{"location":"learn/reference/models/backbone/#velora.models.backbone.BasicCNN.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>a batch of images in the shape <code>(batch_size, in_channels, height, width)</code>.</p> <ul> <li><code>batch_size</code> - number of images in the batch</li> <li><code>in_channels</code> - number of colour channels</li> <li><code>height</code> - height of the images</li> <li><code>width</code> - width of the images</li> </ul> required <p>Returns:</p> Name Type Description <code>y_pred</code> <code>torch.Tensor</code> <p>the flattened predicted feature maps.</p> Source code in <code>velora/models/backbone.py</code> Python<pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the network.\n\n    Parameters:\n        x (torch.Tensor): a batch of images in the shape\n            `(batch_size, in_channels, height, width)`.\n\n            - `batch_size` - number of images in the batch\n            - `in_channels` - number of colour channels\n            - `height` - height of the images\n            - `width` - width of the images\n\n    Returns:\n        y_pred (torch.Tensor): the flattened predicted feature maps.\n    \"\"\"\n    if x.dim() != 4:\n        raise ValueError(\n            f\"Invalid '{x.shape=}'. Should be `(batch_size, in_channels, height, width)`.\"\n        )\n\n    return self.conv(x)\n</code></pre>"},{"location":"learn/reference/models/backbone/#velora.models.backbone.BasicCNN.out_size","title":"<code>out_size(dim)</code>","text":"<p>Calculates the size of the convolution output using a dummy input.</p> <p>Often used as the <code>in_features</code> to linear layers.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>Tuple[int, int]</code> <p>the <code>(height, width)</code> of a single image</p> required <p>Returns:</p> Name Type Description <code>output_size</code> <code>int</code> <p>the number of feature maps.</p> Source code in <code>velora/models/backbone.py</code> Python<pre><code>def out_size(self, dim: Tuple[int, int]) -&gt; int:\n    \"\"\"\n    Calculates the size of the convolution output using a dummy input.\n\n    Often used as the `in_features` to linear layers.\n\n    Parameters:\n        dim (Tuple[int, int]): the `(height, width)` of a single image\n\n    Returns:\n        output_size (int): the number of feature maps.\n    \"\"\"\n    if len(dim) != 2:\n        raise ValueError(f\"Invalid '{dim=}'. Should be '(height, width)'.\")\n\n    with torch.no_grad():\n        x = torch.zeros(1, self.in_channels, *dim)\n        return self.conv(x).size(1)\n</code></pre>"},{"location":"learn/reference/models/backbone/#velora.models.backbone.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>nn.Module</code></p> <p>A dynamic multi-layer perceptron architecture for feature extraction.</p> <p>Warning</p> <p>The network output (<code>y_pred</code>) is not passed through an activation function.</p> <p>This must be applied manually (if required).</p> Source code in <code>velora/models/backbone.py</code> Python<pre><code>class MLP(nn.Module):\n    \"\"\"\n    A dynamic multi-layer perceptron architecture for feature extraction.\n\n    !!! warning\n\n        The network output (`y_pred`) is not passed through an activation function.\n\n        This must be applied manually (if required).\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        n_hidden: List[int] | int,\n        out_features: int,\n        *,\n        activation: ActivationTypeLiteral = \"relu\",\n        dropout_p: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            in_features (int): the number of input features\n            n_hidden (List[int] | int): a `list` of hidden node sizes or\n                a `single` hidden node size. Dynamically creates `nn.Linear`\n                layers based on sizes\n            out_features (int): the number of output features\n            activation (str, optional): the type of activation function used\n                between layers\n            dropout_p (float, optional): the dropout probability rate used between\n                layers\n        \"\"\"\n        super().__init__()\n\n        self.dropout_p = dropout_p\n        n_hidden = [n_hidden] if isinstance(n_hidden, int) else n_hidden\n\n        input = nn.Linear(in_features, n_hidden[0])\n        h_layers = self._set_hidden_layers(n_hidden, activation)\n        output = nn.Linear(n_hidden[-1], out_features)\n\n        self.fc = nn.Sequential(\n            input,\n            ActivationEnum.get(activation),\n            *h_layers,\n            output,\n        )\n\n    def _set_hidden_layers(self, n_hidden: List[int], activation: str) -&gt; nn.ModuleList:\n        \"\"\"\n        Helper method. Dynamically creates the hidden layers with\n        activation functions and dropout layers.\n\n        Parameters:\n            n_hidden (List[int]): a list of hidden node sizes\n            activation (str): the name of the activation function\n\n        Returns:\n            mlp (nn.ModuleList): a list of `nn.Linear` layers.\n        \"\"\"\n        h_layers = nn.ModuleList()\n\n        for i in range(len(n_hidden) - 1):\n            layers = [\n                nn.Linear(n_hidden[i], n_hidden[i + 1]),\n                ActivationEnum.get(activation),\n            ]\n\n            if self.dropout_p &gt; 0.0:\n                layers.append(nn.Dropout(self.dropout_p))\n\n            h_layers.extend(layers)\n\n        return h_layers\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            x (torch.Tensor): the input tensor\n\n        Returns:\n            y_pred (torch.Tensor): network predictions.\n        \"\"\"\n        return self.fc(x)\n</code></pre>"},{"location":"learn/reference/models/backbone/#velora.models.backbone.MLP.__init__","title":"<code>__init__(in_features, n_hidden, out_features, *, activation='relu', dropout_p=0.0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>the number of input features</p> required <code>n_hidden</code> <code>List[int] | int</code> <p>a <code>list</code> of hidden node sizes or a <code>single</code> hidden node size. Dynamically creates <code>nn.Linear</code> layers based on sizes</p> required <code>out_features</code> <code>int</code> <p>the number of output features</p> required <code>activation</code> <code>str</code> <p>the type of activation function used between layers</p> <code>'relu'</code> <code>dropout_p</code> <code>float</code> <p>the dropout probability rate used between layers</p> <code>0.0</code> Source code in <code>velora/models/backbone.py</code> Python<pre><code>def __init__(\n    self,\n    in_features: int,\n    n_hidden: List[int] | int,\n    out_features: int,\n    *,\n    activation: ActivationTypeLiteral = \"relu\",\n    dropout_p: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        in_features (int): the number of input features\n        n_hidden (List[int] | int): a `list` of hidden node sizes or\n            a `single` hidden node size. Dynamically creates `nn.Linear`\n            layers based on sizes\n        out_features (int): the number of output features\n        activation (str, optional): the type of activation function used\n            between layers\n        dropout_p (float, optional): the dropout probability rate used between\n            layers\n    \"\"\"\n    super().__init__()\n\n    self.dropout_p = dropout_p\n    n_hidden = [n_hidden] if isinstance(n_hidden, int) else n_hidden\n\n    input = nn.Linear(in_features, n_hidden[0])\n    h_layers = self._set_hidden_layers(n_hidden, activation)\n    output = nn.Linear(n_hidden[-1], out_features)\n\n    self.fc = nn.Sequential(\n        input,\n        ActivationEnum.get(activation),\n        *h_layers,\n        output,\n    )\n</code></pre>"},{"location":"learn/reference/models/backbone/#velora.models.backbone.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>the input tensor</p> required <p>Returns:</p> Name Type Description <code>y_pred</code> <code>torch.Tensor</code> <p>network predictions.</p> Source code in <code>velora/models/backbone.py</code> Python<pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        x (torch.Tensor): the input tensor\n\n    Returns:\n        y_pred (torch.Tensor): network predictions.\n    \"\"\"\n    return self.fc(x)\n</code></pre>"},{"location":"learn/reference/models/base/","title":"velora.models.base","text":"<p>Home to the agent base class for all pre-built agents.</p>"},{"location":"learn/reference/models/base/#velora.models.base.LiquidNCPModule","title":"<code>LiquidNCPModule</code>","text":"<p>               Bases: <code>nn.Module</code></p> <p>A base class for Liquid NCP modules.</p> <p>Useful for Actor-Critic modules.</p> Source code in <code>velora/models/base.py</code> Python<pre><code>class LiquidNCPModule(nn.Module):\n    \"\"\"\n    A base class for Liquid NCP modules.\n\n    Useful for Actor-Critic modules.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        n_neurons: int,\n        out_features: int,\n        *,\n        init_type: str | WeightInitType = \"kaiming_uniform\",\n        device: torch.device | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            in_features (int): the number of input nodes\n            n_neurons (int): the number of hidden neurons\n            out_features (int): the number of output nodes\n            init_type (str, optional): the type of weight initialization\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__()\n\n        self.in_features = in_features\n        self.n_neurons = n_neurons\n        self.out_features = out_features\n        self.device = device\n\n        self.ncp = LiquidNCPNetwork(\n            in_features=in_features,\n            n_neurons=n_neurons,\n            out_features=out_features,\n            init_type=init_type,\n            device=device,\n        ).to(device)\n\n    def config(self) -&gt; ModuleConfig:\n        \"\"\"\n        Gets details about the module.\n\n        Returns:\n            config (ModuleConfig): a config model containing module details.\n        \"\"\"\n        return ModuleConfig(\n            active_params=self.ncp.active_params,\n            total_params=self.ncp.total_params,\n            architecture=summary(self),\n        )\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.LiquidNCPModule.__init__","title":"<code>__init__(in_features, n_neurons, out_features, *, init_type='kaiming_uniform', device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>the number of input nodes</p> required <code>n_neurons</code> <code>int</code> <p>the number of hidden neurons</p> required <code>out_features</code> <code>int</code> <p>the number of output nodes</p> required <code>init_type</code> <code>str</code> <p>the type of weight initialization</p> <code>'kaiming_uniform'</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/base.py</code> Python<pre><code>def __init__(\n    self,\n    in_features: int,\n    n_neurons: int,\n    out_features: int,\n    *,\n    init_type: str | WeightInitType = \"kaiming_uniform\",\n    device: torch.device | None = None,\n):\n    \"\"\"\n    Parameters:\n        in_features (int): the number of input nodes\n        n_neurons (int): the number of hidden neurons\n        out_features (int): the number of output nodes\n        init_type (str, optional): the type of weight initialization\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__()\n\n    self.in_features = in_features\n    self.n_neurons = n_neurons\n    self.out_features = out_features\n    self.device = device\n\n    self.ncp = LiquidNCPNetwork(\n        in_features=in_features,\n        n_neurons=n_neurons,\n        out_features=out_features,\n        init_type=init_type,\n        device=device,\n    ).to(device)\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.LiquidNCPModule.config","title":"<code>config()</code>","text":"<p>Gets details about the module.</p> <p>Returns:</p> Name Type Description <code>config</code> <code>ModuleConfig</code> <p>a config model containing module details.</p> Source code in <code>velora/models/base.py</code> Python<pre><code>def config(self) -&gt; ModuleConfig:\n    \"\"\"\n    Gets details about the module.\n\n    Returns:\n        config (ModuleConfig): a config model containing module details.\n    \"\"\"\n    return ModuleConfig(\n        active_params=self.ncp.active_params,\n        total_params=self.ncp.total_params,\n        architecture=summary(self),\n    )\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.NCPModule","title":"<code>NCPModule</code>","text":"<p>               Bases: <code>nn.Module</code></p> <p>A base class for NCP modules.</p> <p>Useful for Actor-Critic modules.</p> Source code in <code>velora/models/base.py</code> Python<pre><code>class NCPModule(nn.Module):\n    \"\"\"\n    A base class for NCP modules.\n\n    Useful for Actor-Critic modules.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        n_neurons: int,\n        out_features: int,\n        *,\n        init_type: str | WeightInitType = \"kaiming_uniform\",\n        device: torch.device | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            in_features (int): the number of input nodes\n            n_neurons (int): the number of hidden neurons\n            out_features (int): the number of output nodes\n            init_type (str, optional): the type of weight initialization\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__()\n\n        self.in_features = in_features\n        self.n_neurons = n_neurons\n        self.out_features = out_features\n        self.device = device\n\n        self.ncp = NCPNetwork(\n            in_features=in_features,\n            n_neurons=n_neurons,\n            out_features=out_features,\n            init_type=init_type,\n            device=device,\n        ).to(device)\n\n    def config(self) -&gt; ModuleConfig:\n        \"\"\"\n        Gets details about the module.\n\n        Returns:\n            config (ModuleConfig): a config model containing module details.\n        \"\"\"\n        return ModuleConfig(\n            active_params=self.ncp.active_params,\n            total_params=self.ncp.total_params,\n            architecture=summary(self),\n        )\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.NCPModule.__init__","title":"<code>__init__(in_features, n_neurons, out_features, *, init_type='kaiming_uniform', device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>the number of input nodes</p> required <code>n_neurons</code> <code>int</code> <p>the number of hidden neurons</p> required <code>out_features</code> <code>int</code> <p>the number of output nodes</p> required <code>init_type</code> <code>str</code> <p>the type of weight initialization</p> <code>'kaiming_uniform'</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/base.py</code> Python<pre><code>def __init__(\n    self,\n    in_features: int,\n    n_neurons: int,\n    out_features: int,\n    *,\n    init_type: str | WeightInitType = \"kaiming_uniform\",\n    device: torch.device | None = None,\n):\n    \"\"\"\n    Parameters:\n        in_features (int): the number of input nodes\n        n_neurons (int): the number of hidden neurons\n        out_features (int): the number of output nodes\n        init_type (str, optional): the type of weight initialization\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__()\n\n    self.in_features = in_features\n    self.n_neurons = n_neurons\n    self.out_features = out_features\n    self.device = device\n\n    self.ncp = NCPNetwork(\n        in_features=in_features,\n        n_neurons=n_neurons,\n        out_features=out_features,\n        init_type=init_type,\n        device=device,\n    ).to(device)\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.NCPModule.config","title":"<code>config()</code>","text":"<p>Gets details about the module.</p> <p>Returns:</p> Name Type Description <code>config</code> <code>ModuleConfig</code> <p>a config model containing module details.</p> Source code in <code>velora/models/base.py</code> Python<pre><code>def config(self) -&gt; ModuleConfig:\n    \"\"\"\n    Gets details about the module.\n\n    Returns:\n        config (ModuleConfig): a config model containing module details.\n    \"\"\"\n    return ModuleConfig(\n        active_params=self.ncp.active_params,\n        total_params=self.ncp.total_params,\n        architecture=summary(self),\n    )\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.RLModuleAgent","title":"<code>RLModuleAgent</code>","text":"<p>A base class for RL agents that use modules.</p> <p>Provides a blueprint describing the core methods that agents must have and includes useful utility methods.</p> Source code in <code>velora/models/base.py</code> Python<pre><code>class RLModuleAgent:\n    \"\"\"\n    A base class for RL agents that use modules.\n\n    Provides a blueprint describing the core methods that agents *must* have and\n    includes useful utility methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        env: gym.Env,\n        actor_neurons: int,\n        critic_neurons: int,\n        buffer_size: int,\n        optim: Type[optim.Optimizer],\n        device: torch.device | None,\n        seed: int | None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            env (gym.Env): Gymnasium environment to train on\n            actor_neurons (int): number of decision nodes (inter and command nodes)\n                for the actor\n            critic_neurons (int): number of decision nodes (inter and command nodes)\n                for the critic\n            buffer_size (int): buffer capacity\n            device (torch.device, optional): the device to perform computations on\n            seed (int, optional): random number seed\n        \"\"\"\n        self.env = env\n        self.eval_env = add_core_env_wrappers(env, device)\n        self.actor_neurons = actor_neurons\n        self.critic_neurons = critic_neurons\n        self.buffer_size = buffer_size\n        self.optim = optim\n        self.device = device\n        self.seed = set_seed(seed)\n\n        self.action_dim: int = (\n            self.env.action_space.n.item()\n            if isinstance(self.env.action_space, gym.spaces.Discrete)\n            else self.env.action_space.shape[-1]\n        )\n        self.state_dim: int = self.env.observation_space.shape[-1]\n\n        self.action_scale = None\n        self.action_bias = None\n\n        if isinstance(self.env.action_space, gym.spaces.Box):\n            self.action_scale = (\n                torch.tensor(\n                    self.env.action_space.high - self.env.action_space.low,\n                    device=device,\n                )\n                / 2.0\n            )\n            self.action_bias = (\n                torch.tensor(\n                    self.env.action_space.high + self.env.action_space.low,\n                    device=device,\n                )\n                / 2.0\n            )\n\n        self.config: RLAgentConfig | None = None\n        self.buffer: \"BufferBase\" | None = None\n\n        self.actor: \"ActorModule\" | None = None\n        self.critic: \"CriticModule\" | None = None\n\n        self.entropy: \"EntropyModule\" | None = None\n\n        self.active_params = 0\n        self.total_params = 0\n\n        self.metadata: Dict[str, Any] = {}\n\n    @abstractmethod\n    def train(\n        self,\n        n_episodes: int,\n        max_steps: int,\n        window_size: int,\n        *args,\n        **kwargs,\n    ) -&gt; Any:\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def predict(\n        self,\n        state: torch.Tensor,\n        hidden: torch.Tensor,\n        train_mode: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def save(\n        self,\n        dirpath: str | Path,\n        *,\n        buffer: bool = False,\n        config: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Saves the current model state into `safetensors` and `json` files.\n\n        !!! warning\n\n            `model_config.json` is stored in the `dirpath.parent`.\n\n        Includes:\n\n        - `model_config.json` - contains the core details of the agent (optional)\n        - `metadata.json` - contains the model, optimizer and buffer (optional) metadata\n        - `model_state.safetensors` - contains the model weights and biases\n        - `optim_state.safetensors` - contains the optimizer states (actor and critic)\n        - `buffer_state.safetensors` - contains the buffer state (only if `buffer=True`)\n\n        Parameters:\n            dirpath (str | Path): the location to store the model state. Should only\n                consist of `folder` names. E.g., `&lt;folder&gt;/&lt;folder&gt;`\n            buffer (bool, optional): a flag for storing the buffer state\n            config (bool, optional): a flag for storing the model's config\n        \"\"\"\n        pass  # pragma: no cover\n\n    @classmethod\n    @abstractmethod\n    def load(cls, dirpath: str | Path, *, buffer: bool = False) -&gt; Self:\n        \"\"\"\n        Creates a new agent instance by loading a saved one from the `dirpath`.\n        Also, loads the original training buffer if `buffer=True`.\n\n        These files must exist in the `dirpath`:\n\n        - `metadata.json` - contains the model, optimizer and buffer (optional) metadata\n        - `model_state.safetensors` - contains the model weights and biases\n        - `optim_state.safetensors` - contains the optimizer states (actor and critic)\n        - `buffer_state.safetensors` - contains the buffer state (only if `buffer=True`)\n\n        Parameters:\n            dirpath (str | Path): the location to store the model state. Should only\n                consist of `folder` names. E.g., `&lt;folder&gt;/&lt;folder&gt;`\n            buffer (bool, optional): a flag for storing the buffer state\n\n        Returns:\n            agent (Self): a new agent instance with the saved state\n        \"\"\"\n        pass  # pragma: no cover\n\n    def state_dict(self) -&gt; Dict[StateDictKeys, Dict[str, Any]]:\n        \"\"\"\n        Retrieves the agent's module state dictionaries and splits them into\n        categories.\n\n        Returns:\n            state_dict (Dict[Literal[\"modules\", \"optimizers\"], Dict[str, Any]]): the agent's module state dicts categorized.\n        \"\"\"\n        final_dict: Dict[StateDictKeys, Dict[str, Any]] = {\n            \"modules\": {},\n            \"optimizers\": {},\n        }\n\n        for module in [self.actor, self.critic, self.entropy]:\n            if module is not None:\n                state_dict: Dict[str, Any] = module.state_dict()\n\n                for key, val in state_dict.items():\n                    if \"optim\" in key:\n                        final_dict[\"optimizers\"][key] = val\n                    else:\n                        final_dict[\"modules\"][key] = val\n\n        return final_dict\n\n    def set_metadata(self, values: Dict[str, Any], seed: int) -&gt; Dict[str, Any]:\n        \"\"\"\n        Creates the agents metadata based on a given set of local variables.\n\n        Parameters:\n            values (Dict[str, Any]): local variables\n            seed (int): randomly generated seed\n\n        Returns:\n            metadata (Dict[str, Any]): an updated dictionary of agent metadata.\n        \"\"\"\n        metadata = {\n            k: v for k, v in values.items() if k not in [\"self\", \"__class__\", \"env\"]\n        }\n        metadata[\"device\"] = str(self.device) if self.device is not None else \"cpu\"\n        metadata[\"optim\"] = f\"torch.optim.{self.optim.__name__}\"\n        metadata[\"seed\"] = seed\n\n        return metadata\n\n    def _set_train_params(self, params: Dict[str, Any]) -&gt; TrainConfig:\n        \"\"\"\n        Helper method. Sets the `train_params` given a dictionary of training parameters.\n\n        Parameters:\n            params (Dict[str, Any]): a dictionary of training parameters\n\n        Returns:\n            config (TrainConfig): a training config model\n        \"\"\"\n        params = dict(\n            callbacks=(\n                dict(cb.config() for cb in params[\"callbacks\"])\n                if params[\"callbacks\"]\n                else None\n            ),\n            **{k: v for k, v in params.items() if k not in [\"self\", \"callbacks\"]},\n        )\n        return TrainConfig(**params)\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.RLModuleAgent.__init__","title":"<code>__init__(env, actor_neurons, critic_neurons, buffer_size, optim, device, seed)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>env</code> <code>gym.Env</code> <p>Gymnasium environment to train on</p> required <code>actor_neurons</code> <code>int</code> <p>number of decision nodes (inter and command nodes) for the actor</p> required <code>critic_neurons</code> <code>int</code> <p>number of decision nodes (inter and command nodes) for the critic</p> required <code>buffer_size</code> <code>int</code> <p>buffer capacity</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> required <code>seed</code> <code>int</code> <p>random number seed</p> required Source code in <code>velora/models/base.py</code> Python<pre><code>def __init__(\n    self,\n    env: gym.Env,\n    actor_neurons: int,\n    critic_neurons: int,\n    buffer_size: int,\n    optim: Type[optim.Optimizer],\n    device: torch.device | None,\n    seed: int | None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        env (gym.Env): Gymnasium environment to train on\n        actor_neurons (int): number of decision nodes (inter and command nodes)\n            for the actor\n        critic_neurons (int): number of decision nodes (inter and command nodes)\n            for the critic\n        buffer_size (int): buffer capacity\n        device (torch.device, optional): the device to perform computations on\n        seed (int, optional): random number seed\n    \"\"\"\n    self.env = env\n    self.eval_env = add_core_env_wrappers(env, device)\n    self.actor_neurons = actor_neurons\n    self.critic_neurons = critic_neurons\n    self.buffer_size = buffer_size\n    self.optim = optim\n    self.device = device\n    self.seed = set_seed(seed)\n\n    self.action_dim: int = (\n        self.env.action_space.n.item()\n        if isinstance(self.env.action_space, gym.spaces.Discrete)\n        else self.env.action_space.shape[-1]\n    )\n    self.state_dim: int = self.env.observation_space.shape[-1]\n\n    self.action_scale = None\n    self.action_bias = None\n\n    if isinstance(self.env.action_space, gym.spaces.Box):\n        self.action_scale = (\n            torch.tensor(\n                self.env.action_space.high - self.env.action_space.low,\n                device=device,\n            )\n            / 2.0\n        )\n        self.action_bias = (\n            torch.tensor(\n                self.env.action_space.high + self.env.action_space.low,\n                device=device,\n            )\n            / 2.0\n        )\n\n    self.config: RLAgentConfig | None = None\n    self.buffer: \"BufferBase\" | None = None\n\n    self.actor: \"ActorModule\" | None = None\n    self.critic: \"CriticModule\" | None = None\n\n    self.entropy: \"EntropyModule\" | None = None\n\n    self.active_params = 0\n    self.total_params = 0\n\n    self.metadata: Dict[str, Any] = {}\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.RLModuleAgent.load","title":"<code>load(dirpath, *, buffer=False)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Creates a new agent instance by loading a saved one from the <code>dirpath</code>. Also, loads the original training buffer if <code>buffer=True</code>.</p> <p>These files must exist in the <code>dirpath</code>:</p> <ul> <li><code>metadata.json</code> - contains the model, optimizer and buffer (optional) metadata</li> <li><code>model_state.safetensors</code> - contains the model weights and biases</li> <li><code>optim_state.safetensors</code> - contains the optimizer states (actor and critic)</li> <li><code>buffer_state.safetensors</code> - contains the buffer state (only if <code>buffer=True</code>)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dirpath</code> <code>str | Path</code> <p>the location to store the model state. Should only consist of <code>folder</code> names. E.g., <code>&lt;folder&gt;/&lt;folder&gt;</code></p> required <code>buffer</code> <code>bool</code> <p>a flag for storing the buffer state</p> <code>False</code> <p>Returns:</p> Name Type Description <code>agent</code> <code>Self</code> <p>a new agent instance with the saved state</p> Source code in <code>velora/models/base.py</code> Python<pre><code>@classmethod\n@abstractmethod\ndef load(cls, dirpath: str | Path, *, buffer: bool = False) -&gt; Self:\n    \"\"\"\n    Creates a new agent instance by loading a saved one from the `dirpath`.\n    Also, loads the original training buffer if `buffer=True`.\n\n    These files must exist in the `dirpath`:\n\n    - `metadata.json` - contains the model, optimizer and buffer (optional) metadata\n    - `model_state.safetensors` - contains the model weights and biases\n    - `optim_state.safetensors` - contains the optimizer states (actor and critic)\n    - `buffer_state.safetensors` - contains the buffer state (only if `buffer=True`)\n\n    Parameters:\n        dirpath (str | Path): the location to store the model state. Should only\n            consist of `folder` names. E.g., `&lt;folder&gt;/&lt;folder&gt;`\n        buffer (bool, optional): a flag for storing the buffer state\n\n    Returns:\n        agent (Self): a new agent instance with the saved state\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.RLModuleAgent.save","title":"<code>save(dirpath, *, buffer=False, config=False)</code>  <code>abstractmethod</code>","text":"<p>Saves the current model state into <code>safetensors</code> and <code>json</code> files.</p> <p>Warning</p> <p><code>model_config.json</code> is stored in the <code>dirpath.parent</code>.</p> <p>Includes:</p> <ul> <li><code>model_config.json</code> - contains the core details of the agent (optional)</li> <li><code>metadata.json</code> - contains the model, optimizer and buffer (optional) metadata</li> <li><code>model_state.safetensors</code> - contains the model weights and biases</li> <li><code>optim_state.safetensors</code> - contains the optimizer states (actor and critic)</li> <li><code>buffer_state.safetensors</code> - contains the buffer state (only if <code>buffer=True</code>)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dirpath</code> <code>str | Path</code> <p>the location to store the model state. Should only consist of <code>folder</code> names. E.g., <code>&lt;folder&gt;/&lt;folder&gt;</code></p> required <code>buffer</code> <code>bool</code> <p>a flag for storing the buffer state</p> <code>False</code> <code>config</code> <code>bool</code> <p>a flag for storing the model's config</p> <code>False</code> Source code in <code>velora/models/base.py</code> Python<pre><code>@abstractmethod\ndef save(\n    self,\n    dirpath: str | Path,\n    *,\n    buffer: bool = False,\n    config: bool = False,\n) -&gt; None:\n    \"\"\"\n    Saves the current model state into `safetensors` and `json` files.\n\n    !!! warning\n\n        `model_config.json` is stored in the `dirpath.parent`.\n\n    Includes:\n\n    - `model_config.json` - contains the core details of the agent (optional)\n    - `metadata.json` - contains the model, optimizer and buffer (optional) metadata\n    - `model_state.safetensors` - contains the model weights and biases\n    - `optim_state.safetensors` - contains the optimizer states (actor and critic)\n    - `buffer_state.safetensors` - contains the buffer state (only if `buffer=True`)\n\n    Parameters:\n        dirpath (str | Path): the location to store the model state. Should only\n            consist of `folder` names. E.g., `&lt;folder&gt;/&lt;folder&gt;`\n        buffer (bool, optional): a flag for storing the buffer state\n        config (bool, optional): a flag for storing the model's config\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.RLModuleAgent.set_metadata","title":"<code>set_metadata(values, seed)</code>","text":"<p>Creates the agents metadata based on a given set of local variables.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict[str, Any]</code> <p>local variables</p> required <code>seed</code> <code>int</code> <p>randomly generated seed</p> required <p>Returns:</p> Name Type Description <code>metadata</code> <code>Dict[str, Any]</code> <p>an updated dictionary of agent metadata.</p> Source code in <code>velora/models/base.py</code> Python<pre><code>def set_metadata(self, values: Dict[str, Any], seed: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Creates the agents metadata based on a given set of local variables.\n\n    Parameters:\n        values (Dict[str, Any]): local variables\n        seed (int): randomly generated seed\n\n    Returns:\n        metadata (Dict[str, Any]): an updated dictionary of agent metadata.\n    \"\"\"\n    metadata = {\n        k: v for k, v in values.items() if k not in [\"self\", \"__class__\", \"env\"]\n    }\n    metadata[\"device\"] = str(self.device) if self.device is not None else \"cpu\"\n    metadata[\"optim\"] = f\"torch.optim.{self.optim.__name__}\"\n    metadata[\"seed\"] = seed\n\n    return metadata\n</code></pre>"},{"location":"learn/reference/models/base/#velora.models.base.RLModuleAgent.state_dict","title":"<code>state_dict()</code>","text":"<p>Retrieves the agent's module state dictionaries and splits them into categories.</p> <p>Returns:</p> Name Type Description <code>state_dict</code> <code>Dict[Literal['modules', 'optimizers'], Dict[str, Any]]</code> <p>the agent's module state dicts categorized.</p> Source code in <code>velora/models/base.py</code> Python<pre><code>def state_dict(self) -&gt; Dict[StateDictKeys, Dict[str, Any]]:\n    \"\"\"\n    Retrieves the agent's module state dictionaries and splits them into\n    categories.\n\n    Returns:\n        state_dict (Dict[Literal[\"modules\", \"optimizers\"], Dict[str, Any]]): the agent's module state dicts categorized.\n    \"\"\"\n    final_dict: Dict[StateDictKeys, Dict[str, Any]] = {\n        \"modules\": {},\n        \"optimizers\": {},\n    }\n\n    for module in [self.actor, self.critic, self.entropy]:\n        if module is not None:\n            state_dict: Dict[str, Any] = module.state_dict()\n\n            for key, val in state_dict.items():\n                if \"optim\" in key:\n                    final_dict[\"optimizers\"][key] = val\n                else:\n                    final_dict[\"modules\"][key] = val\n\n    return final_dict\n</code></pre>"},{"location":"learn/reference/models/config/","title":"velora.models.config","text":"<p>Config models for storing agent details.</p>"},{"location":"learn/reference/models/config/#velora.models.config.BufferConfig","title":"<code>BufferConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A config model for buffer details.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['ReplayBuffer', 'RolloutBuffer']</code> <p>the type of buffer</p> <code>capacity</code> <code>int</code> <p>the maximum capacity of the buffer</p> <code>state_dim</code> <code>int</code> <p>dimension of state observations</p> <code>action_dim</code> <code>int</code> <p>dimension of actions</p> <code>hidden_dim</code> <code>int</code> <p>dimension of hidden state</p> Source code in <code>velora/models/config.py</code> Python<pre><code>class BufferConfig(BaseModel):\n    \"\"\"\n    A config model for buffer details.\n\n    Attributes:\n        type: the type of buffer\n        capacity: the maximum capacity of the buffer\n        state_dim: dimension of state observations\n        action_dim: dimension of actions\n        hidden_dim: dimension of hidden state\n    \"\"\"\n\n    type: Literal[\"ReplayBuffer\", \"RolloutBuffer\"]\n    capacity: int\n    state_dim: int\n    action_dim: int\n    hidden_dim: int\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.CriticConfig","title":"<code>CriticConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A critic config model for storing a NeuroFlow agent's critic module details.</p> <p>Attributes:</p> Name Type Description <code>critic1</code> <code>ModuleConfig</code> <p>details about the first critic network</p> <code>critic2</code> <code>ModuleConfig</code> <p>details about the second critic network</p> Source code in <code>velora/models/config.py</code> Python<pre><code>class CriticConfig(BaseModel):\n    \"\"\"\n    A critic config model for storing a NeuroFlow agent's critic module details.\n\n    Attributes:\n        critic1: details about the first critic network\n        critic2: details about the second critic network\n    \"\"\"\n\n    critic1: ModuleConfig\n    critic2: ModuleConfig\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.CuriosityConfig","title":"<code>CuriosityConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A config model for the Intrinsic Curiosity Module (ICM).</p> <p>Attributes:</p> Name Type Description <code>icm</code> <code>ModuleConfig</code> <p>details about the ICM</p> <code>lr</code> <code>float</code> <p>the optimizers learning rate</p> <code>eta</code> <code>float</code> <p>importance scaling factor for intrinsic reward</p> <code>beta</code> <code>float</code> <p>weight balancing for inverse vs. forward model</p> Source code in <code>velora/models/config.py</code> Python<pre><code>class CuriosityConfig(BaseModel):\n    \"\"\"\n    A config model for the Intrinsic Curiosity Module (ICM).\n\n    Attributes:\n        icm: details about the ICM\n        lr: the optimizers learning rate\n        eta: importance scaling factor for intrinsic reward\n        beta: weight balancing for inverse vs. forward model\n    \"\"\"\n\n    icm: ModuleConfig\n    lr: float\n    eta: float\n    beta: float\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.EntropyParameters","title":"<code>EntropyParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A config model for extra parameters for NeuroFlow agents.</p> <p>Attributes:</p> Name Type Description <code>lr</code> <code>float</code> <p>the entropy parameter learning rate</p> <code>initial_alpha</code> <code>float</code> <p>the starting entropy coefficient value</p> <code>target</code> <code>float</code> <p>the target entropy for automatic adjustment</p> Source code in <code>velora/models/config.py</code> Python<pre><code>class EntropyParameters(BaseModel):\n    \"\"\"\n    A config model for extra parameters for NeuroFlow agents.\n\n    Attributes:\n        lr: the entropy parameter learning rate\n        initial_alpha: the starting entropy coefficient value\n        target: the target entropy for automatic adjustment\n    \"\"\"\n\n    lr: float\n    initial_alpha: float\n    target: float\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.ModelDetails","title":"<code>ModelDetails</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A config model for storing an agent's network model details.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>the type of architecture used. Default is <code>actor-critic</code></p> <code>state_dim</code> <code>int</code> <p>number of input features</p> <code>actor_neurons</code> <code>int</code> <p>number of actor network decision nodes</p> <code>critic_neurons</code> <code>int</code> <p>number of critic network decision nodes</p> <code>action_dim</code> <code>int</code> <p>number of output features</p> <code>action_type</code> <code>Literal['discrete', 'continuous']</code> <p>the type of action space. Default is <code>continuous</code></p> <code>tau</code> <code>float</code> <p>the soft update factor for target networks</p> <code>gamma</code> <code>float</code> <p>the reward discount factor</p> <code>target_networks</code> <code>bool</code> <p>whether the agent uses target networks or not. Default is <code>True</code></p> <code>log_std</code> <code>Tuple[float, float] | None</code> <p>lower and upper bounds for the log standard deviation of the action distribution. Only required for <code>continuous</code> spaces. Default is <code>None</code></p> <code>exploration_type</code> <code>Literal['Entropy', 'CAT-Entropy']</code> <p>the type of agent exploration used</p> <code>actor</code> <code>ModuleConfig</code> <p>details about the Actor network</p> <code>critic</code> <code>CriticConfig</code> <p>details about the Critic networks</p> <code>entropy</code> <code>EntropyParameters</code> <p>details about the entropy exploration</p> Source code in <code>velora/models/config.py</code> Python<pre><code>class ModelDetails(BaseModel):\n    \"\"\"\n    A config model for storing an agent's network model details.\n\n    Attributes:\n        type: the type of architecture used. Default is `actor-critic`\n        state_dim: number of input features\n        actor_neurons: number of actor network decision nodes\n        critic_neurons: number of critic network decision nodes\n        action_dim: number of output features\n        action_type: the type of action space. Default is `continuous`\n        tau: the soft update factor for target networks\n        gamma: the reward discount factor\n        target_networks: whether the agent uses target networks or not.\n            Default is `True`\n        log_std: lower and upper bounds for the log standard deviation of the\n            action distribution. Only required for `continuous` spaces.\n            Default is `None`\n        exploration_type: the type of agent exploration used\n        actor: details about the Actor network\n        critic: details about the Critic networks\n        entropy: details about the entropy exploration\n    \"\"\"\n\n    type: str = \"actor-critic\"\n    state_dim: int\n    actor_neurons: int\n    critic_neurons: int\n    action_dim: int\n    tau: float\n    gamma: float\n    action_type: Literal[\"discrete\", \"continuous\"] = \"continuous\"\n    target_networks: bool = True\n    log_std: Tuple[float, float] | None = None\n    exploration_type: Literal[\"Entropy\", \"CAT-Entropy\"]\n    actor: ModuleConfig\n    critic: CriticConfig\n    entropy: EntropyParameters\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.ModuleConfig","title":"<code>ModuleConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A config model for a module's details.</p> <p>Attributes:</p> Name Type Description <code>active_params</code> <code>int</code> <p>active module parameters count</p> <code>total_params</code> <code>int</code> <p>total module parameter count</p> <code>architecture</code> <code>Dict[str, Any]</code> <p>a summary of the module's architecture</p> Source code in <code>velora/models/config.py</code> Python<pre><code>class ModuleConfig(BaseModel):\n    \"\"\"\n    A config model for a module's details.\n\n    Attributes:\n        active_params: active module parameters count\n        total_params: total module parameter count\n        architecture: a summary of the module's architecture\n    \"\"\"\n\n    active_params: int\n    total_params: int\n    architecture: Dict[str, Any]\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.RLAgentConfig","title":"<code>RLAgentConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A config model for NeuroFlow agents. Stored with agent states during the <code>save()</code> method.</p> <p>Attributes:</p> Name Type Description <code>agent</code> <code>str</code> <p>the type of agent used</p> <code>env</code> <code>str</code> <p>the Gymnasium environment ID the model was trained on</p> <code>seed</code> <code>int</code> <p>random number generator value</p> <code>model_details</code> <code>ModelDetails</code> <p>the agent's network model details</p> <code>buffer</code> <code>BufferConfig</code> <p>the buffer details</p> <code>torch</code> <code>TorchConfig</code> <p>the PyTorch details</p> <code>train_params</code> <code>TrainConfig | None</code> <p>the agents training parameters. Default is <code>None</code></p> Source code in <code>velora/models/config.py</code> Python<pre><code>class RLAgentConfig(BaseModel):\n    \"\"\"\n    A config model for NeuroFlow agents. Stored with agent states during the\n    `save()` method.\n\n    Attributes:\n        agent: the type of agent used\n        env: the Gymnasium environment ID the model was trained on\n        seed: random number generator value\n        model_details: the agent's network model details\n        buffer: the buffer details\n        torch: the PyTorch details\n        train_params: the agents training parameters. Default is `None`\n    \"\"\"\n\n    agent: str\n    env: str\n    seed: int\n    model_details: ModelDetails\n    buffer: BufferConfig\n    torch: TorchConfig\n    train_params: TrainConfig | None = None\n\n    def update(self, train_params: TrainConfig) -&gt; Self:\n        \"\"\"\n        Updates the training details of the model.\n\n        Parameters:\n            train_params (TrainConfig): a config containing training parameters\n\n        Returns:\n            self (Self): a new config model with the updated values.\n        \"\"\"\n        return RLAgentConfig(\n            train_params=train_params,\n            **self.model_dump(exclude={\"train_params\"}),\n        )\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.RLAgentConfig.update","title":"<code>update(train_params)</code>","text":"<p>Updates the training details of the model.</p> <p>Parameters:</p> Name Type Description Default <code>train_params</code> <code>TrainConfig</code> <p>a config containing training parameters</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>a new config model with the updated values.</p> Source code in <code>velora/models/config.py</code> Python<pre><code>def update(self, train_params: TrainConfig) -&gt; Self:\n    \"\"\"\n    Updates the training details of the model.\n\n    Parameters:\n        train_params (TrainConfig): a config containing training parameters\n\n    Returns:\n        self (Self): a new config model with the updated values.\n    \"\"\"\n    return RLAgentConfig(\n        train_params=train_params,\n        **self.model_dump(exclude={\"train_params\"}),\n    )\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.SACExtraParameters","title":"<code>SACExtraParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A config model for extra parameters for the Soft Actor-Critic (SAC) agent.</p> <p>Attributes:</p> Name Type Description <code>alpha_lr</code> <code>float</code> <p>the entropy parameter learning rate</p> <code>initial_alpha</code> <code>float</code> <p>the starting entropy coefficient value</p> <code>target_entropy</code> <code>float</code> <p>the target entropy for automatic adjustment</p> <code>log_std_min</code> <code>float | None</code> <p>lower bound for the log standard deviation of the action distribution. Default is <code>None</code></p> <code>log_std_max</code> <code>float | None</code> <p>upper bound for the log standard deviation of the action distribution. Default is <code>None</code></p> Source code in <code>velora/models/config.py</code> Python<pre><code>class SACExtraParameters(BaseModel):\n    \"\"\"\n    A config model for extra parameters for the Soft Actor-Critic (SAC) agent.\n\n    Attributes:\n        alpha_lr: the entropy parameter learning rate\n        initial_alpha: the starting entropy coefficient value\n        target_entropy: the target entropy for automatic adjustment\n        log_std_min: lower bound for the log standard deviation of the\n            action distribution. Default is `None`\n        log_std_max: upper bound for the log standard deviation of the\n            action distribution. Default is `None`\n    \"\"\"\n\n    alpha_lr: float\n    initial_alpha: float\n    target_entropy: float\n    log_std_min: float | None = None\n    log_std_max: float | None = None\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.TorchConfig","title":"<code>TorchConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A config model for PyTorch details.</p> <p>Attributes:</p> Name Type Description <code>device</code> <code>str</code> <p>the device used to train the model</p> <code>optimizer</code> <code>str</code> <p>the type of optimizer used</p> <code>loss</code> <code>str</code> <p>the type of optimizer used</p> Source code in <code>velora/models/config.py</code> Python<pre><code>class TorchConfig(BaseModel):\n    \"\"\"\n    A config model for PyTorch details.\n\n    Attributes:\n        device: the device used to train the model\n        optimizer: the type of optimizer used\n        loss: the type of optimizer used\n    \"\"\"\n\n    device: str\n    optimizer: str\n    loss: str\n</code></pre>"},{"location":"learn/reference/models/config/#velora.models.config.TrainConfig","title":"<code>TrainConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A config model for training parameter details.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>the size of the training batch</p> <code>n_episodes</code> <code>int</code> <p>the total number of episodes trained for. Default is <code>None</code></p> <code>window_size</code> <code>int</code> <p>reward moving average size (in episodes)</p> <code>display_count</code> <code>int</code> <p>console training progress frequency (in episodes)</p> <code>log_freq</code> <code>int</code> <p>metric logging frequency (in episodes)</p> <code>callbacks</code> <code>Dict[str, Any] | None</code> <p>a dictionary of callback details. Default is <code>None</code></p> <code>max_steps</code> <code>int</code> <p>the maximum number of steps per training episode. Default is <code>None</code></p> <code>warmup_steps</code> <code>int</code> <p>number of random steps to take before starting training</p> Source code in <code>velora/models/config.py</code> Python<pre><code>class TrainConfig(BaseModel):\n    \"\"\"\n    A config model for training parameter details.\n\n    Attributes:\n        batch_size: the size of the training batch\n        n_episodes: the total number of episodes trained for. Default is `None`\n        window_size: reward moving average size (in episodes)\n        display_count: console training progress frequency (in episodes)\n        log_freq: metric logging frequency (in episodes)\n        callbacks: a dictionary of callback details. Default is `None`\n        max_steps: the maximum number of steps per training episode.\n            Default is `None`\n        warmup_steps: number of random steps to take before starting\n            training\n    \"\"\"\n\n    batch_size: int\n    n_episodes: int\n    window_size: int\n    display_count: int\n    log_freq: int\n    callbacks: Dict[str, Any] | None = None\n    max_steps: int\n    warmup_steps: int\n</code></pre>"},{"location":"learn/reference/models/lnn/","title":"velora.models.lnn","text":"Documentation <p>Customization: Liquid Networks</p> <p>Liquid Neural Network building blocks.</p>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.cell.NCPLiquidCell","title":"<code>NCPLiquidCell</code>","text":"<p>               Bases: <code>nn.Module</code></p> <p>A Liquid Time-Constant (LTC) cell using a Closed-form (CfC) approach.</p> <p>The LTC cell follows the closed-form continuous-depth (CFC; Equation 10) solution from the paper: Closed-form Continuous-time Neural Models.</p> <p>Equation: $$ x(t) =     \\sigma(-f(x, I, \u03b8_f), t) \\; g(x, I, \u03b8_g)     + \\left[ 1 - \\sigma(-[\\;f(x, I, \u03b8_f)\\;]\\;t) \\right] \\; h(x, I, \u03b8_h) $$</p> Source code in <code>velora/models/lnn/cell.py</code> Python<pre><code>class NCPLiquidCell(nn.Module):\n    \"\"\"\n    A Liquid Time-Constant (LTC) cell using a Closed-form (CfC) approach.\n\n    The LTC cell follows the closed-form continuous-depth\n    (CFC; Equation 10) solution from the paper:\n    [Closed-form Continuous-time Neural Models](https://arxiv.org/abs/2106.13898).\n\n    Equation:\n    $$\n    x(t) =\n        \\\\sigma(-f(x, I, \u03b8_f), t) \\\\; g(x, I, \u03b8_g)\n        + \\\\left[ 1 - \\\\sigma(-[\\\\;f(x, I, \u03b8_f)\\\\;]\\\\;t) \\\\right] \\\\; h(x, I, \u03b8_h)\n    $$\n    \"\"\"\n\n    sparsity_mask: torch.Tensor\n\n    def __init__(\n        self,\n        in_features: int,\n        n_hidden: int,\n        mask: torch.Tensor,\n        *,\n        init_type: str | WeightInitType = \"kaiming_uniform\",\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            in_features (int): number of input nodes.\n            n_hidden (int): number of hidden nodes.\n            mask (torch.Tensor): a matrix of sparse connections\n                usually containing a combination of `[-1, 1, 0]`.\n            init_type (str, optional): the type of weight initialization\n            device (torch.device, optional): the device to load tensors on.\n        \"\"\"\n\n        super().__init__()\n\n        self.in_features = in_features\n        self.n_hidden = n_hidden\n        self.head_size = n_hidden + in_features\n        self.init_type = init_type\n        self.device = device\n\n        # Absolute to maintain masking (-1 -&gt; 1)\n        self.register_buffer(\"sparsity_mask\", self._prep_mask(mask.to(device)))\n\n        self.tanh = nn.Tanh()  # Bounded: [-1, 1]\n        self.sigmoid = nn.Sigmoid()  # Bounded: [0, 1]\n\n        self.g_head = self._make_layer()\n        self.h_head = self._make_layer()\n\n        # LTC heads (f)\n        self.f_head_to_g = self._make_layer()\n        self.f_head_to_h = self._make_layer()\n\n        # Hidden state projection\n        self.proj = self._make_layer()\n\n    def _make_layer(self) -&gt; SparseLinear:\n        \"\"\"\n        Helper method. Creates a new `SparseLinear` layer with the following values:\n\n        - `in_features` - `self.n_hidden + self.in_features`.\n        - `out_features` - `self.n_hidden`.\n        - `mask` - `self.sparsity_mask`.\n        - `device` - `self.device`.\n\n        Returns:\n            layer (SparseLinear): a `SparseLinear` layer.\n        \"\"\"\n        return SparseLinear(\n            self.head_size,\n            self.n_hidden,\n            self.sparsity_mask,\n            init_type=self.init_type,\n            device=self.device,\n        )\n\n    def _prep_mask(self, mask: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Utility method. Preprocesses mask to match head size.\n\n        !!! note \"Performs three operations\"\n\n            1. Adds a padded matrix of 1s to end of mask in shape\n                `(n_extras, n_extras)` where `n_extras=mask.shape[1]`\n            2. Transposes mask from col matrix -&gt; row matrix\n            3. Gets the absolute values of the mask (swapping `-1 -&gt; 1`)\n\n        Parameters:\n            mask (torch.Tensor): weight sparsity mask.\n\n        Returns:\n            mask (torch.Tensor): an updated mask.\n        \"\"\"\n        n_extras = mask.shape[1]\n        extra_nodes = torch.ones((n_extras, n_extras), device=self.device)\n        mask = torch.concatenate([mask.detach(), extra_nodes])\n        return torch.abs(mask.T).to(self.device)\n\n    def _new_hidden(\n        self, x: torch.Tensor, g_out: torch.Tensor, h_out: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Helper method. Computes the new hidden state.\n\n        Parameters:\n            x (torch.Tensor): input values.\n            g_out (torch.Tensor): g_head output.\n            h_out (torch.Tensor): h_head output.\n\n        Returns:\n            hidden (torch.Tensor): a new hidden state\n        \"\"\"\n        g_head = self.tanh(g_out)  # g(x, I, \u03b8_g)\n        h_head = self.tanh(h_out)  # h(x, I, \u03b8_h)\n\n        fh_g = self.f_head_to_g(x)\n        fh_h = self.f_head_to_h(x)\n\n        gate_out = self.sigmoid(fh_g + fh_h)  # [1 - \u03c3(-[f(x, I, \u03b8f)], t)]\n        f_head = 1.0 - gate_out  # \u03c3(-f(x, I, \u03b8f), t)\n\n        return g_head * f_head + gate_out * h_head\n\n    def update_mask(self, mask: torch.Tensor) -&gt; None:\n        \"\"\"\n        Updates the sparsity mask with a new one.\n\n        Parameters:\n            mask (torch.Tensor): new mask\n        \"\"\"\n        self.sparsity_mask = self._prep_mask(mask.to(self.device))\n\n    def forward(\n        self, x: torch.Tensor, hidden: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a forward pass through the cell.\n\n        Parameters:\n            x (torch.Tensor): input values.\n            hidden (torch.Tensor): current hidden state.\n\n        Returns:\n            y_pred (torch.Tensor): the cell prediction.\n            h_state (torch.Tensor): the hidden state.\n        \"\"\"\n        x, hidden = x.to(self.device), hidden.to(self.device)\n        x = torch.cat([x, hidden], dim=1)\n\n        g_out = self.g_head(x)\n        h_out = self.h_head(x)\n\n        new_hidden = self._new_hidden(x, g_out, h_out)\n        y_pred = self.proj(x) + new_hidden\n        return y_pred, new_hidden\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.cell.NCPLiquidCell.__init__","title":"<code>__init__(in_features, n_hidden, mask, *, init_type='kaiming_uniform', device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>number of input nodes.</p> required <code>n_hidden</code> <code>int</code> <p>number of hidden nodes.</p> required <code>mask</code> <code>torch.Tensor</code> <p>a matrix of sparse connections usually containing a combination of <code>[-1, 1, 0]</code>.</p> required <code>init_type</code> <code>str</code> <p>the type of weight initialization</p> <code>'kaiming_uniform'</code> <code>device</code> <code>torch.device</code> <p>the device to load tensors on.</p> <code>None</code> Source code in <code>velora/models/lnn/cell.py</code> Python<pre><code>def __init__(\n    self,\n    in_features: int,\n    n_hidden: int,\n    mask: torch.Tensor,\n    *,\n    init_type: str | WeightInitType = \"kaiming_uniform\",\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        in_features (int): number of input nodes.\n        n_hidden (int): number of hidden nodes.\n        mask (torch.Tensor): a matrix of sparse connections\n            usually containing a combination of `[-1, 1, 0]`.\n        init_type (str, optional): the type of weight initialization\n        device (torch.device, optional): the device to load tensors on.\n    \"\"\"\n\n    super().__init__()\n\n    self.in_features = in_features\n    self.n_hidden = n_hidden\n    self.head_size = n_hidden + in_features\n    self.init_type = init_type\n    self.device = device\n\n    # Absolute to maintain masking (-1 -&gt; 1)\n    self.register_buffer(\"sparsity_mask\", self._prep_mask(mask.to(device)))\n\n    self.tanh = nn.Tanh()  # Bounded: [-1, 1]\n    self.sigmoid = nn.Sigmoid()  # Bounded: [0, 1]\n\n    self.g_head = self._make_layer()\n    self.h_head = self._make_layer()\n\n    # LTC heads (f)\n    self.f_head_to_g = self._make_layer()\n    self.f_head_to_h = self._make_layer()\n\n    # Hidden state projection\n    self.proj = self._make_layer()\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.cell.NCPLiquidCell.forward","title":"<code>forward(x, hidden)</code>","text":"<p>Performs a forward pass through the cell.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>input values.</p> required <code>hidden</code> <code>torch.Tensor</code> <p>current hidden state.</p> required <p>Returns:</p> Name Type Description <code>y_pred</code> <code>torch.Tensor</code> <p>the cell prediction.</p> <code>h_state</code> <code>torch.Tensor</code> <p>the hidden state.</p> Source code in <code>velora/models/lnn/cell.py</code> Python<pre><code>def forward(\n    self, x: torch.Tensor, hidden: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a forward pass through the cell.\n\n    Parameters:\n        x (torch.Tensor): input values.\n        hidden (torch.Tensor): current hidden state.\n\n    Returns:\n        y_pred (torch.Tensor): the cell prediction.\n        h_state (torch.Tensor): the hidden state.\n    \"\"\"\n    x, hidden = x.to(self.device), hidden.to(self.device)\n    x = torch.cat([x, hidden], dim=1)\n\n    g_out = self.g_head(x)\n    h_out = self.h_head(x)\n\n    new_hidden = self._new_hidden(x, g_out, h_out)\n    y_pred = self.proj(x) + new_hidden\n    return y_pred, new_hidden\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.cell.NCPLiquidCell.update_mask","title":"<code>update_mask(mask)</code>","text":"<p>Updates the sparsity mask with a new one.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>torch.Tensor</code> <p>new mask</p> required Source code in <code>velora/models/lnn/cell.py</code> Python<pre><code>def update_mask(self, mask: torch.Tensor) -&gt; None:\n    \"\"\"\n    Updates the sparsity mask with a new one.\n\n    Parameters:\n        mask (torch.Tensor): new mask\n    \"\"\"\n    self.sparsity_mask = self._prep_mask(mask.to(self.device))\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.LiquidNCPNetwork","title":"<code>LiquidNCPNetwork</code>","text":"<p>               Bases: <code>nn.Module</code></p> <p>A CfC Liquid Neural Circuit Policy (NCP) Network with three layers:</p> <ol> <li>Inter (input) - a <code>SparseLinear</code> layer</li> <li>Command (hidden) - a <code>NCPLiquidCell</code> layer</li> <li>Motor (output) - a <code>SparseLinear</code> layer</li> </ol> Decision nodes <p><code>inter</code> and <code>command</code> neurons are automatically calculated using:</p> Python<pre><code>command_neurons = max(int(0.4 * n_neurons), 1)\ninter_neurons = n_neurons - command_neurons\n</code></pre> <p>Combines a Liquid Time-Constant (LTC) cell with Ordinary Neural Circuits (ONCs). Paper references:</p> <ul> <li>Closed-form Continuous-time Neural Models</li> <li>Reinforcement Learning with Ordinary Neural Circuits</li> </ul> Source code in <code>velora/models/lnn/ncp.py</code> Python<pre><code>class LiquidNCPNetwork(nn.Module):\n    \"\"\"\n    A CfC Liquid Neural Circuit Policy (NCP) Network with three layers:\n\n    1. Inter (input) - a `SparseLinear` layer\n    2. Command (hidden) - a `NCPLiquidCell` layer\n    3. Motor (output) - a `SparseLinear` layer\n\n    ??? note \"Decision nodes\"\n\n        `inter` and `command` neurons are automatically calculated using:\n\n        ```python\n        command_neurons = max(int(0.4 * n_neurons), 1)\n        inter_neurons = n_neurons - command_neurons\n        ```\n\n    Combines a Liquid Time-Constant (LTC) cell with Ordinary Neural Circuits (ONCs). Paper references:\n\n    - [Closed-form Continuous-time Neural Models](https://arxiv.org/abs/2106.13898)\n    - [Reinforcement Learning with Ordinary Neural Circuits](https://proceedings.mlr.press/v119/hasani20a.html)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        n_neurons: int,\n        out_features: int,\n        *,\n        sparsity_level: float = 0.5,\n        init_type: str | WeightInitType = \"kaiming_uniform\",\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            in_features (int): number of inputs (sensory nodes)\n            n_neurons (int): number of decision nodes (inter and command nodes)\n            out_features (int): number of out features (motor nodes)\n            sparsity_level (float, optional): controls the connection sparsity\n                between neurons.\n\n                Must be a value between `[0.1, 0.9]` -\n\n                - When `0.1` neurons are very dense.\n                - When `0.9` they are very sparse.\n\n            init_type (str, optional): the type of weight initialization\n            device (torch.device, optional): the device to load tensors on\n        \"\"\"\n        super().__init__()\n\n        self.in_features = in_features\n        self.n_neurons = n_neurons\n        self.out_features = out_features\n        self.device = device\n\n        self.n_units = n_neurons + out_features  # inter + command + motor\n\n        self.wiring = Wiring(\n            in_features,\n            n_neurons,\n            out_features,\n            sparsity_level=sparsity_level,\n        )\n        self.masks, self.counts = self.wiring.data()\n\n        self.inter = SparseLinear(\n            in_features,\n            self.counts.inter,\n            torch.abs(self.masks.inter.T),\n            init_type=init_type,\n            device=device,\n        ).to(device)\n\n        self.command = NCPLiquidCell(\n            self.counts.inter,\n            self.counts.command,\n            self.masks.command,\n            init_type=init_type,\n            device=device,\n        ).to(device)\n        self.hidden_size = self.counts.command\n\n        self.motor = SparseLinear(\n            self.counts.command,\n            self.counts.motor,\n            torch.abs(self.masks.motor.T),\n            init_type=init_type,\n            device=device,\n        ).to(device)\n\n        self.act = nn.Mish()\n\n        self._total_params = total_parameters(self)\n        self._active_params = active_parameters(self)\n\n    @property\n    def total_params(self) -&gt; int:\n        \"\"\"\n        Gets the network's total parameter count.\n\n        Returns:\n            count (int): the total parameter count.\n        \"\"\"\n        return self._total_params\n\n    @property\n    def active_params(self) -&gt; int:\n        \"\"\"\n        Gets the network's active parameter count.\n\n        Returns:\n            count (int): the active parameter count.\n        \"\"\"\n        return self._active_params\n\n    def forward(\n        self, x: torch.Tensor, h_state: Optional[torch.Tensor] = None\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            x (torch.Tensor): an input tensor of shape: `(batch_size, features)`.\n\n                - `batch_size` the number of samples per timestep.\n                - `features` the features at each timestep (e.g.,\n                image features, joint coordinates, word embeddings, raw amplitude\n                values).\n            h_state (torch.Tensor, optional): initial hidden state of the RNN with\n                shape: `(batch_size, n_units)`.\n\n                - `batch_size` the number of samples.\n                - `n_units` the total number of hidden neurons\n                    (`n_neurons + out_features`).\n\n        Returns:\n            y_pred (torch.Tensor): the network prediction. When `batch_size=1`. Out shape is `(out_features)`. Otherwise, `(batch_size, out_features)`.\n            h_state (torch.Tensor): the final hidden state. Output shape is `(batch_size, n_units)`.\n        \"\"\"\n        if x.dim() != 2:\n            raise ValueError(\n                f\"Unsupported dimensionality: '{x.shape}'. Should be 2 dimensional with: '(batch_size, features)'.\"\n            )\n\n        x = x.to(dtype=torch.float32, device=self.device)\n\n        batch_size, features = x.size()\n\n        if h_state is None:\n            h_state = torch.zeros(\n                (batch_size, self.hidden_size),\n                device=self.device,\n            )\n\n        # Batch -&gt; (batch_size, out_features)\n        x = self.act(self.inter(x))\n        x, h_state = self.command(x, h_state.to(self.device))\n        y_pred: torch.Tensor = self.motor(self.act(x))\n\n        # Single item -&gt; (out_features)\n        if y_pred.shape[0] == 1:\n            y_pred = y_pred.squeeze(0)\n\n        # h_state -&gt; (batch_size, n_units)\n        return y_pred, h_state\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.LiquidNCPNetwork.active_params","title":"<code>active_params</code>  <code>property</code>","text":"<p>Gets the network's active parameter count.</p> <p>Returns:</p> Name Type Description <code>count</code> <code>int</code> <p>the active parameter count.</p>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.LiquidNCPNetwork.total_params","title":"<code>total_params</code>  <code>property</code>","text":"<p>Gets the network's total parameter count.</p> <p>Returns:</p> Name Type Description <code>count</code> <code>int</code> <p>the total parameter count.</p>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.LiquidNCPNetwork.__init__","title":"<code>__init__(in_features, n_neurons, out_features, *, sparsity_level=0.5, init_type='kaiming_uniform', device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>number of inputs (sensory nodes)</p> required <code>n_neurons</code> <code>int</code> <p>number of decision nodes (inter and command nodes)</p> required <code>out_features</code> <code>int</code> <p>number of out features (motor nodes)</p> required <code>sparsity_level</code> <code>float</code> <p>controls the connection sparsity between neurons.</p> <p>Must be a value between <code>[0.1, 0.9]</code> -</p> <ul> <li>When <code>0.1</code> neurons are very dense.</li> <li>When <code>0.9</code> they are very sparse.</li> </ul> <code>0.5</code> <code>init_type</code> <code>str</code> <p>the type of weight initialization</p> <code>'kaiming_uniform'</code> <code>device</code> <code>torch.device</code> <p>the device to load tensors on</p> <code>None</code> Source code in <code>velora/models/lnn/ncp.py</code> Python<pre><code>def __init__(\n    self,\n    in_features: int,\n    n_neurons: int,\n    out_features: int,\n    *,\n    sparsity_level: float = 0.5,\n    init_type: str | WeightInitType = \"kaiming_uniform\",\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        in_features (int): number of inputs (sensory nodes)\n        n_neurons (int): number of decision nodes (inter and command nodes)\n        out_features (int): number of out features (motor nodes)\n        sparsity_level (float, optional): controls the connection sparsity\n            between neurons.\n\n            Must be a value between `[0.1, 0.9]` -\n\n            - When `0.1` neurons are very dense.\n            - When `0.9` they are very sparse.\n\n        init_type (str, optional): the type of weight initialization\n        device (torch.device, optional): the device to load tensors on\n    \"\"\"\n    super().__init__()\n\n    self.in_features = in_features\n    self.n_neurons = n_neurons\n    self.out_features = out_features\n    self.device = device\n\n    self.n_units = n_neurons + out_features  # inter + command + motor\n\n    self.wiring = Wiring(\n        in_features,\n        n_neurons,\n        out_features,\n        sparsity_level=sparsity_level,\n    )\n    self.masks, self.counts = self.wiring.data()\n\n    self.inter = SparseLinear(\n        in_features,\n        self.counts.inter,\n        torch.abs(self.masks.inter.T),\n        init_type=init_type,\n        device=device,\n    ).to(device)\n\n    self.command = NCPLiquidCell(\n        self.counts.inter,\n        self.counts.command,\n        self.masks.command,\n        init_type=init_type,\n        device=device,\n    ).to(device)\n    self.hidden_size = self.counts.command\n\n    self.motor = SparseLinear(\n        self.counts.command,\n        self.counts.motor,\n        torch.abs(self.masks.motor.T),\n        init_type=init_type,\n        device=device,\n    ).to(device)\n\n    self.act = nn.Mish()\n\n    self._total_params = total_parameters(self)\n    self._active_params = active_parameters(self)\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.LiquidNCPNetwork.forward","title":"<code>forward(x, h_state=None)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>an input tensor of shape: <code>(batch_size, features)</code>.</p> <ul> <li><code>batch_size</code> the number of samples per timestep.</li> <li><code>features</code> the features at each timestep (e.g., image features, joint coordinates, word embeddings, raw amplitude values).</li> </ul> required <code>h_state</code> <code>torch.Tensor</code> <p>initial hidden state of the RNN with shape: <code>(batch_size, n_units)</code>.</p> <ul> <li><code>batch_size</code> the number of samples.</li> <li><code>n_units</code> the total number of hidden neurons     (<code>n_neurons + out_features</code>).</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>y_pred</code> <code>torch.Tensor</code> <p>the network prediction. When <code>batch_size=1</code>. Out shape is <code>(out_features)</code>. Otherwise, <code>(batch_size, out_features)</code>.</p> <code>h_state</code> <code>torch.Tensor</code> <p>the final hidden state. Output shape is <code>(batch_size, n_units)</code>.</p> Source code in <code>velora/models/lnn/ncp.py</code> Python<pre><code>def forward(\n    self, x: torch.Tensor, h_state: Optional[torch.Tensor] = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        x (torch.Tensor): an input tensor of shape: `(batch_size, features)`.\n\n            - `batch_size` the number of samples per timestep.\n            - `features` the features at each timestep (e.g.,\n            image features, joint coordinates, word embeddings, raw amplitude\n            values).\n        h_state (torch.Tensor, optional): initial hidden state of the RNN with\n            shape: `(batch_size, n_units)`.\n\n            - `batch_size` the number of samples.\n            - `n_units` the total number of hidden neurons\n                (`n_neurons + out_features`).\n\n    Returns:\n        y_pred (torch.Tensor): the network prediction. When `batch_size=1`. Out shape is `(out_features)`. Otherwise, `(batch_size, out_features)`.\n        h_state (torch.Tensor): the final hidden state. Output shape is `(batch_size, n_units)`.\n    \"\"\"\n    if x.dim() != 2:\n        raise ValueError(\n            f\"Unsupported dimensionality: '{x.shape}'. Should be 2 dimensional with: '(batch_size, features)'.\"\n        )\n\n    x = x.to(dtype=torch.float32, device=self.device)\n\n    batch_size, features = x.size()\n\n    if h_state is None:\n        h_state = torch.zeros(\n            (batch_size, self.hidden_size),\n            device=self.device,\n        )\n\n    # Batch -&gt; (batch_size, out_features)\n    x = self.act(self.inter(x))\n    x, h_state = self.command(x, h_state.to(self.device))\n    y_pred: torch.Tensor = self.motor(self.act(x))\n\n    # Single item -&gt; (out_features)\n    if y_pred.shape[0] == 1:\n        y_pred = y_pred.squeeze(0)\n\n    # h_state -&gt; (batch_size, n_units)\n    return y_pred, h_state\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.NCPNetwork","title":"<code>NCPNetwork</code>","text":"<p>               Bases: <code>nn.Module</code></p> <p>A Neural Circuit Policy (NCP) Network with three layers:</p> <ol> <li>Inter (input) - a <code>SparseLinear</code> layer</li> <li>Command (hidden) - a <code>SparseLinear</code> layer</li> <li>Motor (output) - a <code>SparseLinear</code> layer</li> </ol> <p>Uses the Mish activation function between each layer.</p> Decision nodes <p><code>inter</code> and <code>command</code> neurons are automatically calculated using:</p> Python<pre><code>command_neurons = max(int(0.4 * n_neurons), 1)\ninter_neurons = n_neurons - command_neurons\n</code></pre> <p>Uses an Ordinary Neural Circuit (ONC) architecture without Liquid dynamics. Paper references:</p> <ul> <li>Reinforcement Learning with Ordinary Neural Circuits</li> <li>Mish: A Self Regularized Non-Monotonic Activation Function</li> </ul> Source code in <code>velora/models/lnn/ncp.py</code> Python<pre><code>class NCPNetwork(nn.Module):\n    \"\"\"\n    A Neural Circuit Policy (NCP) Network with three layers:\n\n    1. Inter (input) - a `SparseLinear` layer\n    2. Command (hidden) - a `SparseLinear` layer\n    3. Motor (output) - a `SparseLinear` layer\n\n    Uses the Mish activation function between each layer.\n\n    ??? note \"Decision nodes\"\n\n        `inter` and `command` neurons are automatically calculated using:\n\n        ```python\n        command_neurons = max(int(0.4 * n_neurons), 1)\n        inter_neurons = n_neurons - command_neurons\n        ```\n\n    Uses an Ordinary Neural Circuit (ONC) architecture without Liquid dynamics.\n    Paper references:\n\n    - [Reinforcement Learning with Ordinary Neural Circuits](https://proceedings.mlr.press/v119/hasani20a.html)\n    - [Mish: A Self Regularized Non-Monotonic Activation Function](https://arxiv.org/abs/1908.08681)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        n_neurons: int,\n        out_features: int,\n        *,\n        sparsity_level: float = 0.5,\n        init_type: str | WeightInitType = \"kaiming_uniform\",\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            in_features (int): number of inputs (sensory nodes)\n            n_neurons (int): number of decision nodes (inter and command nodes)\n            out_features (int): number of out features (motor nodes)\n            sparsity_level (float, optional): controls the connection sparsity\n                between neurons.\n\n                Must be a value between `[0.1, 0.9]` -\n\n                - When `0.1` neurons are very dense.\n                - When `0.9` they are very sparse.\n\n            init_type (str, optional): the type of weight initialization\n            device (torch.device, optional): the device to load tensors on\n        \"\"\"\n        super().__init__()\n\n        self.in_features = in_features\n        self.n_neurons = n_neurons\n        self.out_features = out_features\n        self.device = device\n\n        self.n_units = n_neurons + out_features  # inter + command + motor\n\n        self.wiring = Wiring(\n            in_features,\n            n_neurons,\n            out_features,\n            sparsity_level=sparsity_level,\n        )\n        self.masks, self.counts = self.wiring.data()\n\n        self.ncp = nn.Sequential(\n            SparseLinear(\n                in_features,\n                self.counts.inter,\n                torch.abs(self.masks.inter.T),\n                init_type=init_type,\n                device=device,\n            ),\n            nn.Mish(),\n            SparseLinear(\n                self.counts.inter,\n                self.counts.command,\n                torch.abs(self.masks.command.T),\n                init_type=init_type,\n                device=device,\n            ),\n            nn.Mish(),\n            SparseLinear(\n                self.counts.command,\n                self.counts.motor,\n                torch.abs(self.masks.motor.T),\n                init_type=init_type,\n                device=device,\n            ),\n        ).to(device)\n\n        self._total_params = total_parameters(self)\n        self._active_params = active_parameters(self)\n\n    @property\n    def total_params(self) -&gt; int:\n        \"\"\"\n        Gets the network's total parameter count.\n\n        Returns:\n            count (int): the total parameter count.\n        \"\"\"\n        return self._total_params\n\n    @property\n    def active_params(self) -&gt; int:\n        \"\"\"\n        Gets the network's active parameter count.\n\n        Returns:\n            count (int): the active parameter count.\n        \"\"\"\n        return self._active_params\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            x (torch.Tensor): an input tensor of shape: `(batch_size, features)`.\n\n                - `batch_size` the number of samples per timestep.\n                - `features` the features at each timestep (e.g.,\n                image features, joint coordinates, word embeddings, raw amplitude\n                values).\n\n        Returns:\n            y_pred (torch.Tensor): the network prediction. When `batch_size=1`. Out shape is `(out_features)`. Otherwise, `(batch_size, out_features)`.\n        \"\"\"\n        if x.dim() != 2:\n            raise ValueError(\n                f\"Unsupported dimensionality: '{x.shape}'. Should be 2 dimensional with: '(batch_size, features)'.\"\n            )\n\n        x = x.to(dtype=torch.float32, device=self.device)\n\n        # Batch -&gt; (batch_size, out_features)\n        y_pred: torch.Tensor = self.ncp(x)\n\n        # Single item -&gt; (out_features)\n        if y_pred.shape[0] == 1:\n            y_pred = y_pred.squeeze(0)\n\n        return y_pred\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.NCPNetwork.active_params","title":"<code>active_params</code>  <code>property</code>","text":"<p>Gets the network's active parameter count.</p> <p>Returns:</p> Name Type Description <code>count</code> <code>int</code> <p>the active parameter count.</p>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.NCPNetwork.total_params","title":"<code>total_params</code>  <code>property</code>","text":"<p>Gets the network's total parameter count.</p> <p>Returns:</p> Name Type Description <code>count</code> <code>int</code> <p>the total parameter count.</p>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.NCPNetwork.__init__","title":"<code>__init__(in_features, n_neurons, out_features, *, sparsity_level=0.5, init_type='kaiming_uniform', device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>number of inputs (sensory nodes)</p> required <code>n_neurons</code> <code>int</code> <p>number of decision nodes (inter and command nodes)</p> required <code>out_features</code> <code>int</code> <p>number of out features (motor nodes)</p> required <code>sparsity_level</code> <code>float</code> <p>controls the connection sparsity between neurons.</p> <p>Must be a value between <code>[0.1, 0.9]</code> -</p> <ul> <li>When <code>0.1</code> neurons are very dense.</li> <li>When <code>0.9</code> they are very sparse.</li> </ul> <code>0.5</code> <code>init_type</code> <code>str</code> <p>the type of weight initialization</p> <code>'kaiming_uniform'</code> <code>device</code> <code>torch.device</code> <p>the device to load tensors on</p> <code>None</code> Source code in <code>velora/models/lnn/ncp.py</code> Python<pre><code>def __init__(\n    self,\n    in_features: int,\n    n_neurons: int,\n    out_features: int,\n    *,\n    sparsity_level: float = 0.5,\n    init_type: str | WeightInitType = \"kaiming_uniform\",\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        in_features (int): number of inputs (sensory nodes)\n        n_neurons (int): number of decision nodes (inter and command nodes)\n        out_features (int): number of out features (motor nodes)\n        sparsity_level (float, optional): controls the connection sparsity\n            between neurons.\n\n            Must be a value between `[0.1, 0.9]` -\n\n            - When `0.1` neurons are very dense.\n            - When `0.9` they are very sparse.\n\n        init_type (str, optional): the type of weight initialization\n        device (torch.device, optional): the device to load tensors on\n    \"\"\"\n    super().__init__()\n\n    self.in_features = in_features\n    self.n_neurons = n_neurons\n    self.out_features = out_features\n    self.device = device\n\n    self.n_units = n_neurons + out_features  # inter + command + motor\n\n    self.wiring = Wiring(\n        in_features,\n        n_neurons,\n        out_features,\n        sparsity_level=sparsity_level,\n    )\n    self.masks, self.counts = self.wiring.data()\n\n    self.ncp = nn.Sequential(\n        SparseLinear(\n            in_features,\n            self.counts.inter,\n            torch.abs(self.masks.inter.T),\n            init_type=init_type,\n            device=device,\n        ),\n        nn.Mish(),\n        SparseLinear(\n            self.counts.inter,\n            self.counts.command,\n            torch.abs(self.masks.command.T),\n            init_type=init_type,\n            device=device,\n        ),\n        nn.Mish(),\n        SparseLinear(\n            self.counts.command,\n            self.counts.motor,\n            torch.abs(self.masks.motor.T),\n            init_type=init_type,\n            device=device,\n        ),\n    ).to(device)\n\n    self._total_params = total_parameters(self)\n    self._active_params = active_parameters(self)\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.ncp.NCPNetwork.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>an input tensor of shape: <code>(batch_size, features)</code>.</p> <ul> <li><code>batch_size</code> the number of samples per timestep.</li> <li><code>features</code> the features at each timestep (e.g., image features, joint coordinates, word embeddings, raw amplitude values).</li> </ul> required <p>Returns:</p> Name Type Description <code>y_pred</code> <code>torch.Tensor</code> <p>the network prediction. When <code>batch_size=1</code>. Out shape is <code>(out_features)</code>. Otherwise, <code>(batch_size, out_features)</code>.</p> Source code in <code>velora/models/lnn/ncp.py</code> Python<pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        x (torch.Tensor): an input tensor of shape: `(batch_size, features)`.\n\n            - `batch_size` the number of samples per timestep.\n            - `features` the features at each timestep (e.g.,\n            image features, joint coordinates, word embeddings, raw amplitude\n            values).\n\n    Returns:\n        y_pred (torch.Tensor): the network prediction. When `batch_size=1`. Out shape is `(out_features)`. Otherwise, `(batch_size, out_features)`.\n    \"\"\"\n    if x.dim() != 2:\n        raise ValueError(\n            f\"Unsupported dimensionality: '{x.shape}'. Should be 2 dimensional with: '(batch_size, features)'.\"\n        )\n\n    x = x.to(dtype=torch.float32, device=self.device)\n\n    # Batch -&gt; (batch_size, out_features)\n    y_pred: torch.Tensor = self.ncp(x)\n\n    # Single item -&gt; (out_features)\n    if y_pred.shape[0] == 1:\n        y_pred = y_pred.squeeze(0)\n\n    return y_pred\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.sparse.SparseLinear","title":"<code>SparseLinear</code>","text":"<p>               Bases: <code>nn.Module</code></p> <p>A <code>torch.nn.Linear</code> layer with sparsely weighted connections.</p> Source code in <code>velora/models/lnn/sparse.py</code> Python<pre><code>class SparseLinear(nn.Module):\n    \"\"\"A `torch.nn.Linear` layer with sparsely weighted connections.\"\"\"\n\n    bias: torch.Tensor\n    mask: torch.Tensor\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        mask: torch.Tensor,\n        *,\n        init_type: str | WeightInitType = \"kaiming_uniform\",\n        bias: bool = True,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            in_features (int): number of input features\n            out_features (int): number of output features\n            mask (torch.Tensor): sparsity mask tensor of shape\n                `(out_features, in_features)`\n            init_type (str, optional): the type of weight initialization\n            bias (bool, optional): a flag to enable additive bias\n            device (torch.device, optional): device to perform computations on\n        \"\"\"\n        super().__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.device = device\n\n        self.register_buffer(\"mask\", mask.to(device).detach())\n\n        weight = torch.empty((out_features, in_features), device=device)\n        self.weight = nn.Parameter(weight)\n\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_features, device=device))\n        else:\n            self.register_parameter(\"bias\", None)\n\n        self.reset_parameters(init_type)\n\n        with torch.no_grad():\n            self.weight.data.mul_(self.mask)\n\n    def reset_parameters(self, style: str | WeightInitType) -&gt; None:\n        \"\"\"\n        Initializes weights and biases using an initialization method.\n        \"\"\"\n        weight_fn = get_init_fn(style)\n        weight_fn(self)\n\n    def update_mask(self, mask: torch.Tensor) -&gt; None:\n        \"\"\"\n        Updates the sparsity mask with a new one.\n\n        Parameters:\n            mask (torch.Tensor): new mask\n        \"\"\"\n        self.mask = mask.to(self.device).detach()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Perform a forward pass through the layer.\n\n        Parameters:\n            x (torch.Tensor): input tensor with shape `(..., in_features)`\n\n        Returns:\n            y_pred (torch.Tensor): layer prediction with sparsity applied with shape `(..., out_features)`.\n        \"\"\"\n        return F.linear(x, self.weight * self.mask, self.bias)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String representation of layer parameters.\"\"\"\n        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.sparse.SparseLinear.__init__","title":"<code>__init__(in_features, out_features, mask, *, init_type='kaiming_uniform', bias=True, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>number of input features</p> required <code>out_features</code> <code>int</code> <p>number of output features</p> required <code>mask</code> <code>torch.Tensor</code> <p>sparsity mask tensor of shape <code>(out_features, in_features)</code></p> required <code>init_type</code> <code>str</code> <p>the type of weight initialization</p> <code>'kaiming_uniform'</code> <code>bias</code> <code>bool</code> <p>a flag to enable additive bias</p> <code>True</code> <code>device</code> <code>torch.device</code> <p>device to perform computations on</p> <code>None</code> Source code in <code>velora/models/lnn/sparse.py</code> Python<pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    mask: torch.Tensor,\n    *,\n    init_type: str | WeightInitType = \"kaiming_uniform\",\n    bias: bool = True,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        in_features (int): number of input features\n        out_features (int): number of output features\n        mask (torch.Tensor): sparsity mask tensor of shape\n            `(out_features, in_features)`\n        init_type (str, optional): the type of weight initialization\n        bias (bool, optional): a flag to enable additive bias\n        device (torch.device, optional): device to perform computations on\n    \"\"\"\n    super().__init__()\n\n    self.in_features = in_features\n    self.out_features = out_features\n    self.device = device\n\n    self.register_buffer(\"mask\", mask.to(device).detach())\n\n    weight = torch.empty((out_features, in_features), device=device)\n    self.weight = nn.Parameter(weight)\n\n    if bias:\n        self.bias = nn.Parameter(torch.empty(out_features, device=device))\n    else:\n        self.register_parameter(\"bias\", None)\n\n    self.reset_parameters(init_type)\n\n    with torch.no_grad():\n        self.weight.data.mul_(self.mask)\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.sparse.SparseLinear.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String representation of layer parameters.</p> Source code in <code>velora/models/lnn/sparse.py</code> Python<pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String representation of layer parameters.\"\"\"\n    return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.sparse.SparseLinear.forward","title":"<code>forward(x)</code>","text":"<p>Perform a forward pass through the layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>input tensor with shape <code>(..., in_features)</code></p> required <p>Returns:</p> Name Type Description <code>y_pred</code> <code>torch.Tensor</code> <p>layer prediction with sparsity applied with shape <code>(..., out_features)</code>.</p> Source code in <code>velora/models/lnn/sparse.py</code> Python<pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a forward pass through the layer.\n\n    Parameters:\n        x (torch.Tensor): input tensor with shape `(..., in_features)`\n\n    Returns:\n        y_pred (torch.Tensor): layer prediction with sparsity applied with shape `(..., out_features)`.\n    \"\"\"\n    return F.linear(x, self.weight * self.mask, self.bias)\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.sparse.SparseLinear.reset_parameters","title":"<code>reset_parameters(style)</code>","text":"<p>Initializes weights and biases using an initialization method.</p> Source code in <code>velora/models/lnn/sparse.py</code> Python<pre><code>def reset_parameters(self, style: str | WeightInitType) -&gt; None:\n    \"\"\"\n    Initializes weights and biases using an initialization method.\n    \"\"\"\n    weight_fn = get_init_fn(style)\n    weight_fn(self)\n</code></pre>"},{"location":"learn/reference/models/lnn/#velora.models.lnn.sparse.SparseLinear.update_mask","title":"<code>update_mask(mask)</code>","text":"<p>Updates the sparsity mask with a new one.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>torch.Tensor</code> <p>new mask</p> required Source code in <code>velora/models/lnn/sparse.py</code> Python<pre><code>def update_mask(self, mask: torch.Tensor) -&gt; None:\n    \"\"\"\n    Updates the sparsity mask with a new one.\n\n    Parameters:\n        mask (torch.Tensor): new mask\n    \"\"\"\n    self.mask = mask.to(self.device).detach()\n</code></pre>"},{"location":"learn/reference/models/modules/","title":"velora.models.nf.modules","text":"Documentation <p>Customization: Modules</p> <p>Class-based modules used inside Velora's agents, simplifying underlying PyTorch functionality.</p>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModule","title":"<code>ActorModule</code>","text":"<p>               Bases: <code>BaseModule</code></p> <p>An Actor module for NeuroFlow. Uses a Liquid NCP SAC Actor with a Gaussian policy.</p> <p>Usable with continuous action spaces.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>class ActorModule(BaseModule):\n    \"\"\"\n    An Actor module for NeuroFlow. Uses a Liquid NCP SAC Actor with a\n    Gaussian policy.\n\n    Usable with continuous action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim: int,\n        n_neurons: int,\n        action_dim: int,\n        action_scale: torch.Tensor,\n        action_bias: torch.Tensor,\n        *,\n        log_std_min: float = -5,\n        log_std_max: float = 2,\n        optim: Type[optim.Optimizer] = optim.Adam,\n        lr: float = 3e-4,\n        device: torch.device | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            state_dim (int): dimension of the state space\n            n_neurons (int): number of decision/hidden neurons\n            action_dim (int): dimension of the action space\n            action_scale (torch.Tensor): scale factor to map normalized actions to\n                environment's action range\n            action_bias (torch.Tensor): bias/offset to center normalized actions to\n                environment's action range\n            log_std_min (float, optional): minimum log standard deviation\n            log_std_max (float, optional): maximum log standard deviation\n            optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n            lr (float, optional): optimizer learning rate\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        self.state_dim = state_dim\n        self.n_neurons = n_neurons\n        self.action_dim = action_dim\n        self.action_scale = action_scale\n        self.action_bias = action_bias\n        self.log_std = (log_std_min, log_std_max)\n        self.lr = lr\n        self.device = device\n\n        self.network = SACActor(\n            state_dim,\n            n_neurons,\n            action_dim,\n            action_scale,\n            action_bias,\n            log_std_min=log_std_min,\n            log_std_max=log_std_max,\n            device=device,\n        ).to(device)\n\n        self.hidden_size = self.network.ncp.hidden_size\n\n        self.optim = optim(self.network.parameters(), lr=lr)\n\n        self.config = self.network.config()\n\n        self.active_params = self.config.active_params\n        self.total_params = self.config.total_params\n\n        self.network: SACActor = torch.jit.script(self.network)\n\n    def gradient_step(self, loss: torch.Tensor) -&gt; None:\n        \"\"\"\n        Performs a gradient update step.\n\n        Parameters:\n            loss (torch.Tensor): network loss\n        \"\"\"\n        self.optim.zero_grad()\n        loss.backward()\n        self.optim.step()\n\n    def predict(\n        self,\n        obs: torch.Tensor,\n        hidden: torch.Tensor | None = None,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Makes a deterministic prediction using the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            hidden (torch.Tensor, optional): the current hidden state\n\n        Returns:\n            actions (torch.Tensor): the action predictions.\n            hidden (torch.Tensor): the new hidden state.\n        \"\"\"\n        action, hidden = self.network.predict(obs, hidden)\n        return action, hidden\n\n    def forward(\n        self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            hidden (torch.Tensor, optional): the current hidden state\n\n        Returns:\n            actions (torch.Tensor): the action predictions.\n            log_prob (torch.Tensor): log probabilities of actions.\n            hidden (torch.Tensor): the new hidden state.\n        \"\"\"\n        action, log_prob, hidden = self.network(obs, hidden)\n        return action, log_prob, hidden\n\n    def eval_mode(self) -&gt; None:\n        \"\"\"Sets the network to evaluation mode.\"\"\"\n        self.network.eval()\n\n    def train_mode(self) -&gt; None:\n        \"\"\"Sets the network to training mode.\"\"\"\n        self.network.train()\n\n    def state_dict(self) -&gt; Dict[str, Dict[str, Any]]:\n        return {\n            \"actor\": self.network.state_dict(),\n            \"actor_optim\": self.optim.state_dict(),\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Dict[str, Any]]) -&gt; None:\n        self.network.load_state_dict(state_dict[\"actor\"])\n        self.optim.load_state_dict(state_dict[\"actor_optim\"])\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"state_dim={self.state_dim}, \"\n            f\"n_neurons={self.n_neurons}, \"\n            f\"action_dim={self.action_dim}, \"\n            f\"action_scale={self.action_scale}, \"\n            f\"action_bias={self.action_bias}, \"\n            f\"optim={type(self.optim).__name__}, \"\n            f\"log_std={self.log_std}, \"\n            f\"lr={self.lr}, \"\n            f\"device={self.device})\"\n        )\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModule.__init__","title":"<code>__init__(state_dim, n_neurons, action_dim, action_scale, action_bias, *, log_std_min=-5, log_std_max=2, optim=optim.Adam, lr=0.0003, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>dimension of the state space</p> required <code>n_neurons</code> <code>int</code> <p>number of decision/hidden neurons</p> required <code>action_dim</code> <code>int</code> <p>dimension of the action space</p> required <code>action_scale</code> <code>torch.Tensor</code> <p>scale factor to map normalized actions to environment's action range</p> required <code>action_bias</code> <code>torch.Tensor</code> <p>bias/offset to center normalized actions to environment's action range</p> required <code>log_std_min</code> <code>float</code> <p>minimum log standard deviation</p> <code>-5</code> <code>log_std_max</code> <code>float</code> <p>maximum log standard deviation</p> <code>2</code> <code>optim</code> <code>Type[optim.Optimizer]</code> <p>a <code>PyTorch</code> optimizer class</p> <code>optim.Adam</code> <code>lr</code> <code>float</code> <p>optimizer learning rate</p> <code>0.0003</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def __init__(\n    self,\n    state_dim: int,\n    n_neurons: int,\n    action_dim: int,\n    action_scale: torch.Tensor,\n    action_bias: torch.Tensor,\n    *,\n    log_std_min: float = -5,\n    log_std_max: float = 2,\n    optim: Type[optim.Optimizer] = optim.Adam,\n    lr: float = 3e-4,\n    device: torch.device | None = None,\n):\n    \"\"\"\n    Parameters:\n        state_dim (int): dimension of the state space\n        n_neurons (int): number of decision/hidden neurons\n        action_dim (int): dimension of the action space\n        action_scale (torch.Tensor): scale factor to map normalized actions to\n            environment's action range\n        action_bias (torch.Tensor): bias/offset to center normalized actions to\n            environment's action range\n        log_std_min (float, optional): minimum log standard deviation\n        log_std_max (float, optional): maximum log standard deviation\n        optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n        lr (float, optional): optimizer learning rate\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    self.state_dim = state_dim\n    self.n_neurons = n_neurons\n    self.action_dim = action_dim\n    self.action_scale = action_scale\n    self.action_bias = action_bias\n    self.log_std = (log_std_min, log_std_max)\n    self.lr = lr\n    self.device = device\n\n    self.network = SACActor(\n        state_dim,\n        n_neurons,\n        action_dim,\n        action_scale,\n        action_bias,\n        log_std_min=log_std_min,\n        log_std_max=log_std_max,\n        device=device,\n    ).to(device)\n\n    self.hidden_size = self.network.ncp.hidden_size\n\n    self.optim = optim(self.network.parameters(), lr=lr)\n\n    self.config = self.network.config()\n\n    self.active_params = self.config.active_params\n    self.total_params = self.config.total_params\n\n    self.network: SACActor = torch.jit.script(self.network)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModule.eval_mode","title":"<code>eval_mode()</code>","text":"<p>Sets the network to evaluation mode.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def eval_mode(self) -&gt; None:\n    \"\"\"Sets the network to evaluation mode.\"\"\"\n    self.network.eval()\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModule.forward","title":"<code>forward(obs, hidden=None)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the current hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>the action predictions.</p> <code>log_prob</code> <code>torch.Tensor</code> <p>log probabilities of actions.</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def forward(\n    self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        hidden (torch.Tensor, optional): the current hidden state\n\n    Returns:\n        actions (torch.Tensor): the action predictions.\n        log_prob (torch.Tensor): log probabilities of actions.\n        hidden (torch.Tensor): the new hidden state.\n    \"\"\"\n    action, log_prob, hidden = self.network(obs, hidden)\n    return action, log_prob, hidden\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModule.gradient_step","title":"<code>gradient_step(loss)</code>","text":"<p>Performs a gradient update step.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>torch.Tensor</code> <p>network loss</p> required Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def gradient_step(self, loss: torch.Tensor) -&gt; None:\n    \"\"\"\n    Performs a gradient update step.\n\n    Parameters:\n        loss (torch.Tensor): network loss\n    \"\"\"\n    self.optim.zero_grad()\n    loss.backward()\n    self.optim.step()\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModule.predict","title":"<code>predict(obs, hidden=None)</code>","text":"<p>Makes a deterministic prediction using the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the current hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>the action predictions.</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def predict(\n    self,\n    obs: torch.Tensor,\n    hidden: torch.Tensor | None = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Makes a deterministic prediction using the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        hidden (torch.Tensor, optional): the current hidden state\n\n    Returns:\n        actions (torch.Tensor): the action predictions.\n        hidden (torch.Tensor): the new hidden state.\n    \"\"\"\n    action, hidden = self.network.predict(obs, hidden)\n    return action, hidden\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModule.train_mode","title":"<code>train_mode()</code>","text":"<p>Sets the network to training mode.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def train_mode(self) -&gt; None:\n    \"\"\"Sets the network to training mode.\"\"\"\n    self.network.train()\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModuleDiscrete","title":"<code>ActorModuleDiscrete</code>","text":"<p>               Bases: <code>BaseModule</code></p> <p>An Actor module for NeuroFlow. Uses a Liquid NCP SAC Actor with a Categorical policy.</p> <p>Usable with discrete action spaces.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>class ActorModuleDiscrete(BaseModule):\n    \"\"\"\n    An Actor module for NeuroFlow. Uses a Liquid NCP SAC Actor with a\n    Categorical policy.\n\n    Usable with discrete action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim: int,\n        n_neurons: int,\n        action_dim: int,\n        *,\n        optim: Type[optim.Optimizer] = optim.Adam,\n        lr: float = 3e-4,\n        device: torch.device | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            state_dim (int): dimension of the state space\n            n_neurons (int): number of decision/hidden neurons\n            action_dim (int): dimension of the action space\n            optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n            lr (float, optional): optimizer learning rate\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        self.state_dim = state_dim\n        self.n_neurons = n_neurons\n        self.action_dim = action_dim\n        self.lr = lr\n        self.device = device\n\n        self.network = SACActorDiscrete(\n            state_dim,\n            n_neurons,\n            action_dim,\n            device=device,\n        ).to(device)\n\n        self.hidden_size = self.network.ncp.hidden_size\n\n        self.optim = optim(self.network.parameters(), lr=lr)\n\n        self.config = self.network.config()\n\n        self.active_params = self.config.active_params\n        self.total_params = self.config.total_params\n\n        self.network: SACActorDiscrete = torch.jit.script(self.network)\n\n    def gradient_step(self, loss: torch.Tensor) -&gt; None:\n        \"\"\"\n        Performs a gradient update step.\n\n        Parameters:\n            loss (torch.Tensor): network loss\n        \"\"\"\n        self.optim.zero_grad()\n        loss.backward()\n        self.optim.step()\n\n    def predict(\n        self,\n        obs: torch.Tensor,\n        hidden: torch.Tensor | None = None,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Makes a deterministic prediction using the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            hidden (torch.Tensor, optional): the current hidden state\n\n        Returns:\n            actions (torch.Tensor): the action predictions.\n            hidden (torch.Tensor): the new hidden state.\n        \"\"\"\n        action, hidden = self.network.predict(obs, hidden)\n        return action, hidden\n\n    def forward(\n        self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            hidden (torch.Tensor, optional): the current hidden state\n\n        Returns:\n            actions (torch.Tensor): the action predictions.\n            probs (torch.Tensor): softmax probabilities for each action.\n            log_prob (torch.Tensor): log probabilities of actions.\n            hidden (torch.Tensor): the new hidden state.\n        \"\"\"\n        actions, probs, log_prob, hidden = self.network(obs, hidden)\n        return actions, probs, log_prob, hidden\n\n    def eval_mode(self) -&gt; None:\n        \"\"\"Sets the network to evaluation mode.\"\"\"\n        self.network.eval()\n\n    def train_mode(self) -&gt; None:\n        \"\"\"Sets the network to training mode.\"\"\"\n        self.network.train()\n\n    def state_dict(self) -&gt; Dict[str, Dict[str, Any]]:\n        return {\n            \"actor\": self.network.state_dict(),\n            \"actor_optim\": self.optim.state_dict(),\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Dict[str, Any]]) -&gt; None:\n        self.network.load_state_dict(state_dict[\"actor\"])\n        self.optim.load_state_dict(state_dict[\"actor_optim\"])\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"state_dim={self.state_dim}, \"\n            f\"n_neurons={self.n_neurons}, \"\n            f\"action_dim={self.action_dim}, \"\n            f\"optim={type(self.optim).__name__}, \"\n            f\"lr={self.lr}, \"\n            f\"device={self.device})\"\n        )\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModuleDiscrete.__init__","title":"<code>__init__(state_dim, n_neurons, action_dim, *, optim=optim.Adam, lr=0.0003, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>dimension of the state space</p> required <code>n_neurons</code> <code>int</code> <p>number of decision/hidden neurons</p> required <code>action_dim</code> <code>int</code> <p>dimension of the action space</p> required <code>optim</code> <code>Type[optim.Optimizer]</code> <p>a <code>PyTorch</code> optimizer class</p> <code>optim.Adam</code> <code>lr</code> <code>float</code> <p>optimizer learning rate</p> <code>0.0003</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def __init__(\n    self,\n    state_dim: int,\n    n_neurons: int,\n    action_dim: int,\n    *,\n    optim: Type[optim.Optimizer] = optim.Adam,\n    lr: float = 3e-4,\n    device: torch.device | None = None,\n):\n    \"\"\"\n    Parameters:\n        state_dim (int): dimension of the state space\n        n_neurons (int): number of decision/hidden neurons\n        action_dim (int): dimension of the action space\n        optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n        lr (float, optional): optimizer learning rate\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    self.state_dim = state_dim\n    self.n_neurons = n_neurons\n    self.action_dim = action_dim\n    self.lr = lr\n    self.device = device\n\n    self.network = SACActorDiscrete(\n        state_dim,\n        n_neurons,\n        action_dim,\n        device=device,\n    ).to(device)\n\n    self.hidden_size = self.network.ncp.hidden_size\n\n    self.optim = optim(self.network.parameters(), lr=lr)\n\n    self.config = self.network.config()\n\n    self.active_params = self.config.active_params\n    self.total_params = self.config.total_params\n\n    self.network: SACActorDiscrete = torch.jit.script(self.network)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModuleDiscrete.eval_mode","title":"<code>eval_mode()</code>","text":"<p>Sets the network to evaluation mode.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def eval_mode(self) -&gt; None:\n    \"\"\"Sets the network to evaluation mode.\"\"\"\n    self.network.eval()\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModuleDiscrete.forward","title":"<code>forward(obs, hidden=None)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the current hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>the action predictions.</p> <code>probs</code> <code>torch.Tensor</code> <p>softmax probabilities for each action.</p> <code>log_prob</code> <code>torch.Tensor</code> <p>log probabilities of actions.</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def forward(\n    self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        hidden (torch.Tensor, optional): the current hidden state\n\n    Returns:\n        actions (torch.Tensor): the action predictions.\n        probs (torch.Tensor): softmax probabilities for each action.\n        log_prob (torch.Tensor): log probabilities of actions.\n        hidden (torch.Tensor): the new hidden state.\n    \"\"\"\n    actions, probs, log_prob, hidden = self.network(obs, hidden)\n    return actions, probs, log_prob, hidden\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModuleDiscrete.gradient_step","title":"<code>gradient_step(loss)</code>","text":"<p>Performs a gradient update step.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>torch.Tensor</code> <p>network loss</p> required Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def gradient_step(self, loss: torch.Tensor) -&gt; None:\n    \"\"\"\n    Performs a gradient update step.\n\n    Parameters:\n        loss (torch.Tensor): network loss\n    \"\"\"\n    self.optim.zero_grad()\n    loss.backward()\n    self.optim.step()\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModuleDiscrete.predict","title":"<code>predict(obs, hidden=None)</code>","text":"<p>Makes a deterministic prediction using the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the current hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>the action predictions.</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def predict(\n    self,\n    obs: torch.Tensor,\n    hidden: torch.Tensor | None = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Makes a deterministic prediction using the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        hidden (torch.Tensor, optional): the current hidden state\n\n    Returns:\n        actions (torch.Tensor): the action predictions.\n        hidden (torch.Tensor): the new hidden state.\n    \"\"\"\n    action, hidden = self.network.predict(obs, hidden)\n    return action, hidden\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.ActorModuleDiscrete.train_mode","title":"<code>train_mode()</code>","text":"<p>Sets the network to training mode.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def train_mode(self) -&gt; None:\n    \"\"\"Sets the network to training mode.\"\"\"\n    self.network.train()\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.BaseModule","title":"<code>BaseModule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base module for all agent modules.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>class BaseModule(ABC):\n    \"\"\"\n    A base module for all agent modules.\n    \"\"\"\n\n    @abstractmethod\n    def state_dict(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Creates a state dictionary for the module.\n\n        Returns:\n            state_dict (Dict[str, Dict[str, Any]]): the state dicts for the module, including networks and optimizers.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def load_state_dict(self, state_dict: Dict[str, Dict[str, Any]]) -&gt; None:\n        \"\"\"\n        Load the modules state dict from a previously saved state.\n\n        Parameters:\n            state_dict (Dict[str, Dict[str, Any]]): a previously saved state dict\n        \"\"\"\n        pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.BaseModule.load_state_dict","title":"<code>load_state_dict(state_dict)</code>  <code>abstractmethod</code>","text":"<p>Load the modules state dict from a previously saved state.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Dict[str, Any]]</code> <p>a previously saved state dict</p> required Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>@abstractmethod\ndef load_state_dict(self, state_dict: Dict[str, Dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Load the modules state dict from a previously saved state.\n\n    Parameters:\n        state_dict (Dict[str, Dict[str, Any]]): a previously saved state dict\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.BaseModule.state_dict","title":"<code>state_dict()</code>  <code>abstractmethod</code>","text":"<p>Creates a state dictionary for the module.</p> <p>Returns:</p> Name Type Description <code>state_dict</code> <code>Dict[str, Dict[str, Any]]</code> <p>the state dicts for the module, including networks and optimizers.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>@abstractmethod\ndef state_dict(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Creates a state dictionary for the module.\n\n    Returns:\n        state_dict (Dict[str, Dict[str, Any]]): the state dicts for the module, including networks and optimizers.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModule","title":"<code>CriticModule</code>","text":"<p>               Bases: <code>BaseModule</code></p> <p>A Critic module for NeuroFlow. Uses a pair of NCP SAC Critic's with separate target networks to estimate Q-values.</p> <p>Usable with continuous action spaces.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>class CriticModule(BaseModule):\n    \"\"\"\n    A Critic module for NeuroFlow. Uses a pair of NCP SAC Critic's with separate\n    target networks to estimate Q-values.\n\n    Usable with continuous action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim: int,\n        n_neurons: int,\n        action_dim: int,\n        *,\n        optim: Type[optim.Optimizer] = optim.Adam,\n        lr: float = 3e-4,\n        tau: float = 0.005,\n        device: torch.device | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            state_dim (int): dimension of the state space\n            n_neurons (int): number of decision/hidden neurons\n            action_dim (int): dimension of the action space\n            optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n            lr (float, optional): optimizer learning rates\n            tau (float, optional): soft target network update factor\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        self.state_dim = state_dim\n        self.n_neurons = n_neurons\n        self.action_dim = action_dim\n        self.lr = lr\n        self.tau = tau\n        self.device = device\n\n        self.network1 = SACCriticNCP(\n            state_dim,\n            n_neurons,\n            action_dim,\n            device=device,\n        ).to(device)\n\n        self.network2 = SACCriticNCP(\n            state_dim,\n            n_neurons,\n            action_dim,\n            device=device,\n        ).to(device)\n\n        self.target1 = deepcopy(self.network1)\n        self.target2 = deepcopy(self.network2)\n\n        self.optim1 = optim(self.network1.parameters(), lr=lr)\n        self.optim2 = optim(self.network2.parameters(), lr=lr)\n\n        self.config = CriticConfig(\n            critic1=self.network1.config(),\n            critic2=self.network2.config(),\n        )\n\n        self.active_params = (\n            self.config.critic1.active_params + self.config.critic2.active_params\n        )\n        self.total_params = (\n            self.config.critic1.total_params + self.config.critic2.total_params\n        )\n\n        self.network1: SACCriticNCP = torch.jit.script(self.network1)\n        self.network2: SACCriticNCP = torch.jit.script(self.network2)\n\n        self.target1: SACCriticNCP = torch.jit.script(self.target1)\n        self.target2: SACCriticNCP = torch.jit.script(self.target2)\n\n    def update_targets(self) -&gt; None:\n        \"\"\"\n        Performs a soft update on the target networks.\n        \"\"\"\n        soft_update(self.network1, self.target1, tau=self.tau)\n        soft_update(self.network2, self.target2, tau=self.tau)\n\n    def gradient_step(self, c1_loss: torch.Tensor, c2_loss: torch.Tensor) -&gt; None:\n        \"\"\"\n        Performs a gradient update step.\n\n        Parameters:\n            c1_loss (torch.Tensor): critic loss for first network\n            c2_loss (torch.Tensor): critic loss for second network\n        \"\"\"\n        self.optim1.zero_grad()\n        c1_loss.backward()\n        self.optim1.step()\n\n        self.optim2.zero_grad()\n        c2_loss.backward()\n        self.optim2.step()\n\n    def predict(\n        self, obs: torch.Tensor, actions: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Makes a prediction using the critic networks.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            actions (torch.Tensor): the batch of actions\n\n        Returns:\n            q_values1 (torch.Tensor): the Q-Value predictions from the first network.\n            q_values2 (torch.Tensor): the Q-Value predictions from the second network.\n        \"\"\"\n        q_values1 = self.network1(obs, actions)\n        q_values2 = self.network2(obs, actions)\n\n        return q_values1, q_values2\n\n    def target_predict(self, obs: torch.Tensor, actions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Makes a prediction using the target networks.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            actions (torch.Tensor): the batch of actions\n\n        Returns:\n            next_q (torch.Tensor): the smallest Q-Value predictions between the target networks.\n        \"\"\"\n        q_values1 = self.target1(obs, actions)\n        q_values2 = self.target2(obs, actions)\n\n        return torch.min(q_values1, q_values2)\n\n    def state_dict(self) -&gt; Dict[str, Dict[str, Any]]:\n        return {\n            \"critic\": self.network1.state_dict(),\n            \"critic2\": self.network2.state_dict(),\n            \"critic_target\": self.target1.state_dict(),\n            \"critic2_target\": self.target2.state_dict(),\n            \"critic_optim\": self.optim1.state_dict(),\n            \"critic2_optim\": self.optim2.state_dict(),\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Dict[str, Any]]) -&gt; None:\n        self.network1.load_state_dict(state_dict[\"critic\"])\n        self.network2.load_state_dict(state_dict[\"critic2\"])\n\n        self.target1.load_state_dict(state_dict[\"critic_target\"])\n        self.target2.load_state_dict(state_dict[\"critic2_target\"])\n\n        self.optim1.load_state_dict(state_dict[\"critic_optim\"])\n        self.optim2.load_state_dict(state_dict[\"critic2_optim\"])\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"state_dim={self.state_dim}, \"\n            f\"n_neurons={self.n_neurons}, \"\n            f\"action_dim={self.action_dim}, \"\n            f\"optim={type(self.optim1).__name__}, \"\n            f\"lr={self.lr}, \"\n            f\"tau={self.tau}, \"\n            f\"device={self.device})\"\n        )\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModule.__init__","title":"<code>__init__(state_dim, n_neurons, action_dim, *, optim=optim.Adam, lr=0.0003, tau=0.005, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>dimension of the state space</p> required <code>n_neurons</code> <code>int</code> <p>number of decision/hidden neurons</p> required <code>action_dim</code> <code>int</code> <p>dimension of the action space</p> required <code>optim</code> <code>Type[optim.Optimizer]</code> <p>a <code>PyTorch</code> optimizer class</p> <code>optim.Adam</code> <code>lr</code> <code>float</code> <p>optimizer learning rates</p> <code>0.0003</code> <code>tau</code> <code>float</code> <p>soft target network update factor</p> <code>0.005</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def __init__(\n    self,\n    state_dim: int,\n    n_neurons: int,\n    action_dim: int,\n    *,\n    optim: Type[optim.Optimizer] = optim.Adam,\n    lr: float = 3e-4,\n    tau: float = 0.005,\n    device: torch.device | None = None,\n):\n    \"\"\"\n    Parameters:\n        state_dim (int): dimension of the state space\n        n_neurons (int): number of decision/hidden neurons\n        action_dim (int): dimension of the action space\n        optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n        lr (float, optional): optimizer learning rates\n        tau (float, optional): soft target network update factor\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    self.state_dim = state_dim\n    self.n_neurons = n_neurons\n    self.action_dim = action_dim\n    self.lr = lr\n    self.tau = tau\n    self.device = device\n\n    self.network1 = SACCriticNCP(\n        state_dim,\n        n_neurons,\n        action_dim,\n        device=device,\n    ).to(device)\n\n    self.network2 = SACCriticNCP(\n        state_dim,\n        n_neurons,\n        action_dim,\n        device=device,\n    ).to(device)\n\n    self.target1 = deepcopy(self.network1)\n    self.target2 = deepcopy(self.network2)\n\n    self.optim1 = optim(self.network1.parameters(), lr=lr)\n    self.optim2 = optim(self.network2.parameters(), lr=lr)\n\n    self.config = CriticConfig(\n        critic1=self.network1.config(),\n        critic2=self.network2.config(),\n    )\n\n    self.active_params = (\n        self.config.critic1.active_params + self.config.critic2.active_params\n    )\n    self.total_params = (\n        self.config.critic1.total_params + self.config.critic2.total_params\n    )\n\n    self.network1: SACCriticNCP = torch.jit.script(self.network1)\n    self.network2: SACCriticNCP = torch.jit.script(self.network2)\n\n    self.target1: SACCriticNCP = torch.jit.script(self.target1)\n    self.target2: SACCriticNCP = torch.jit.script(self.target2)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModule.gradient_step","title":"<code>gradient_step(c1_loss, c2_loss)</code>","text":"<p>Performs a gradient update step.</p> <p>Parameters:</p> Name Type Description Default <code>c1_loss</code> <code>torch.Tensor</code> <p>critic loss for first network</p> required <code>c2_loss</code> <code>torch.Tensor</code> <p>critic loss for second network</p> required Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def gradient_step(self, c1_loss: torch.Tensor, c2_loss: torch.Tensor) -&gt; None:\n    \"\"\"\n    Performs a gradient update step.\n\n    Parameters:\n        c1_loss (torch.Tensor): critic loss for first network\n        c2_loss (torch.Tensor): critic loss for second network\n    \"\"\"\n    self.optim1.zero_grad()\n    c1_loss.backward()\n    self.optim1.step()\n\n    self.optim2.zero_grad()\n    c2_loss.backward()\n    self.optim2.step()\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModule.predict","title":"<code>predict(obs, actions)</code>","text":"<p>Makes a prediction using the critic networks.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>actions</code> <code>torch.Tensor</code> <p>the batch of actions</p> required <p>Returns:</p> Name Type Description <code>q_values1</code> <code>torch.Tensor</code> <p>the Q-Value predictions from the first network.</p> <code>q_values2</code> <code>torch.Tensor</code> <p>the Q-Value predictions from the second network.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def predict(\n    self, obs: torch.Tensor, actions: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Makes a prediction using the critic networks.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        actions (torch.Tensor): the batch of actions\n\n    Returns:\n        q_values1 (torch.Tensor): the Q-Value predictions from the first network.\n        q_values2 (torch.Tensor): the Q-Value predictions from the second network.\n    \"\"\"\n    q_values1 = self.network1(obs, actions)\n    q_values2 = self.network2(obs, actions)\n\n    return q_values1, q_values2\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModule.target_predict","title":"<code>target_predict(obs, actions)</code>","text":"<p>Makes a prediction using the target networks.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>actions</code> <code>torch.Tensor</code> <p>the batch of actions</p> required <p>Returns:</p> Name Type Description <code>next_q</code> <code>torch.Tensor</code> <p>the smallest Q-Value predictions between the target networks.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def target_predict(self, obs: torch.Tensor, actions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Makes a prediction using the target networks.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        actions (torch.Tensor): the batch of actions\n\n    Returns:\n        next_q (torch.Tensor): the smallest Q-Value predictions between the target networks.\n    \"\"\"\n    q_values1 = self.target1(obs, actions)\n    q_values2 = self.target2(obs, actions)\n\n    return torch.min(q_values1, q_values2)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModule.update_targets","title":"<code>update_targets()</code>","text":"<p>Performs a soft update on the target networks.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def update_targets(self) -&gt; None:\n    \"\"\"\n    Performs a soft update on the target networks.\n    \"\"\"\n    soft_update(self.network1, self.target1, tau=self.tau)\n    soft_update(self.network2, self.target2, tau=self.tau)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModuleDiscrete","title":"<code>CriticModuleDiscrete</code>","text":"<p>               Bases: <code>BaseModule</code></p> <p>A Critic module for NeuroFlow. Uses a pair of NCP SAC Critic's with separate target networks to estimate Q-values.</p> <p>Usable with discrete action spaces.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>class CriticModuleDiscrete(BaseModule):\n    \"\"\"\n    A Critic module for NeuroFlow. Uses a pair of NCP SAC Critic's with separate\n    target networks to estimate Q-values.\n\n    Usable with discrete action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim: int,\n        n_neurons: int,\n        action_dim: int,\n        *,\n        optim: Type[optim.Optimizer] = optim.Adam,\n        lr: float = 3e-4,\n        tau: float = 0.005,\n        device: torch.device | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            state_dim (int): dimension of the state space\n            n_neurons (int): number of decision/hidden neurons\n            action_dim (int): dimension of the action space\n            optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n            lr (float, optional): optimizer learning rates\n            tau (float, optional): soft target network update factor\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        self.state_dim = state_dim\n        self.n_neurons = n_neurons\n        self.action_dim = action_dim\n        self.lr = lr\n        self.tau = tau\n        self.device = device\n\n        self.network1 = SACCriticNCPDiscrete(\n            state_dim,\n            n_neurons,\n            action_dim,\n            device=device,\n        ).to(device)\n\n        self.network2 = SACCriticNCPDiscrete(\n            state_dim,\n            n_neurons,\n            action_dim,\n            device=device,\n        ).to(device)\n\n        self.target1 = deepcopy(self.network1)\n        self.target2 = deepcopy(self.network2)\n\n        self.optim1 = optim(self.network1.parameters(), lr=lr)\n        self.optim2 = optim(self.network2.parameters(), lr=lr)\n\n        self.config = CriticConfig(\n            critic1=self.network1.config(),\n            critic2=self.network2.config(),\n        )\n\n        self.active_params = (\n            self.config.critic1.active_params + self.config.critic2.active_params\n        )\n        self.total_params = (\n            self.config.critic1.total_params + self.config.critic2.total_params\n        )\n\n        self.network1: SACCriticNCPDiscrete = torch.jit.script(self.network1)\n        self.network2: SACCriticNCPDiscrete = torch.jit.script(self.network2)\n\n        self.target1: SACCriticNCPDiscrete = torch.jit.script(self.target1)\n        self.target2: SACCriticNCPDiscrete = torch.jit.script(self.target2)\n\n    def update_targets(self) -&gt; None:\n        \"\"\"\n        Performs a soft update on the target networks.\n        \"\"\"\n        soft_update(self.network1, self.target1, tau=self.tau)\n        soft_update(self.network2, self.target2, tau=self.tau)\n\n    def gradient_step(self, c1_loss: torch.Tensor, c2_loss: torch.Tensor) -&gt; None:\n        \"\"\"\n        Performs a gradient update step.\n\n        Parameters:\n            c1_loss (torch.Tensor): critic loss for first network\n            c2_loss (torch.Tensor): critic loss for second network\n        \"\"\"\n        self.optim1.zero_grad()\n        c1_loss.backward()\n        self.optim1.step()\n\n        self.optim2.zero_grad()\n        c2_loss.backward()\n        self.optim2.step()\n\n    def predict(self, obs: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Makes a prediction using the critic networks.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n\n        Returns:\n            q_values1 (torch.Tensor): the Q-Value predictions from the first network.\n            q_values2 (torch.Tensor): the Q-Value predictions from the second network.\n        \"\"\"\n        q_values1 = self.network1(obs)\n        q_values2 = self.network2(obs)\n\n        return q_values1, q_values2\n\n    def target_predict(self, obs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Makes a prediction using the target networks.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n\n        Returns:\n            next_q (torch.Tensor): the smallest Q-Value predictions between the target networks.\n        \"\"\"\n        q_values1 = self.target1(obs)\n        q_values2 = self.target2(obs)\n\n        return torch.min(q_values1, q_values2)\n\n    def state_dict(self) -&gt; Dict[str, Dict[str, Any]]:\n        return {\n            \"critic\": self.network1.state_dict(),\n            \"critic2\": self.network2.state_dict(),\n            \"critic_target\": self.target1.state_dict(),\n            \"critic2_target\": self.target2.state_dict(),\n            \"critic_optim\": self.optim1.state_dict(),\n            \"critic2_optim\": self.optim2.state_dict(),\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Dict[str, Any]]) -&gt; None:\n        self.network1.load_state_dict(state_dict[\"critic\"])\n        self.network2.load_state_dict(state_dict[\"critic2\"])\n\n        self.target1.load_state_dict(state_dict[\"critic_target\"])\n        self.target2.load_state_dict(state_dict[\"critic2_target\"])\n\n        self.optim1.load_state_dict(state_dict[\"critic_optim\"])\n        self.optim2.load_state_dict(state_dict[\"critic2_optim\"])\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"state_dim={self.state_dim}, \"\n            f\"n_neurons={self.n_neurons}, \"\n            f\"action_dim={self.action_dim}, \"\n            f\"optim={type(self.optim1).__name__}, \"\n            f\"lr={self.lr}, \"\n            f\"tau={self.tau}, \"\n            f\"device={self.device})\"\n        )\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModuleDiscrete.__init__","title":"<code>__init__(state_dim, n_neurons, action_dim, *, optim=optim.Adam, lr=0.0003, tau=0.005, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>dimension of the state space</p> required <code>n_neurons</code> <code>int</code> <p>number of decision/hidden neurons</p> required <code>action_dim</code> <code>int</code> <p>dimension of the action space</p> required <code>optim</code> <code>Type[optim.Optimizer]</code> <p>a <code>PyTorch</code> optimizer class</p> <code>optim.Adam</code> <code>lr</code> <code>float</code> <p>optimizer learning rates</p> <code>0.0003</code> <code>tau</code> <code>float</code> <p>soft target network update factor</p> <code>0.005</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def __init__(\n    self,\n    state_dim: int,\n    n_neurons: int,\n    action_dim: int,\n    *,\n    optim: Type[optim.Optimizer] = optim.Adam,\n    lr: float = 3e-4,\n    tau: float = 0.005,\n    device: torch.device | None = None,\n):\n    \"\"\"\n    Parameters:\n        state_dim (int): dimension of the state space\n        n_neurons (int): number of decision/hidden neurons\n        action_dim (int): dimension of the action space\n        optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n        lr (float, optional): optimizer learning rates\n        tau (float, optional): soft target network update factor\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    self.state_dim = state_dim\n    self.n_neurons = n_neurons\n    self.action_dim = action_dim\n    self.lr = lr\n    self.tau = tau\n    self.device = device\n\n    self.network1 = SACCriticNCPDiscrete(\n        state_dim,\n        n_neurons,\n        action_dim,\n        device=device,\n    ).to(device)\n\n    self.network2 = SACCriticNCPDiscrete(\n        state_dim,\n        n_neurons,\n        action_dim,\n        device=device,\n    ).to(device)\n\n    self.target1 = deepcopy(self.network1)\n    self.target2 = deepcopy(self.network2)\n\n    self.optim1 = optim(self.network1.parameters(), lr=lr)\n    self.optim2 = optim(self.network2.parameters(), lr=lr)\n\n    self.config = CriticConfig(\n        critic1=self.network1.config(),\n        critic2=self.network2.config(),\n    )\n\n    self.active_params = (\n        self.config.critic1.active_params + self.config.critic2.active_params\n    )\n    self.total_params = (\n        self.config.critic1.total_params + self.config.critic2.total_params\n    )\n\n    self.network1: SACCriticNCPDiscrete = torch.jit.script(self.network1)\n    self.network2: SACCriticNCPDiscrete = torch.jit.script(self.network2)\n\n    self.target1: SACCriticNCPDiscrete = torch.jit.script(self.target1)\n    self.target2: SACCriticNCPDiscrete = torch.jit.script(self.target2)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModuleDiscrete.gradient_step","title":"<code>gradient_step(c1_loss, c2_loss)</code>","text":"<p>Performs a gradient update step.</p> <p>Parameters:</p> Name Type Description Default <code>c1_loss</code> <code>torch.Tensor</code> <p>critic loss for first network</p> required <code>c2_loss</code> <code>torch.Tensor</code> <p>critic loss for second network</p> required Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def gradient_step(self, c1_loss: torch.Tensor, c2_loss: torch.Tensor) -&gt; None:\n    \"\"\"\n    Performs a gradient update step.\n\n    Parameters:\n        c1_loss (torch.Tensor): critic loss for first network\n        c2_loss (torch.Tensor): critic loss for second network\n    \"\"\"\n    self.optim1.zero_grad()\n    c1_loss.backward()\n    self.optim1.step()\n\n    self.optim2.zero_grad()\n    c2_loss.backward()\n    self.optim2.step()\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModuleDiscrete.predict","title":"<code>predict(obs)</code>","text":"<p>Makes a prediction using the critic networks.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <p>Returns:</p> Name Type Description <code>q_values1</code> <code>torch.Tensor</code> <p>the Q-Value predictions from the first network.</p> <code>q_values2</code> <code>torch.Tensor</code> <p>the Q-Value predictions from the second network.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def predict(self, obs: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Makes a prediction using the critic networks.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n\n    Returns:\n        q_values1 (torch.Tensor): the Q-Value predictions from the first network.\n        q_values2 (torch.Tensor): the Q-Value predictions from the second network.\n    \"\"\"\n    q_values1 = self.network1(obs)\n    q_values2 = self.network2(obs)\n\n    return q_values1, q_values2\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModuleDiscrete.target_predict","title":"<code>target_predict(obs)</code>","text":"<p>Makes a prediction using the target networks.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <p>Returns:</p> Name Type Description <code>next_q</code> <code>torch.Tensor</code> <p>the smallest Q-Value predictions between the target networks.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def target_predict(self, obs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Makes a prediction using the target networks.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n\n    Returns:\n        next_q (torch.Tensor): the smallest Q-Value predictions between the target networks.\n    \"\"\"\n    q_values1 = self.target1(obs)\n    q_values2 = self.target2(obs)\n\n    return torch.min(q_values1, q_values2)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.CriticModuleDiscrete.update_targets","title":"<code>update_targets()</code>","text":"<p>Performs a soft update on the target networks.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def update_targets(self) -&gt; None:\n    \"\"\"\n    Performs a soft update on the target networks.\n    \"\"\"\n    soft_update(self.network1, self.target1, tau=self.tau)\n    soft_update(self.network2, self.target2, tau=self.tau)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModule","title":"<code>EntropyModule</code>","text":"<p>               Bases: <code>BaseModule</code></p> <p>An Entropy module for NeuroFlow. Uses automatic entropy tuning from SAC based on the paper: Soft Actor-Critic Algorithms and Applications.</p> <p>Usable with continuous action spaces.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>class EntropyModule(BaseModule):\n    \"\"\"\n    An Entropy module for NeuroFlow. Uses automatic entropy tuning from SAC\n    based on the paper: [Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905).\n\n    Usable with continuous action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        action_dim: int,\n        *,\n        initial_alpha: float = 1.0,\n        optim: Type[optim.Optimizer] = optim.Adam,\n        lr: float = 3e-4,\n        device: torch.device | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            action_dim (int): dimension of the action space\n            initial_alpha (float, optional): the starting entropy coefficient value\n            optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n            lr (float, optional): optimizer learning rates\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        self.action_dim = action_dim\n        self.initial_alpha = initial_alpha\n        self.lr = lr\n        self.device = device\n\n        self.target = -action_dim\n        self.log_alpha = nn.Parameter(torch.tensor(initial_alpha, device=device).log())\n\n        self.optim = optim([self.log_alpha], lr=lr)\n\n    @property\n    def alpha(self) -&gt; torch.Tensor:\n        \"\"\"\n        Get the current entropy coefficient (alpha).\n\n        Returns:\n            alpha (torch.Tensor): the entropy coefficient.\n        \"\"\"\n        return self.log_alpha.exp()\n\n    def compute_loss(self, log_probs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the entropy coefficient loss.\n\n        Parameters:\n            log_probs (torch.Tensor): log probabilities for actions\n\n        Returns:\n            loss (torch.Tensor): the entropy loss value.\n        \"\"\"\n        loss = torch.tensor(0.0, device=log_probs.device)\n\n        entropy = (log_probs + self.target).detach()\n        loss = -(self.log_alpha * entropy).mean()\n\n        return loss\n\n    def gradient_step(self, loss: torch.Tensor) -&gt; None:\n        \"\"\"\n        Performs a gradient update step.\n\n        Parameters:\n            loss (torch.Tensor): loss to backpropagate\n        \"\"\"\n        self.optim.zero_grad()\n        loss.backward()\n        self.optim.step()\n\n    def config(self) -&gt; EntropyParameters:\n        \"\"\"\n        Creates a module config.\n\n        Returns:\n            config (EntropyParameters): a parameter config model.\n        \"\"\"\n        return EntropyParameters(\n            lr=self.lr,\n            initial_alpha=self.initial_alpha,\n            target=self.target,\n        )\n\n    def state_dict(self) -&gt; Dict[str, Dict[str, Any]]:\n        return {\n            \"entropy_optim\": self.optim.state_dict(),\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Dict[str, Any]]) -&gt; None:\n        self.optim.load_state_dict(state_dict[\"entropy_optim\"])\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"action_dim={self.action_dim}, \"\n            f\"initial_alpha={self.initial_alpha}, \"\n            f\"optim={type(self.optim).__name__}, \"\n            f\"lr={self.lr}, \"\n            f\"device={self.device})\"\n        )\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModule.alpha","title":"<code>alpha</code>  <code>property</code>","text":"<p>Get the current entropy coefficient (alpha).</p> <p>Returns:</p> Name Type Description <code>alpha</code> <code>torch.Tensor</code> <p>the entropy coefficient.</p>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModule.__init__","title":"<code>__init__(action_dim, *, initial_alpha=1.0, optim=optim.Adam, lr=0.0003, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>action_dim</code> <code>int</code> <p>dimension of the action space</p> required <code>initial_alpha</code> <code>float</code> <p>the starting entropy coefficient value</p> <code>1.0</code> <code>optim</code> <code>Type[optim.Optimizer]</code> <p>a <code>PyTorch</code> optimizer class</p> <code>optim.Adam</code> <code>lr</code> <code>float</code> <p>optimizer learning rates</p> <code>0.0003</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def __init__(\n    self,\n    action_dim: int,\n    *,\n    initial_alpha: float = 1.0,\n    optim: Type[optim.Optimizer] = optim.Adam,\n    lr: float = 3e-4,\n    device: torch.device | None = None,\n):\n    \"\"\"\n    Parameters:\n        action_dim (int): dimension of the action space\n        initial_alpha (float, optional): the starting entropy coefficient value\n        optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n        lr (float, optional): optimizer learning rates\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    self.action_dim = action_dim\n    self.initial_alpha = initial_alpha\n    self.lr = lr\n    self.device = device\n\n    self.target = -action_dim\n    self.log_alpha = nn.Parameter(torch.tensor(initial_alpha, device=device).log())\n\n    self.optim = optim([self.log_alpha], lr=lr)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModule.compute_loss","title":"<code>compute_loss(log_probs)</code>","text":"<p>Computes the entropy coefficient loss.</p> <p>Parameters:</p> Name Type Description Default <code>log_probs</code> <code>torch.Tensor</code> <p>log probabilities for actions</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>torch.Tensor</code> <p>the entropy loss value.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def compute_loss(self, log_probs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the entropy coefficient loss.\n\n    Parameters:\n        log_probs (torch.Tensor): log probabilities for actions\n\n    Returns:\n        loss (torch.Tensor): the entropy loss value.\n    \"\"\"\n    loss = torch.tensor(0.0, device=log_probs.device)\n\n    entropy = (log_probs + self.target).detach()\n    loss = -(self.log_alpha * entropy).mean()\n\n    return loss\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModule.config","title":"<code>config()</code>","text":"<p>Creates a module config.</p> <p>Returns:</p> Name Type Description <code>config</code> <code>EntropyParameters</code> <p>a parameter config model.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def config(self) -&gt; EntropyParameters:\n    \"\"\"\n    Creates a module config.\n\n    Returns:\n        config (EntropyParameters): a parameter config model.\n    \"\"\"\n    return EntropyParameters(\n        lr=self.lr,\n        initial_alpha=self.initial_alpha,\n        target=self.target,\n    )\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModule.gradient_step","title":"<code>gradient_step(loss)</code>","text":"<p>Performs a gradient update step.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>torch.Tensor</code> <p>loss to backpropagate</p> required Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def gradient_step(self, loss: torch.Tensor) -&gt; None:\n    \"\"\"\n    Performs a gradient update step.\n\n    Parameters:\n        loss (torch.Tensor): loss to backpropagate\n    \"\"\"\n    self.optim.zero_grad()\n    loss.backward()\n    self.optim.step()\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModuleDiscrete","title":"<code>EntropyModuleDiscrete</code>","text":"<p>               Bases: <code>BaseModule</code></p> <p>An Entropy module for NeuroFlow. Uses automatic entropy tuning from SAC based on the paper: Soft Actor-Critic for Discrete Action Settings.</p> <p>Usable with discrete action spaces.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>class EntropyModuleDiscrete(BaseModule):\n    \"\"\"\n    An Entropy module for NeuroFlow. Uses automatic entropy tuning from SAC\n    based on the paper: [Soft Actor-Critic for Discrete Action Settings](https://arxiv.org/abs/1910.07207).\n\n    Usable with discrete action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        action_dim: int,\n        *,\n        initial_alpha: float = 1.0,\n        optim: Type[optim.Optimizer] = optim.Adam,\n        lr: float = 3e-4,\n        device: torch.device | None = None,\n    ):\n        \"\"\"\n        Parameters:\n            action_dim (int): dimension of the action space\n            initial_alpha (float, optional): the starting entropy coefficient value\n            optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n            lr (float, optional): optimizer learning rates\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        self.action_dim = action_dim\n        self.initial_alpha = initial_alpha\n        self.lr = lr\n        self.device = device\n\n        self.target = 0.98 * torch.tensor(1 / action_dim, device=device).log()\n        self.log_alpha = nn.Parameter(torch.tensor(initial_alpha, device=device).log())\n\n        self.optim = optim([self.log_alpha], lr=lr)\n\n    @property\n    def alpha(self) -&gt; torch.Tensor:\n        \"\"\"\n        Get the current entropy coefficient (alpha).\n\n        Returns:\n            alpha (torch.Tensor): the entropy coefficient.\n        \"\"\"\n        return self.log_alpha.exp()\n\n    def compute_loss(\n        self, probs: torch.Tensor, log_probs: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the entropy coefficient loss.\n\n        Parameters:\n            probs (torch.Tensor): probabilities for actions\n            log_probs (torch.Tensor): log probabilities for actions\n\n        Returns:\n            loss (torch.Tensor): the entropy loss value.\n        \"\"\"\n        with torch.no_grad():\n            entropy_mean = -torch.sum(probs * log_probs, dim=1).mean()\n\n        loss = self.log_alpha * (entropy_mean - self.target)\n        return loss\n\n    def gradient_step(self, loss: torch.Tensor) -&gt; None:\n        \"\"\"\n        Performs a gradient update step.\n\n        Parameters:\n            loss (torch.Tensor): loss to backpropagate\n        \"\"\"\n        self.optim.zero_grad()\n        loss.backward()\n        self.optim.step()\n\n    def config(self) -&gt; EntropyParameters:\n        \"\"\"\n        Creates a module config.\n\n        Returns:\n            config (EntropyParameters): a parameter config model.\n        \"\"\"\n        return EntropyParameters(\n            lr=self.lr,\n            initial_alpha=self.initial_alpha,\n            target=self.target,\n        )\n\n    def state_dict(self) -&gt; Dict[str, Dict[str, Any]]:\n        return {\n            \"entropy_optim\": self.optim.state_dict(),\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Dict[str, Any]]) -&gt; None:\n        self.optim.load_state_dict(state_dict[\"entropy_optim\"])\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"action_dim={self.action_dim}, \"\n            f\"initial_alpha={self.initial_alpha}, \"\n            f\"optim={type(self.optim).__name__}, \"\n            f\"lr={self.lr}, \"\n            f\"device={self.device})\"\n        )\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModuleDiscrete.alpha","title":"<code>alpha</code>  <code>property</code>","text":"<p>Get the current entropy coefficient (alpha).</p> <p>Returns:</p> Name Type Description <code>alpha</code> <code>torch.Tensor</code> <p>the entropy coefficient.</p>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModuleDiscrete.__init__","title":"<code>__init__(action_dim, *, initial_alpha=1.0, optim=optim.Adam, lr=0.0003, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>action_dim</code> <code>int</code> <p>dimension of the action space</p> required <code>initial_alpha</code> <code>float</code> <p>the starting entropy coefficient value</p> <code>1.0</code> <code>optim</code> <code>Type[optim.Optimizer]</code> <p>a <code>PyTorch</code> optimizer class</p> <code>optim.Adam</code> <code>lr</code> <code>float</code> <p>optimizer learning rates</p> <code>0.0003</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def __init__(\n    self,\n    action_dim: int,\n    *,\n    initial_alpha: float = 1.0,\n    optim: Type[optim.Optimizer] = optim.Adam,\n    lr: float = 3e-4,\n    device: torch.device | None = None,\n):\n    \"\"\"\n    Parameters:\n        action_dim (int): dimension of the action space\n        initial_alpha (float, optional): the starting entropy coefficient value\n        optim (Type[optim.Optimizer], optional): a `PyTorch` optimizer class\n        lr (float, optional): optimizer learning rates\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    self.action_dim = action_dim\n    self.initial_alpha = initial_alpha\n    self.lr = lr\n    self.device = device\n\n    self.target = 0.98 * torch.tensor(1 / action_dim, device=device).log()\n    self.log_alpha = nn.Parameter(torch.tensor(initial_alpha, device=device).log())\n\n    self.optim = optim([self.log_alpha], lr=lr)\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModuleDiscrete.compute_loss","title":"<code>compute_loss(probs, log_probs)</code>","text":"<p>Computes the entropy coefficient loss.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>torch.Tensor</code> <p>probabilities for actions</p> required <code>log_probs</code> <code>torch.Tensor</code> <p>log probabilities for actions</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>torch.Tensor</code> <p>the entropy loss value.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def compute_loss(\n    self, probs: torch.Tensor, log_probs: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the entropy coefficient loss.\n\n    Parameters:\n        probs (torch.Tensor): probabilities for actions\n        log_probs (torch.Tensor): log probabilities for actions\n\n    Returns:\n        loss (torch.Tensor): the entropy loss value.\n    \"\"\"\n    with torch.no_grad():\n        entropy_mean = -torch.sum(probs * log_probs, dim=1).mean()\n\n    loss = self.log_alpha * (entropy_mean - self.target)\n    return loss\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModuleDiscrete.config","title":"<code>config()</code>","text":"<p>Creates a module config.</p> <p>Returns:</p> Name Type Description <code>config</code> <code>EntropyParameters</code> <p>a parameter config model.</p> Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def config(self) -&gt; EntropyParameters:\n    \"\"\"\n    Creates a module config.\n\n    Returns:\n        config (EntropyParameters): a parameter config model.\n    \"\"\"\n    return EntropyParameters(\n        lr=self.lr,\n        initial_alpha=self.initial_alpha,\n        target=self.target,\n    )\n</code></pre>"},{"location":"learn/reference/models/modules/#velora.models.nf.modules.EntropyModuleDiscrete.gradient_step","title":"<code>gradient_step(loss)</code>","text":"<p>Performs a gradient update step.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>torch.Tensor</code> <p>loss to backpropagate</p> required Source code in <code>velora/models/nf/modules.py</code> Python<pre><code>def gradient_step(self, loss: torch.Tensor) -&gt; None:\n    \"\"\"\n    Performs a gradient update step.\n\n    Parameters:\n        loss (torch.Tensor): loss to backpropagate\n    \"\"\"\n    self.optim.zero_grad()\n    loss.backward()\n    self.optim.step()\n</code></pre>"},{"location":"learn/reference/models/nf/","title":"velora.models.nf","text":"<p>Velora's dedicated RL agents.</p>"},{"location":"learn/reference/models/nf/#velora.models.nf.NeuroFlow","title":"<code>NeuroFlow</code>","text":"<p>               Bases: <code>RLModuleAgent</code></p> Documentation <p>User Guide - Tutorials: NeuroFlow - Discrete</p> <p>A custom Liquid RL agent that combines a variety of RL techniques.</p> <p>Designed for <code>discrete</code> action spaces.</p> Source code in <code>velora/models/nf/agent.py</code> Python<pre><code>class NeuroFlow(RLModuleAgent):\n    \"\"\"\n    ???+ abstract \"Documentation\"\n\n        &gt; [User Guide - Tutorials: NeuroFlow - Discrete](https://velora.achronus.dev/learn/tutorial/agents/nf2)\n\n    A custom Liquid RL agent that combines a variety of RL techniques.\n\n    Designed for `discrete` action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        env_id: Union[str, \"DiscreteGymNames\"],\n        actor_neurons: int,\n        critic_neurons: int,\n        *,\n        optim: Type[optim.Optimizer] = optim.Adam,\n        buffer_size: int = 1_000_000,\n        actor_lr: float = 3e-4,\n        critic_lr: float = 3e-4,\n        alpha_lr: float = 3e-4,\n        initial_alpha: float = 1.0,\n        tau: float = 0.005,\n        gamma: float = 0.99,\n        device: torch.device | None = None,\n        seed: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            env_id (str): the Gymnasium environment ID to train the model on\n            actor_neurons (int): number of decision nodes (inter and command nodes)\n                for the actor\n            critic_neurons (int): number of decision nodes (inter and command nodes)\n                for the critic\n            optim (Type[torch.optim.Optimizer], optional): the type of `PyTorch`\n                optimizer to use\n            buffer_size (int, optional): the maximum size of the `ReplayBuffer`\n            actor_lr (float, optional): the actor optimizer learning rate\n            critic_lr (float, optional): the critic optimizer learning rate\n            alpha_lr (float, optional): the entropy parameter learning rate\n            initial_alpha (float, optional): the starting entropy coefficient value\n            tau (float, optional): the soft update factor used to slowly update\n                the target networks\n            gamma (float, optional): the reward discount factor\n            device (torch.device, optional): the device to perform computations on\n            seed (int, optional): random number seed for experiment\n                reproducibility. When `None` generates a seed automatically\n        \"\"\"\n        env = gym.make(env_id, render_mode=\"rgb_array\")\n\n        if not isinstance(env.action_space, gym.spaces.Discrete):\n            raise EnvironmentError(\n                f\"Invalid '{env.action_space=}'. Must be 'gym.spaces.Discrete'.\"\n            )\n\n        super().__init__(\n            env,\n            actor_neurons,\n            critic_neurons,\n            buffer_size,\n            optim,\n            device,\n            seed,\n        )\n\n        self.initial_alpha = initial_alpha\n        self.gamma = gamma\n        self.tau = tau\n\n        self.actor: ActorModuleDiscrete = ActorModuleDiscrete(\n            self.state_dim,\n            self.actor_neurons,\n            self.action_dim,\n            optim=optim,\n            lr=actor_lr,\n            device=self.device,\n        )\n\n        self.critic: CriticModuleDiscrete = CriticModuleDiscrete(\n            self.state_dim,\n            self.critic_neurons,\n            self.action_dim,\n            optim=optim,\n            lr=critic_lr,\n            tau=tau,\n            device=self.device,\n        )\n\n        self.hidden_dim = self.actor.hidden_size\n\n        self.entropy: EntropyModuleDiscrete = EntropyModuleDiscrete(\n            self.action_dim,\n            initial_alpha=initial_alpha,\n            optim=optim,\n            lr=alpha_lr,\n            device=device,\n        )\n\n        self.loss = nn.MSELoss()\n        self.buffer: ReplayBuffer = ReplayBuffer(\n            buffer_size,\n            self.state_dim,\n            1,\n            self.actor.hidden_size,\n            device=device,\n        )\n\n        self.active_params = self.actor.active_params + self.critic.active_params\n        self.total_params = self.actor.total_params + self.critic.total_params\n\n        # Init config details\n        self.config = RLAgentConfig(\n            agent=self.__class__.__name__,\n            env=env_id,\n            seed=self.seed,\n            model_details=ModelDetails(\n                **locals(),\n                state_dim=self.state_dim,\n                action_dim=self.action_dim,\n                action_type=\"discrete\",\n                exploration_type=\"Entropy\",\n                actor=self.actor.config,\n                critic=self.critic.config,\n                entropy=self.entropy.config(),\n            ),\n            buffer=self.buffer.config(),\n            torch=TorchConfig(\n                device=str(self.device),\n                optimizer=optim.__name__,\n                loss=self.loss.__class__.__name__,\n            ),\n        )\n\n        self.metadata = self.set_metadata(locals(), self.seed)\n\n    def _update_critics(self, batch: \"BatchExperience\") -&gt; torch.Tensor:\n        \"\"\"\n        Helper method. Performs Critic network updates.\n\n        Parameters:\n            batch (BatchExperience): an object containing a batch of experience\n                with `(states, actions, rewards, next_states, dones, hidden)`\n                from the buffer\n\n        Returns:\n            critic_loss (torch.Tensor): total Critic network loss `(c1_loss + c2_loss)`.\n        \"\"\"\n        with torch.no_grad():\n            _, next_probs, _, _ = self.actor.forward(batch.next_states, batch.hiddens)\n            next_log_probs = torch.log(next_probs + 1e-8)\n\n            # Compute target Q-value (all actions)\n            min_next_q = self.critic.target_predict(batch.next_states)  # [b, a_dim]\n\n            next_q = next_probs * (min_next_q - self.entropy.alpha * next_log_probs)\n            next_q = torch.sum(next_q, dim=-1, keepdim=True)\n            target_q = batch.rewards + (1 - batch.dones) * self.gamma * next_q  # [b, 1]\n\n        # Compute Q-value predictions for current critics (all actions)\n        current_q1, current_q2 = self.critic.predict(batch.states)  # [b, a_dim]\n\n        # Select Q-values for the actions taken in the batch - shape: (b, 1)\n        current_q1 = current_q1.gather(1, batch.actions.long())\n        current_q2 = current_q2.gather(1, batch.actions.long())\n\n        # Calculate loss\n        c1_loss: torch.Tensor = self.loss(current_q1, target_q)\n        c2_loss: torch.Tensor = self.loss(current_q2, target_q)\n        critic_loss: torch.Tensor = c1_loss + c2_loss\n\n        # Update critics\n        self.critic.gradient_step(c1_loss, c2_loss)\n\n        return critic_loss\n\n    def _train_step(self, batch_size: int) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Helper method. Performs a single training step.\n\n        Parameters:\n            batch_size (int): number of samples in a batch\n\n        Returns:\n            losses (Dict[str, torch.Tensor]): a dictionary of losses -\n\n            - critic: the total critic loss.\n            - actor: the actor loss.\n            - entropy: the entropy loss.\n        \"\"\"\n        batch = self.buffer.sample(batch_size)\n\n        # Compute critic loss\n        critic_loss = self._update_critics(batch)\n\n        # Make predictions\n        _, probs, log_probs, _ = self.actor.forward(batch.states, batch.hiddens)\n        q1, q2 = self.critic.predict(batch.states)\n\n        # Compute actor and entropy losses\n        next_q = torch.min(q1, q2)  # [b, a_dim]\n        actor_loss = probs * (self.entropy.alpha * log_probs - next_q)\n        actor_loss = torch.sum(actor_loss, dim=-1, keepdim=False).mean()\n\n        entropy_loss = self.entropy.compute_loss(\n            probs,\n            torch.log(probs + 1e-8),\n        )\n\n        # Update gradients\n        self.actor.gradient_step(actor_loss)\n        self.entropy.gradient_step(entropy_loss)\n\n        # Update target networks\n        self.critic.update_targets()\n\n        return {\n            \"critic\": critic_loss.detach(),\n            \"actor\": actor_loss.detach(),\n            \"entropy\": entropy_loss.detach(),\n        }\n\n    @override\n    def train(\n        self,\n        batch_size: int,\n        *,\n        n_episodes: int = 10_000,\n        callbacks: List[\"TrainCallback\"] | None = None,\n        log_freq: int = 10,\n        display_count: int = 100,\n        window_size: int = 100,\n        max_steps: int = 1000,\n        warmup_steps: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Trains the agent on a Gymnasium environment using a `ReplayBuffer`.\n\n        Parameters:\n            batch_size (int): the number of features in a single batch\n            n_episodes (int, optional): the total number of episodes to train for\n            callbacks (List[TrainCallback], optional): a list of training callbacks\n                that are applied during the training process\n            log_freq (int, optional): metric logging frequency (in episodes)\n            display_count (int, optional): console training progress frequency\n                (in episodes)\n            window_size (int, optional): the reward moving average size\n                (in episodes)\n            max_steps (int, optional): the total number of steps per episode\n            warmup_steps (int, optional): the number of samples to generate in the\n                buffer before starting training. If `None` uses `batch_size * 2`\n        \"\"\"\n        warmup_steps = batch_size * 2 if warmup_steps is None else warmup_steps\n\n        # Add training details to config\n        self.config = self.config.update(self._set_train_params(locals()))\n\n        # Display console details\n        self.env.reset(seed=self.seed)  # Set seed\n        training_info(\n            self,\n            n_episodes,\n            batch_size,\n            window_size,\n            warmup_steps,\n            callbacks or [],\n        )\n\n        if warmup_steps &gt; 0:\n            self.buffer.warm(self, warmup_steps, 2 if warmup_steps &lt; 8 else 8)\n\n        with TrainHandler(\n            self, n_episodes, max_steps, log_freq, window_size, callbacks\n        ) as handler:\n            for current_ep in range(1, n_episodes + 1):\n                ep_reward = 0.0\n                hidden = None\n\n                state, _ = handler.env.reset()\n\n                for current_step in range(1, max_steps + 1):\n                    action, hidden = self.predict(state, hidden, train_mode=True)\n                    next_state, reward, terminated, truncated, info = handler.env.step(\n                        action\n                    )\n                    done = terminated or truncated\n\n                    self.buffer.add(state, action, reward, next_state, done, hidden)\n\n                    losses = self._train_step(batch_size)\n\n                    handler.metrics.add_step(**losses)\n                    handler.step(current_step)\n\n                    state = next_state\n\n                    if done:\n                        ep_reward = info[\"episode\"][\"r\"].item()\n\n                        handler.metrics.add_episode(\n                            current_ep,\n                            info[\"episode\"][\"r\"],\n                            info[\"episode\"][\"l\"],\n                        )\n                        break\n\n                if current_ep % log_freq == 0 or current_ep == n_episodes:\n                    handler.log(current_ep, \"episode\")\n\n                if (\n                    current_ep % display_count == 0\n                    or current_ep == n_episodes\n                    or handler.stop()\n                ):\n                    handler.metrics.info(current_ep)\n\n                handler.episode(current_ep, ep_reward)\n\n                # Terminate on early stopping\n                if handler.stop():\n                    break\n\n    @override\n    def predict(\n        self,\n        state: torch.Tensor,\n        hidden: torch.Tensor | None = None,\n        *,\n        train_mode: bool = False,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Makes an action prediction using the Actor network.\n\n        Parameters:\n            state (torch.Tensor): the current state\n            hidden (torch.Tensor, optional): the current hidden state\n            train_mode (bool, optional): whether to make deterministic (when\n                `False`) or stochastic (when `True`) action predictions\n\n        Returns:\n            action (torch.Tensor): the action prediction on the given state\n            hidden (torch.Tensor): the Actor network's new hidden state\n        \"\"\"\n        self.actor.eval_mode()\n        with torch.no_grad():\n            state = state.unsqueeze(0) if state.dim() &lt; 2 else state\n\n            if not train_mode:\n                action, hidden = self.actor.predict(state, hidden)\n            else:\n                action, _, _, hidden = self.actor.forward(state, hidden)\n\n        self.actor.train_mode()\n        return action, hidden\n\n    def save(\n        self,\n        dirpath: str | Path,\n        *,\n        buffer: bool = False,\n        config: bool = False,\n    ) -&gt; None:\n        save_model(self, dirpath, buffer=buffer, config=config)\n\n    @classmethod\n    def load(cls, dirpath: str | Path, *, buffer: bool = False) -&gt; Self:\n        return load_model(cls, dirpath, buffer=buffer)\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            + \", \".join([f\"{key}={val}\" for key, val in self.metadata.items()])\n            + \")\"\n        )\n</code></pre>"},{"location":"learn/reference/models/nf/#velora.models.nf.NeuroFlow.__init__","title":"<code>__init__(env_id, actor_neurons, critic_neurons, *, optim=optim.Adam, buffer_size=1000000, actor_lr=0.0003, critic_lr=0.0003, alpha_lr=0.0003, initial_alpha=1.0, tau=0.005, gamma=0.99, device=None, seed=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>env_id</code> <code>str</code> <p>the Gymnasium environment ID to train the model on</p> required <code>actor_neurons</code> <code>int</code> <p>number of decision nodes (inter and command nodes) for the actor</p> required <code>critic_neurons</code> <code>int</code> <p>number of decision nodes (inter and command nodes) for the critic</p> required <code>optim</code> <code>Type[torch.optim.Optimizer]</code> <p>the type of <code>PyTorch</code> optimizer to use</p> <code>optim.Adam</code> <code>buffer_size</code> <code>int</code> <p>the maximum size of the <code>ReplayBuffer</code></p> <code>1000000</code> <code>actor_lr</code> <code>float</code> <p>the actor optimizer learning rate</p> <code>0.0003</code> <code>critic_lr</code> <code>float</code> <p>the critic optimizer learning rate</p> <code>0.0003</code> <code>alpha_lr</code> <code>float</code> <p>the entropy parameter learning rate</p> <code>0.0003</code> <code>initial_alpha</code> <code>float</code> <p>the starting entropy coefficient value</p> <code>1.0</code> <code>tau</code> <code>float</code> <p>the soft update factor used to slowly update the target networks</p> <code>0.005</code> <code>gamma</code> <code>float</code> <p>the reward discount factor</p> <code>0.99</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> <code>seed</code> <code>int</code> <p>random number seed for experiment reproducibility. When <code>None</code> generates a seed automatically</p> <code>None</code> Source code in <code>velora/models/nf/agent.py</code> Python<pre><code>def __init__(\n    self,\n    env_id: Union[str, \"DiscreteGymNames\"],\n    actor_neurons: int,\n    critic_neurons: int,\n    *,\n    optim: Type[optim.Optimizer] = optim.Adam,\n    buffer_size: int = 1_000_000,\n    actor_lr: float = 3e-4,\n    critic_lr: float = 3e-4,\n    alpha_lr: float = 3e-4,\n    initial_alpha: float = 1.0,\n    tau: float = 0.005,\n    gamma: float = 0.99,\n    device: torch.device | None = None,\n    seed: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        env_id (str): the Gymnasium environment ID to train the model on\n        actor_neurons (int): number of decision nodes (inter and command nodes)\n            for the actor\n        critic_neurons (int): number of decision nodes (inter and command nodes)\n            for the critic\n        optim (Type[torch.optim.Optimizer], optional): the type of `PyTorch`\n            optimizer to use\n        buffer_size (int, optional): the maximum size of the `ReplayBuffer`\n        actor_lr (float, optional): the actor optimizer learning rate\n        critic_lr (float, optional): the critic optimizer learning rate\n        alpha_lr (float, optional): the entropy parameter learning rate\n        initial_alpha (float, optional): the starting entropy coefficient value\n        tau (float, optional): the soft update factor used to slowly update\n            the target networks\n        gamma (float, optional): the reward discount factor\n        device (torch.device, optional): the device to perform computations on\n        seed (int, optional): random number seed for experiment\n            reproducibility. When `None` generates a seed automatically\n    \"\"\"\n    env = gym.make(env_id, render_mode=\"rgb_array\")\n\n    if not isinstance(env.action_space, gym.spaces.Discrete):\n        raise EnvironmentError(\n            f\"Invalid '{env.action_space=}'. Must be 'gym.spaces.Discrete'.\"\n        )\n\n    super().__init__(\n        env,\n        actor_neurons,\n        critic_neurons,\n        buffer_size,\n        optim,\n        device,\n        seed,\n    )\n\n    self.initial_alpha = initial_alpha\n    self.gamma = gamma\n    self.tau = tau\n\n    self.actor: ActorModuleDiscrete = ActorModuleDiscrete(\n        self.state_dim,\n        self.actor_neurons,\n        self.action_dim,\n        optim=optim,\n        lr=actor_lr,\n        device=self.device,\n    )\n\n    self.critic: CriticModuleDiscrete = CriticModuleDiscrete(\n        self.state_dim,\n        self.critic_neurons,\n        self.action_dim,\n        optim=optim,\n        lr=critic_lr,\n        tau=tau,\n        device=self.device,\n    )\n\n    self.hidden_dim = self.actor.hidden_size\n\n    self.entropy: EntropyModuleDiscrete = EntropyModuleDiscrete(\n        self.action_dim,\n        initial_alpha=initial_alpha,\n        optim=optim,\n        lr=alpha_lr,\n        device=device,\n    )\n\n    self.loss = nn.MSELoss()\n    self.buffer: ReplayBuffer = ReplayBuffer(\n        buffer_size,\n        self.state_dim,\n        1,\n        self.actor.hidden_size,\n        device=device,\n    )\n\n    self.active_params = self.actor.active_params + self.critic.active_params\n    self.total_params = self.actor.total_params + self.critic.total_params\n\n    # Init config details\n    self.config = RLAgentConfig(\n        agent=self.__class__.__name__,\n        env=env_id,\n        seed=self.seed,\n        model_details=ModelDetails(\n            **locals(),\n            state_dim=self.state_dim,\n            action_dim=self.action_dim,\n            action_type=\"discrete\",\n            exploration_type=\"Entropy\",\n            actor=self.actor.config,\n            critic=self.critic.config,\n            entropy=self.entropy.config(),\n        ),\n        buffer=self.buffer.config(),\n        torch=TorchConfig(\n            device=str(self.device),\n            optimizer=optim.__name__,\n            loss=self.loss.__class__.__name__,\n        ),\n    )\n\n    self.metadata = self.set_metadata(locals(), self.seed)\n</code></pre>"},{"location":"learn/reference/models/nf/#velora.models.nf.NeuroFlow.predict","title":"<code>predict(state, hidden=None, *, train_mode=False)</code>","text":"<p>Makes an action prediction using the Actor network.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>torch.Tensor</code> <p>the current state</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the current hidden state</p> <code>None</code> <code>train_mode</code> <code>bool</code> <p>whether to make deterministic (when <code>False</code>) or stochastic (when <code>True</code>) action predictions</p> <code>False</code> <p>Returns:</p> Name Type Description <code>action</code> <code>torch.Tensor</code> <p>the action prediction on the given state</p> <code>hidden</code> <code>torch.Tensor</code> <p>the Actor network's new hidden state</p> Source code in <code>velora/models/nf/agent.py</code> Python<pre><code>@override\ndef predict(\n    self,\n    state: torch.Tensor,\n    hidden: torch.Tensor | None = None,\n    *,\n    train_mode: bool = False,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Makes an action prediction using the Actor network.\n\n    Parameters:\n        state (torch.Tensor): the current state\n        hidden (torch.Tensor, optional): the current hidden state\n        train_mode (bool, optional): whether to make deterministic (when\n            `False`) or stochastic (when `True`) action predictions\n\n    Returns:\n        action (torch.Tensor): the action prediction on the given state\n        hidden (torch.Tensor): the Actor network's new hidden state\n    \"\"\"\n    self.actor.eval_mode()\n    with torch.no_grad():\n        state = state.unsqueeze(0) if state.dim() &lt; 2 else state\n\n        if not train_mode:\n            action, hidden = self.actor.predict(state, hidden)\n        else:\n            action, _, _, hidden = self.actor.forward(state, hidden)\n\n    self.actor.train_mode()\n    return action, hidden\n</code></pre>"},{"location":"learn/reference/models/nf/#velora.models.nf.NeuroFlow.train","title":"<code>train(batch_size, *, n_episodes=10000, callbacks=None, log_freq=10, display_count=100, window_size=100, max_steps=1000, warmup_steps=None)</code>","text":"<p>Trains the agent on a Gymnasium environment using a <code>ReplayBuffer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of features in a single batch</p> required <code>n_episodes</code> <code>int</code> <p>the total number of episodes to train for</p> <code>10000</code> <code>callbacks</code> <code>List[TrainCallback]</code> <p>a list of training callbacks that are applied during the training process</p> <code>None</code> <code>log_freq</code> <code>int</code> <p>metric logging frequency (in episodes)</p> <code>10</code> <code>display_count</code> <code>int</code> <p>console training progress frequency (in episodes)</p> <code>100</code> <code>window_size</code> <code>int</code> <p>the reward moving average size (in episodes)</p> <code>100</code> <code>max_steps</code> <code>int</code> <p>the total number of steps per episode</p> <code>1000</code> <code>warmup_steps</code> <code>int</code> <p>the number of samples to generate in the buffer before starting training. If <code>None</code> uses <code>batch_size * 2</code></p> <code>None</code> Source code in <code>velora/models/nf/agent.py</code> Python<pre><code>@override\ndef train(\n    self,\n    batch_size: int,\n    *,\n    n_episodes: int = 10_000,\n    callbacks: List[\"TrainCallback\"] | None = None,\n    log_freq: int = 10,\n    display_count: int = 100,\n    window_size: int = 100,\n    max_steps: int = 1000,\n    warmup_steps: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Trains the agent on a Gymnasium environment using a `ReplayBuffer`.\n\n    Parameters:\n        batch_size (int): the number of features in a single batch\n        n_episodes (int, optional): the total number of episodes to train for\n        callbacks (List[TrainCallback], optional): a list of training callbacks\n            that are applied during the training process\n        log_freq (int, optional): metric logging frequency (in episodes)\n        display_count (int, optional): console training progress frequency\n            (in episodes)\n        window_size (int, optional): the reward moving average size\n            (in episodes)\n        max_steps (int, optional): the total number of steps per episode\n        warmup_steps (int, optional): the number of samples to generate in the\n            buffer before starting training. If `None` uses `batch_size * 2`\n    \"\"\"\n    warmup_steps = batch_size * 2 if warmup_steps is None else warmup_steps\n\n    # Add training details to config\n    self.config = self.config.update(self._set_train_params(locals()))\n\n    # Display console details\n    self.env.reset(seed=self.seed)  # Set seed\n    training_info(\n        self,\n        n_episodes,\n        batch_size,\n        window_size,\n        warmup_steps,\n        callbacks or [],\n    )\n\n    if warmup_steps &gt; 0:\n        self.buffer.warm(self, warmup_steps, 2 if warmup_steps &lt; 8 else 8)\n\n    with TrainHandler(\n        self, n_episodes, max_steps, log_freq, window_size, callbacks\n    ) as handler:\n        for current_ep in range(1, n_episodes + 1):\n            ep_reward = 0.0\n            hidden = None\n\n            state, _ = handler.env.reset()\n\n            for current_step in range(1, max_steps + 1):\n                action, hidden = self.predict(state, hidden, train_mode=True)\n                next_state, reward, terminated, truncated, info = handler.env.step(\n                    action\n                )\n                done = terminated or truncated\n\n                self.buffer.add(state, action, reward, next_state, done, hidden)\n\n                losses = self._train_step(batch_size)\n\n                handler.metrics.add_step(**losses)\n                handler.step(current_step)\n\n                state = next_state\n\n                if done:\n                    ep_reward = info[\"episode\"][\"r\"].item()\n\n                    handler.metrics.add_episode(\n                        current_ep,\n                        info[\"episode\"][\"r\"],\n                        info[\"episode\"][\"l\"],\n                    )\n                    break\n\n            if current_ep % log_freq == 0 or current_ep == n_episodes:\n                handler.log(current_ep, \"episode\")\n\n            if (\n                current_ep % display_count == 0\n                or current_ep == n_episodes\n                or handler.stop()\n            ):\n                handler.metrics.info(current_ep)\n\n            handler.episode(current_ep, ep_reward)\n\n            # Terminate on early stopping\n            if handler.stop():\n                break\n</code></pre>"},{"location":"learn/reference/models/nf/#velora.models.nf.NeuroFlowCT","title":"<code>NeuroFlowCT</code>","text":"<p>               Bases: <code>RLModuleAgent</code></p> Documentation <p>User Guide - Tutorials: NeuroFlow - Continuous</p> <p>A custom Liquid RL agent that combines a variety of RL techniques.</p> <p>Designed for <code>continuous</code> action spaces.</p> Source code in <code>velora/models/nf/agent.py</code> Python<pre><code>class NeuroFlowCT(RLModuleAgent):\n    \"\"\"\n    ???+ abstract \"Documentation\"\n\n        &gt; [User Guide - Tutorials: NeuroFlow - Continuous](https://velora.achronus.dev/learn/tutorial/agents/nf)\n\n    A custom Liquid RL agent that combines a variety of RL techniques.\n\n    Designed for `continuous` action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        env_id: Union[str, \"ContinuousGymNames\"],\n        actor_neurons: int,\n        critic_neurons: int,\n        *,\n        optim: Type[optim.Optimizer] = optim.Adam,\n        buffer_size: int = 1_000_000,\n        actor_lr: float = 3e-4,\n        critic_lr: float = 3e-4,\n        alpha_lr: float = 3e-4,\n        initial_alpha: float = 1.0,\n        log_std: Tuple[float, float] = (-5, 2),\n        tau: float = 0.005,\n        gamma: float = 0.99,\n        device: torch.device | None = None,\n        seed: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            env_id (str): the Gymnasium environment ID to train the model on\n            actor_neurons (int): number of decision nodes (inter and command nodes)\n                for the actor\n            critic_neurons (int): number of decision nodes (inter and command nodes)\n                for the critic\n            optim (Type[torch.optim.Optimizer], optional): the type of `PyTorch`\n                optimizer to use\n            buffer_size (int, optional): the maximum size of the `ReplayBuffer`\n            actor_lr (float, optional): the actor optimizer learning rate\n            critic_lr (float, optional): the critic optimizer learning rate\n            alpha_lr (float, optional): the entropy parameter learning rate\n            initial_alpha (float, optional): the starting entropy coefficient value\n            log_std (Tuple[float, float], optional): `(low, high)` bounds for the\n                log standard deviation of the action distribution. Controls the\n                variance of actions\n            tau (float, optional): the soft update factor used to slowly update\n                the target networks\n            gamma (float, optional): the reward discount factor\n            device (torch.device, optional): the device to perform computations on\n            seed (int, optional): random number seed for experiment\n                reproducibility. When `None` generates a seed automatically\n        \"\"\"\n        env = gym.make(env_id, render_mode=\"rgb_array\")\n\n        if not isinstance(env.action_space, gym.spaces.Box):\n            raise EnvironmentError(\n                f\"Invalid '{env.action_space=}'. Must be 'gym.spaces.Box'.\"\n            )\n\n        super().__init__(\n            env,\n            actor_neurons,\n            critic_neurons,\n            buffer_size,\n            optim,\n            device,\n            seed,\n        )\n\n        self.initial_alpha = initial_alpha\n        self.log_std = log_std\n        self.gamma = gamma\n        self.tau = tau\n\n        self.actor: ActorModule = ActorModule(\n            self.state_dim,\n            self.actor_neurons,\n            self.action_dim,\n            self.action_scale,\n            self.action_bias,\n            log_std_min=log_std[0],\n            log_std_max=log_std[1],\n            optim=optim,\n            lr=actor_lr,\n            device=self.device,\n        )\n\n        self.critic: CriticModule = CriticModule(\n            self.state_dim,\n            self.critic_neurons,\n            self.action_dim,\n            optim=optim,\n            lr=critic_lr,\n            tau=tau,\n            device=self.device,\n        )\n\n        self.hidden_dim = self.actor.hidden_size\n\n        self.entropy: EntropyModule = EntropyModule(\n            self.action_dim,\n            initial_alpha=initial_alpha,\n            optim=optim,\n            lr=alpha_lr,\n            device=device,\n        )\n\n        self.loss = nn.MSELoss()\n        self.buffer: ReplayBuffer = ReplayBuffer(\n            buffer_size,\n            self.state_dim,\n            self.action_dim,\n            self.actor.hidden_size,\n            device=device,\n        )\n\n        self.active_params = self.actor.active_params + self.critic.active_params\n        self.total_params = self.actor.total_params + self.critic.total_params\n\n        # Init config details\n        self.config = RLAgentConfig(\n            agent=self.__class__.__name__,\n            env=env_id,\n            seed=self.seed,\n            model_details=ModelDetails(\n                **locals(),\n                state_dim=self.state_dim,\n                action_dim=self.action_dim,\n                exploration_type=\"Entropy\",\n                actor=self.actor.config,\n                critic=self.critic.config,\n                entropy=self.entropy.config(),\n            ),\n            buffer=self.buffer.config(),\n            torch=TorchConfig(\n                device=str(self.device),\n                optimizer=optim.__name__,\n                loss=self.loss.__class__.__name__,\n            ),\n        )\n\n        self.metadata = self.set_metadata(locals(), self.seed)\n\n    def _update_critics(self, batch: \"BatchExperience\") -&gt; torch.Tensor:\n        \"\"\"\n        Helper method. Performs Critic network updates.\n\n        Parameters:\n            batch (BatchExperience): an object containing a batch of experience\n                with `(states, actions, rewards, next_states, dones, hidden)`\n                from the buffer\n\n        Returns:\n            critic_loss (torch.Tensor): total Critic network loss `(c1_loss + c2_loss)`.\n        \"\"\"\n        with torch.no_grad():\n            next_actions, next_log_probs, _ = self.actor.forward(\n                batch.next_states, batch.hiddens\n            )\n\n            # Compute target Q-value\n            next_q = self.critic.target_predict(batch.next_states, next_actions)\n            next_q = next_q - self.entropy.alpha * next_log_probs\n            target_q = batch.rewards + (1 - batch.dones) * self.gamma * next_q\n\n        current_q1, current_q2 = self.critic.predict(batch.states, batch.actions)\n\n        # Calculate loss\n        c1_loss: torch.Tensor = self.loss(current_q1, target_q)\n        c2_loss: torch.Tensor = self.loss(current_q2, target_q)\n        critic_loss: torch.Tensor = c1_loss + c2_loss\n\n        # Update critics\n        self.critic.gradient_step(c1_loss, c2_loss)\n\n        return critic_loss\n\n    def _train_step(self, batch_size: int) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Helper method. Performs a single training step.\n\n        Parameters:\n            batch_size (int): number of samples in a batch\n\n        Returns:\n            losses (Dict[str, torch.Tensor]): a dictionary of losses -\n\n            - critic: the total critic loss.\n            - actor: the actor loss.\n            - entropy: the entropy loss.\n        \"\"\"\n        batch = self.buffer.sample(batch_size)\n\n        # Compute critic loss\n        critic_loss = self._update_critics(batch)\n\n        # Make predictions\n        actions, log_probs, _ = self.actor.forward(batch.states, batch.hiddens)\n        q1, q2 = self.critic.predict(batch.states, actions)\n\n        # Compute actor and entropy losses\n        next_q = torch.min(q1, q2)\n        actor_loss = (self.entropy.alpha * log_probs - next_q).mean()\n        entropy_loss = self.entropy.compute_loss(log_probs)\n\n        # Update gradients\n        self.actor.gradient_step(actor_loss)\n        self.entropy.gradient_step(entropy_loss)\n\n        # Update target networks\n        self.critic.update_targets()\n\n        return {\n            \"critic\": critic_loss.detach(),\n            \"actor\": actor_loss.detach(),\n            \"entropy\": entropy_loss.detach(),\n        }\n\n    @override\n    def train(\n        self,\n        batch_size: int,\n        *,\n        n_episodes: int = 10_000,\n        callbacks: List[\"TrainCallback\"] | None = None,\n        log_freq: int = 10,\n        display_count: int = 100,\n        window_size: int = 100,\n        max_steps: int = 1000,\n        warmup_steps: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Trains the agent on a Gymnasium environment using a `ReplayBuffer`.\n\n        Parameters:\n            batch_size (int): the number of features in a single batch\n            n_episodes (int, optional): the total number of episodes to train for\n            callbacks (List[TrainCallback], optional): a list of training callbacks\n                that are applied during the training process\n            log_freq (int, optional): metric logging frequency (in episodes)\n            display_count (int, optional): console training progress frequency\n                (in episodes)\n            window_size (int, optional): the reward moving average size\n                (in episodes)\n            max_steps (int, optional): the total number of steps per episode\n            warmup_steps (int, optional): the number of samples to generate in the\n                buffer before starting training. If `None` uses `batch_size * 2`\n        \"\"\"\n        warmup_steps = batch_size * 2 if warmup_steps is None else warmup_steps\n\n        # Add training details to config\n        self.config = self.config.update(self._set_train_params(locals()))\n\n        # Display console details\n        self.env.reset(seed=self.seed)  # Set seed\n        training_info(\n            self,\n            n_episodes,\n            batch_size,\n            window_size,\n            warmup_steps,\n            callbacks or [],\n        )\n\n        if warmup_steps &gt; 0:\n            self.buffer.warm(self, warmup_steps, 2 if warmup_steps &lt; 8 else 8)\n\n        with TrainHandler(\n            self, n_episodes, max_steps, log_freq, window_size, callbacks\n        ) as handler:\n            for current_ep in range(1, n_episodes + 1):\n                ep_reward = 0.0\n                hidden = None\n\n                state, _ = handler.env.reset()\n\n                for current_step in range(1, max_steps + 1):\n                    action, hidden = self.predict(state, hidden, train_mode=True)\n                    next_state, reward, terminated, truncated, info = handler.env.step(\n                        action\n                    )\n                    done = terminated or truncated\n\n                    self.buffer.add(state, action, reward, next_state, done, hidden)\n\n                    losses = self._train_step(batch_size)\n\n                    handler.metrics.add_step(**losses)\n                    handler.step(current_step)\n\n                    state = next_state\n\n                    if done:\n                        ep_reward = info[\"episode\"][\"r\"].item()\n\n                        handler.metrics.add_episode(\n                            current_ep,\n                            info[\"episode\"][\"r\"],\n                            info[\"episode\"][\"l\"],\n                        )\n                        break\n\n                if current_ep % log_freq == 0 or current_ep == n_episodes:\n                    handler.log(current_ep, \"episode\")\n\n                if (\n                    current_ep % display_count == 0\n                    or current_ep == n_episodes\n                    or handler.stop()\n                ):\n                    handler.metrics.info(current_ep)\n\n                handler.episode(current_ep, ep_reward)\n\n                # Terminate on early stopping\n                if handler.stop():\n                    break\n\n    @override\n    def predict(\n        self,\n        state: torch.Tensor,\n        hidden: torch.Tensor | None = None,\n        *,\n        train_mode: bool = False,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Makes an action prediction using the Actor network.\n\n        Parameters:\n            state (torch.Tensor): the current state\n            hidden (torch.Tensor, optional): the current hidden state\n            train_mode (bool, optional): whether to make deterministic (when\n                `False`) or stochastic (when `True`) action predictions\n\n        Returns:\n            action (torch.Tensor): the action prediction on the given state\n            hidden (torch.Tensor): the Actor network's new hidden state\n        \"\"\"\n        self.actor.eval_mode()\n        with torch.no_grad():\n            state = state.unsqueeze(0) if state.dim() &lt; 2 else state\n\n            if not train_mode:\n                action, hidden = self.actor.predict(state, hidden)\n            else:\n                action, _, hidden = self.actor.forward(state, hidden)\n\n        self.actor.train_mode()\n        return action, hidden\n\n    def save(\n        self,\n        dirpath: str | Path,\n        *,\n        buffer: bool = False,\n        config: bool = False,\n    ) -&gt; None:\n        save_model(self, dirpath, buffer=buffer, config=config)\n\n    @classmethod\n    def load(cls, dirpath: str | Path, *, buffer: bool = False) -&gt; Self:\n        return load_model(cls, dirpath, buffer=buffer)\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            + \", \".join([f\"{key}={val}\" for key, val in self.metadata.items()])\n            + \")\"\n        )\n</code></pre>"},{"location":"learn/reference/models/nf/#velora.models.nf.NeuroFlowCT.__init__","title":"<code>__init__(env_id, actor_neurons, critic_neurons, *, optim=optim.Adam, buffer_size=1000000, actor_lr=0.0003, critic_lr=0.0003, alpha_lr=0.0003, initial_alpha=1.0, log_std=(-5, 2), tau=0.005, gamma=0.99, device=None, seed=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>env_id</code> <code>str</code> <p>the Gymnasium environment ID to train the model on</p> required <code>actor_neurons</code> <code>int</code> <p>number of decision nodes (inter and command nodes) for the actor</p> required <code>critic_neurons</code> <code>int</code> <p>number of decision nodes (inter and command nodes) for the critic</p> required <code>optim</code> <code>Type[torch.optim.Optimizer]</code> <p>the type of <code>PyTorch</code> optimizer to use</p> <code>optim.Adam</code> <code>buffer_size</code> <code>int</code> <p>the maximum size of the <code>ReplayBuffer</code></p> <code>1000000</code> <code>actor_lr</code> <code>float</code> <p>the actor optimizer learning rate</p> <code>0.0003</code> <code>critic_lr</code> <code>float</code> <p>the critic optimizer learning rate</p> <code>0.0003</code> <code>alpha_lr</code> <code>float</code> <p>the entropy parameter learning rate</p> <code>0.0003</code> <code>initial_alpha</code> <code>float</code> <p>the starting entropy coefficient value</p> <code>1.0</code> <code>log_std</code> <code>Tuple[float, float]</code> <p><code>(low, high)</code> bounds for the log standard deviation of the action distribution. Controls the variance of actions</p> <code>(-5, 2)</code> <code>tau</code> <code>float</code> <p>the soft update factor used to slowly update the target networks</p> <code>0.005</code> <code>gamma</code> <code>float</code> <p>the reward discount factor</p> <code>0.99</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> <code>seed</code> <code>int</code> <p>random number seed for experiment reproducibility. When <code>None</code> generates a seed automatically</p> <code>None</code> Source code in <code>velora/models/nf/agent.py</code> Python<pre><code>def __init__(\n    self,\n    env_id: Union[str, \"ContinuousGymNames\"],\n    actor_neurons: int,\n    critic_neurons: int,\n    *,\n    optim: Type[optim.Optimizer] = optim.Adam,\n    buffer_size: int = 1_000_000,\n    actor_lr: float = 3e-4,\n    critic_lr: float = 3e-4,\n    alpha_lr: float = 3e-4,\n    initial_alpha: float = 1.0,\n    log_std: Tuple[float, float] = (-5, 2),\n    tau: float = 0.005,\n    gamma: float = 0.99,\n    device: torch.device | None = None,\n    seed: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        env_id (str): the Gymnasium environment ID to train the model on\n        actor_neurons (int): number of decision nodes (inter and command nodes)\n            for the actor\n        critic_neurons (int): number of decision nodes (inter and command nodes)\n            for the critic\n        optim (Type[torch.optim.Optimizer], optional): the type of `PyTorch`\n            optimizer to use\n        buffer_size (int, optional): the maximum size of the `ReplayBuffer`\n        actor_lr (float, optional): the actor optimizer learning rate\n        critic_lr (float, optional): the critic optimizer learning rate\n        alpha_lr (float, optional): the entropy parameter learning rate\n        initial_alpha (float, optional): the starting entropy coefficient value\n        log_std (Tuple[float, float], optional): `(low, high)` bounds for the\n            log standard deviation of the action distribution. Controls the\n            variance of actions\n        tau (float, optional): the soft update factor used to slowly update\n            the target networks\n        gamma (float, optional): the reward discount factor\n        device (torch.device, optional): the device to perform computations on\n        seed (int, optional): random number seed for experiment\n            reproducibility. When `None` generates a seed automatically\n    \"\"\"\n    env = gym.make(env_id, render_mode=\"rgb_array\")\n\n    if not isinstance(env.action_space, gym.spaces.Box):\n        raise EnvironmentError(\n            f\"Invalid '{env.action_space=}'. Must be 'gym.spaces.Box'.\"\n        )\n\n    super().__init__(\n        env,\n        actor_neurons,\n        critic_neurons,\n        buffer_size,\n        optim,\n        device,\n        seed,\n    )\n\n    self.initial_alpha = initial_alpha\n    self.log_std = log_std\n    self.gamma = gamma\n    self.tau = tau\n\n    self.actor: ActorModule = ActorModule(\n        self.state_dim,\n        self.actor_neurons,\n        self.action_dim,\n        self.action_scale,\n        self.action_bias,\n        log_std_min=log_std[0],\n        log_std_max=log_std[1],\n        optim=optim,\n        lr=actor_lr,\n        device=self.device,\n    )\n\n    self.critic: CriticModule = CriticModule(\n        self.state_dim,\n        self.critic_neurons,\n        self.action_dim,\n        optim=optim,\n        lr=critic_lr,\n        tau=tau,\n        device=self.device,\n    )\n\n    self.hidden_dim = self.actor.hidden_size\n\n    self.entropy: EntropyModule = EntropyModule(\n        self.action_dim,\n        initial_alpha=initial_alpha,\n        optim=optim,\n        lr=alpha_lr,\n        device=device,\n    )\n\n    self.loss = nn.MSELoss()\n    self.buffer: ReplayBuffer = ReplayBuffer(\n        buffer_size,\n        self.state_dim,\n        self.action_dim,\n        self.actor.hidden_size,\n        device=device,\n    )\n\n    self.active_params = self.actor.active_params + self.critic.active_params\n    self.total_params = self.actor.total_params + self.critic.total_params\n\n    # Init config details\n    self.config = RLAgentConfig(\n        agent=self.__class__.__name__,\n        env=env_id,\n        seed=self.seed,\n        model_details=ModelDetails(\n            **locals(),\n            state_dim=self.state_dim,\n            action_dim=self.action_dim,\n            exploration_type=\"Entropy\",\n            actor=self.actor.config,\n            critic=self.critic.config,\n            entropy=self.entropy.config(),\n        ),\n        buffer=self.buffer.config(),\n        torch=TorchConfig(\n            device=str(self.device),\n            optimizer=optim.__name__,\n            loss=self.loss.__class__.__name__,\n        ),\n    )\n\n    self.metadata = self.set_metadata(locals(), self.seed)\n</code></pre>"},{"location":"learn/reference/models/nf/#velora.models.nf.NeuroFlowCT.predict","title":"<code>predict(state, hidden=None, *, train_mode=False)</code>","text":"<p>Makes an action prediction using the Actor network.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>torch.Tensor</code> <p>the current state</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the current hidden state</p> <code>None</code> <code>train_mode</code> <code>bool</code> <p>whether to make deterministic (when <code>False</code>) or stochastic (when <code>True</code>) action predictions</p> <code>False</code> <p>Returns:</p> Name Type Description <code>action</code> <code>torch.Tensor</code> <p>the action prediction on the given state</p> <code>hidden</code> <code>torch.Tensor</code> <p>the Actor network's new hidden state</p> Source code in <code>velora/models/nf/agent.py</code> Python<pre><code>@override\ndef predict(\n    self,\n    state: torch.Tensor,\n    hidden: torch.Tensor | None = None,\n    *,\n    train_mode: bool = False,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Makes an action prediction using the Actor network.\n\n    Parameters:\n        state (torch.Tensor): the current state\n        hidden (torch.Tensor, optional): the current hidden state\n        train_mode (bool, optional): whether to make deterministic (when\n            `False`) or stochastic (when `True`) action predictions\n\n    Returns:\n        action (torch.Tensor): the action prediction on the given state\n        hidden (torch.Tensor): the Actor network's new hidden state\n    \"\"\"\n    self.actor.eval_mode()\n    with torch.no_grad():\n        state = state.unsqueeze(0) if state.dim() &lt; 2 else state\n\n        if not train_mode:\n            action, hidden = self.actor.predict(state, hidden)\n        else:\n            action, _, hidden = self.actor.forward(state, hidden)\n\n    self.actor.train_mode()\n    return action, hidden\n</code></pre>"},{"location":"learn/reference/models/nf/#velora.models.nf.NeuroFlowCT.train","title":"<code>train(batch_size, *, n_episodes=10000, callbacks=None, log_freq=10, display_count=100, window_size=100, max_steps=1000, warmup_steps=None)</code>","text":"<p>Trains the agent on a Gymnasium environment using a <code>ReplayBuffer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of features in a single batch</p> required <code>n_episodes</code> <code>int</code> <p>the total number of episodes to train for</p> <code>10000</code> <code>callbacks</code> <code>List[TrainCallback]</code> <p>a list of training callbacks that are applied during the training process</p> <code>None</code> <code>log_freq</code> <code>int</code> <p>metric logging frequency (in episodes)</p> <code>10</code> <code>display_count</code> <code>int</code> <p>console training progress frequency (in episodes)</p> <code>100</code> <code>window_size</code> <code>int</code> <p>the reward moving average size (in episodes)</p> <code>100</code> <code>max_steps</code> <code>int</code> <p>the total number of steps per episode</p> <code>1000</code> <code>warmup_steps</code> <code>int</code> <p>the number of samples to generate in the buffer before starting training. If <code>None</code> uses <code>batch_size * 2</code></p> <code>None</code> Source code in <code>velora/models/nf/agent.py</code> Python<pre><code>@override\ndef train(\n    self,\n    batch_size: int,\n    *,\n    n_episodes: int = 10_000,\n    callbacks: List[\"TrainCallback\"] | None = None,\n    log_freq: int = 10,\n    display_count: int = 100,\n    window_size: int = 100,\n    max_steps: int = 1000,\n    warmup_steps: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Trains the agent on a Gymnasium environment using a `ReplayBuffer`.\n\n    Parameters:\n        batch_size (int): the number of features in a single batch\n        n_episodes (int, optional): the total number of episodes to train for\n        callbacks (List[TrainCallback], optional): a list of training callbacks\n            that are applied during the training process\n        log_freq (int, optional): metric logging frequency (in episodes)\n        display_count (int, optional): console training progress frequency\n            (in episodes)\n        window_size (int, optional): the reward moving average size\n            (in episodes)\n        max_steps (int, optional): the total number of steps per episode\n        warmup_steps (int, optional): the number of samples to generate in the\n            buffer before starting training. If `None` uses `batch_size * 2`\n    \"\"\"\n    warmup_steps = batch_size * 2 if warmup_steps is None else warmup_steps\n\n    # Add training details to config\n    self.config = self.config.update(self._set_train_params(locals()))\n\n    # Display console details\n    self.env.reset(seed=self.seed)  # Set seed\n    training_info(\n        self,\n        n_episodes,\n        batch_size,\n        window_size,\n        warmup_steps,\n        callbacks or [],\n    )\n\n    if warmup_steps &gt; 0:\n        self.buffer.warm(self, warmup_steps, 2 if warmup_steps &lt; 8 else 8)\n\n    with TrainHandler(\n        self, n_episodes, max_steps, log_freq, window_size, callbacks\n    ) as handler:\n        for current_ep in range(1, n_episodes + 1):\n            ep_reward = 0.0\n            hidden = None\n\n            state, _ = handler.env.reset()\n\n            for current_step in range(1, max_steps + 1):\n                action, hidden = self.predict(state, hidden, train_mode=True)\n                next_state, reward, terminated, truncated, info = handler.env.step(\n                    action\n                )\n                done = terminated or truncated\n\n                self.buffer.add(state, action, reward, next_state, done, hidden)\n\n                losses = self._train_step(batch_size)\n\n                handler.metrics.add_step(**losses)\n                handler.step(current_step)\n\n                state = next_state\n\n                if done:\n                    ep_reward = info[\"episode\"][\"r\"].item()\n\n                    handler.metrics.add_episode(\n                        current_ep,\n                        info[\"episode\"][\"r\"],\n                        info[\"episode\"][\"l\"],\n                    )\n                    break\n\n            if current_ep % log_freq == 0 or current_ep == n_episodes:\n                handler.log(current_ep, \"episode\")\n\n            if (\n                current_ep % display_count == 0\n                or current_ep == n_episodes\n                or handler.stop()\n            ):\n                handler.metrics.info(current_ep)\n\n            handler.episode(current_ep, ep_reward)\n\n            # Terminate on early stopping\n            if handler.stop():\n                break\n</code></pre>"},{"location":"learn/reference/models/sac/","title":"velora.models.sac","text":"Documentation <p>Customization: Modules</p> <p>Soft Actor-Critic (SAC) network modules built using PyTorch.</p>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActor","title":"<code>SACActor</code>","text":"<p>               Bases: <code>LiquidNCPModule</code></p> <p>A Liquid NCP Actor Network for the SAC algorithm. Outputs a Gaussian distribution over actions.</p> <p>Usable with continuous action spaces.</p> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>class SACActor(LiquidNCPModule):\n    \"\"\"\n    A Liquid NCP Actor Network for the SAC algorithm. Outputs a Gaussian\n    distribution over actions.\n\n    Usable with continuous action spaces.\n    \"\"\"\n\n    action_scale: torch.Tensor\n    action_bias: torch.Tensor\n\n    def __init__(\n        self,\n        num_obs: int,\n        n_neurons: int,\n        num_actions: int,\n        action_scale: torch.Tensor,\n        action_bias: torch.Tensor,\n        *,\n        log_std_min: float = -5,\n        log_std_max: float = 2,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            num_obs (int): the number of input observations\n            n_neurons (int): the number of hidden neurons\n            num_actions (int): the number of actions\n            action_scale (torch.Tensor): scale factor to map normalized actions to\n                environment's action range\n            action_bias (torch.Tensor): bias/offset to center normalized actions to\n                environment's action range\n            log_std_min (float, optional): lower bound for the log standard\n                deviation of the action distribution. Controls the minimum\n                variance of actions\n            log_std_max (float, optional): upper bound for the log standard\n                deviation of the action distribution. Controls the maximum\n                variance of actions\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__(num_obs, n_neurons, num_actions * 2, device=device)\n\n        self.log_std_min = log_std_min\n        self.log_std_max = log_std_max\n\n        self.register_buffer(\"action_scale\", action_scale)\n        self.register_buffer(\"action_bias\", action_bias)\n\n    @torch.jit.ignore\n    def get_sample(\n        self, mean: torch.Tensor, std: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Computes a set of action samples and log probabilities using the\n        reparameterization trick from a Gaussian distribution.\n\n        Parameters:\n            mean (torch.Tensor): network prediction means.\n            std (torch.Tensor): network standard deviation predictions.\n\n        Returns:\n            actions (torch.Tensor): action samples.\n            log_probs (torch.Tensor): log probabilities.\n        \"\"\"\n        dist = Normal(mean, std)\n        x_t = dist.rsample()  # Reparameterization trick\n        log_probs = dist.log_prob(x_t)\n\n        return x_t, log_probs\n\n    @torch.jit.ignore\n    def predict(\n        self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a deterministic prediction.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            hidden (torch.Tensor, optional): the hidden state\n\n        Returns:\n            actions (torch.Tensor): sampled actions\n            hidden (torch.Tensor): the new hidden state\n        \"\"\"\n        x, new_hidden = self.ncp(obs, hidden)\n\n        mean, _ = torch.chunk(x, 2, dim=-1)\n\n        # Bound actions between [-1, 1]\n        actions_normalized = torch.tanh(mean)\n\n        # Scale back to env action space\n        actions = actions_normalized * self.action_scale + self.action_bias\n\n        return actions, new_hidden\n\n    def forward(\n        self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            hidden (torch.Tensor, optional): the hidden state\n\n        Returns:\n            actions (torch.Tensor): the action predictions.\n            log_prob (torch.Tensor): log probabilities of actions.\n            hidden (torch.Tensor): the new hidden state.\n        \"\"\"\n        x, new_hidden = self.ncp(obs, hidden)\n\n        # Split output into mean and log_std\n        mean, log_std = torch.chunk(x, 2, dim=-1)\n\n        # Bound between [-20, 2]\n        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n        std = log_std.exp()\n\n        # Sample from normal distribution\n        x_t, dist_log_probs = self.get_sample(mean, std)\n        actions_normalized = torch.tanh(x_t)  # Bounded: [-1, 1]\n\n        # Scale back to environment's action space\n        actions = actions_normalized * self.action_scale + self.action_bias\n\n        # Calculate log probability, accounting for tanh\n        log_prob = dist_log_probs - torch.log(\n            self.action_scale * (1 - actions_normalized.pow(2)) + 1e-6\n        )\n        log_prob = log_prob.sum(dim=-1, keepdim=True)\n\n        return actions, log_prob, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActor.__init__","title":"<code>__init__(num_obs, n_neurons, num_actions, action_scale, action_bias, *, log_std_min=-5, log_std_max=2, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_obs</code> <code>int</code> <p>the number of input observations</p> required <code>n_neurons</code> <code>int</code> <p>the number of hidden neurons</p> required <code>num_actions</code> <code>int</code> <p>the number of actions</p> required <code>action_scale</code> <code>torch.Tensor</code> <p>scale factor to map normalized actions to environment's action range</p> required <code>action_bias</code> <code>torch.Tensor</code> <p>bias/offset to center normalized actions to environment's action range</p> required <code>log_std_min</code> <code>float</code> <p>lower bound for the log standard deviation of the action distribution. Controls the minimum variance of actions</p> <code>-5</code> <code>log_std_max</code> <code>float</code> <p>upper bound for the log standard deviation of the action distribution. Controls the maximum variance of actions</p> <code>2</code> <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>def __init__(\n    self,\n    num_obs: int,\n    n_neurons: int,\n    num_actions: int,\n    action_scale: torch.Tensor,\n    action_bias: torch.Tensor,\n    *,\n    log_std_min: float = -5,\n    log_std_max: float = 2,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        num_obs (int): the number of input observations\n        n_neurons (int): the number of hidden neurons\n        num_actions (int): the number of actions\n        action_scale (torch.Tensor): scale factor to map normalized actions to\n            environment's action range\n        action_bias (torch.Tensor): bias/offset to center normalized actions to\n            environment's action range\n        log_std_min (float, optional): lower bound for the log standard\n            deviation of the action distribution. Controls the minimum\n            variance of actions\n        log_std_max (float, optional): upper bound for the log standard\n            deviation of the action distribution. Controls the maximum\n            variance of actions\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__(num_obs, n_neurons, num_actions * 2, device=device)\n\n    self.log_std_min = log_std_min\n    self.log_std_max = log_std_max\n\n    self.register_buffer(\"action_scale\", action_scale)\n    self.register_buffer(\"action_bias\", action_bias)\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActor.forward","title":"<code>forward(obs, hidden=None)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>the action predictions.</p> <code>log_prob</code> <code>torch.Tensor</code> <p>log probabilities of actions.</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state.</p> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>def forward(\n    self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        hidden (torch.Tensor, optional): the hidden state\n\n    Returns:\n        actions (torch.Tensor): the action predictions.\n        log_prob (torch.Tensor): log probabilities of actions.\n        hidden (torch.Tensor): the new hidden state.\n    \"\"\"\n    x, new_hidden = self.ncp(obs, hidden)\n\n    # Split output into mean and log_std\n    mean, log_std = torch.chunk(x, 2, dim=-1)\n\n    # Bound between [-20, 2]\n    log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n    std = log_std.exp()\n\n    # Sample from normal distribution\n    x_t, dist_log_probs = self.get_sample(mean, std)\n    actions_normalized = torch.tanh(x_t)  # Bounded: [-1, 1]\n\n    # Scale back to environment's action space\n    actions = actions_normalized * self.action_scale + self.action_bias\n\n    # Calculate log probability, accounting for tanh\n    log_prob = dist_log_probs - torch.log(\n        self.action_scale * (1 - actions_normalized.pow(2)) + 1e-6\n    )\n    log_prob = log_prob.sum(dim=-1, keepdim=True)\n\n    return actions, log_prob, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActor.get_sample","title":"<code>get_sample(mean, std)</code>","text":"<p>Computes a set of action samples and log probabilities using the reparameterization trick from a Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>torch.Tensor</code> <p>network prediction means.</p> required <code>std</code> <code>torch.Tensor</code> <p>network standard deviation predictions.</p> required <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>action samples.</p> <code>log_probs</code> <code>torch.Tensor</code> <p>log probabilities.</p> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>@torch.jit.ignore\ndef get_sample(\n    self, mean: torch.Tensor, std: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Computes a set of action samples and log probabilities using the\n    reparameterization trick from a Gaussian distribution.\n\n    Parameters:\n        mean (torch.Tensor): network prediction means.\n        std (torch.Tensor): network standard deviation predictions.\n\n    Returns:\n        actions (torch.Tensor): action samples.\n        log_probs (torch.Tensor): log probabilities.\n    \"\"\"\n    dist = Normal(mean, std)\n    x_t = dist.rsample()  # Reparameterization trick\n    log_probs = dist.log_prob(x_t)\n\n    return x_t, log_probs\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActor.predict","title":"<code>predict(obs, hidden=None)</code>","text":"<p>Performs a deterministic prediction.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>sampled actions</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state</p> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>@torch.jit.ignore\ndef predict(\n    self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a deterministic prediction.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        hidden (torch.Tensor, optional): the hidden state\n\n    Returns:\n        actions (torch.Tensor): sampled actions\n        hidden (torch.Tensor): the new hidden state\n    \"\"\"\n    x, new_hidden = self.ncp(obs, hidden)\n\n    mean, _ = torch.chunk(x, 2, dim=-1)\n\n    # Bound actions between [-1, 1]\n    actions_normalized = torch.tanh(mean)\n\n    # Scale back to env action space\n    actions = actions_normalized * self.action_scale + self.action_bias\n\n    return actions, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActorDiscrete","title":"<code>SACActorDiscrete</code>","text":"<p>               Bases: <code>LiquidNCPModule</code></p> <p>A Liquid NCP Actor Network for the SAC algorithm. Outputs a categorical distribution over actions.</p> <p>Usable with discrete action spaces.</p> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>class SACActorDiscrete(LiquidNCPModule):\n    \"\"\"\n    A Liquid NCP Actor Network for the SAC algorithm. Outputs a categorical\n    distribution over actions.\n\n    Usable with discrete action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_obs: int,\n        n_neurons: int,\n        num_actions: int,\n        *,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            num_obs (int): the number of input observations\n            n_neurons (int): the number of hidden neurons\n            num_actions (int): the number of actions\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__(num_obs, n_neurons, num_actions, device=device)\n\n        self.num_actions = num_actions\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    @torch.jit.ignore\n    def get_sample(self, probs: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Computes a set of action samples and log probabilities using a\n        Categorical distribution.\n\n        Parameters:\n            probs (torch.Tensor): Softmax probabilities for each action\n\n        Returns:\n            actions (torch.Tensor): action samples.\n            log_probs (torch.Tensor): action log probabilities.\n        \"\"\"\n        dist = Categorical(probs=probs)\n\n        actions = dist.sample()\n        log_probs = dist.log_prob(actions).unsqueeze(-1)\n\n        return actions, log_probs\n\n    @torch.jit.ignore\n    def predict(\n        self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a deterministic prediction.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            hidden (torch.Tensor, optional): the hidden state\n\n        Returns:\n            actions (torch.Tensor): sampled actions\n            hidden (torch.Tensor): the new hidden state\n        \"\"\"\n        logits, new_hidden = self.ncp(obs, hidden)\n        x = self.softmax(logits)\n        actions = torch.argmax(x, dim=-1)\n\n        return actions, new_hidden\n\n    def forward(\n        self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            hidden (torch.Tensor, optional): the hidden state\n\n        Returns:\n            actions (torch.Tensor): the action predictions.\n            probs (torch.Tensor): softmax probabilities for each action.\n            log_prob (torch.Tensor): log probabilities of actions.\n            hidden (torch.Tensor): the new hidden state.\n        \"\"\"\n        logits, new_hidden = self.ncp(obs, hidden)\n        probs = self.softmax(logits)\n\n        actions, log_prob = self.get_sample(probs)\n        return actions, probs, log_prob, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActorDiscrete.__init__","title":"<code>__init__(num_obs, n_neurons, num_actions, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_obs</code> <code>int</code> <p>the number of input observations</p> required <code>n_neurons</code> <code>int</code> <p>the number of hidden neurons</p> required <code>num_actions</code> <code>int</code> <p>the number of actions</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>def __init__(\n    self,\n    num_obs: int,\n    n_neurons: int,\n    num_actions: int,\n    *,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        num_obs (int): the number of input observations\n        n_neurons (int): the number of hidden neurons\n        num_actions (int): the number of actions\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__(num_obs, n_neurons, num_actions, device=device)\n\n    self.num_actions = num_actions\n\n    self.softmax = nn.Softmax(dim=-1)\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActorDiscrete.forward","title":"<code>forward(obs, hidden=None)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>the action predictions.</p> <code>probs</code> <code>torch.Tensor</code> <p>softmax probabilities for each action.</p> <code>log_prob</code> <code>torch.Tensor</code> <p>log probabilities of actions.</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state.</p> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>def forward(\n    self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        hidden (torch.Tensor, optional): the hidden state\n\n    Returns:\n        actions (torch.Tensor): the action predictions.\n        probs (torch.Tensor): softmax probabilities for each action.\n        log_prob (torch.Tensor): log probabilities of actions.\n        hidden (torch.Tensor): the new hidden state.\n    \"\"\"\n    logits, new_hidden = self.ncp(obs, hidden)\n    probs = self.softmax(logits)\n\n    actions, log_prob = self.get_sample(probs)\n    return actions, probs, log_prob, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActorDiscrete.get_sample","title":"<code>get_sample(probs)</code>","text":"<p>Computes a set of action samples and log probabilities using a Categorical distribution.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>torch.Tensor</code> <p>Softmax probabilities for each action</p> required <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>action samples.</p> <code>log_probs</code> <code>torch.Tensor</code> <p>action log probabilities.</p> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>@torch.jit.ignore\ndef get_sample(self, probs: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Computes a set of action samples and log probabilities using a\n    Categorical distribution.\n\n    Parameters:\n        probs (torch.Tensor): Softmax probabilities for each action\n\n    Returns:\n        actions (torch.Tensor): action samples.\n        log_probs (torch.Tensor): action log probabilities.\n    \"\"\"\n    dist = Categorical(probs=probs)\n\n    actions = dist.sample()\n    log_probs = dist.log_prob(actions).unsqueeze(-1)\n\n    return actions, log_probs\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACActorDiscrete.predict","title":"<code>predict(obs, hidden=None)</code>","text":"<p>Performs a deterministic prediction.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>actions</code> <code>torch.Tensor</code> <p>sampled actions</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state</p> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>@torch.jit.ignore\ndef predict(\n    self, obs: torch.Tensor, hidden: torch.Tensor | None = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a deterministic prediction.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        hidden (torch.Tensor, optional): the hidden state\n\n    Returns:\n        actions (torch.Tensor): sampled actions\n        hidden (torch.Tensor): the new hidden state\n    \"\"\"\n    logits, new_hidden = self.ncp(obs, hidden)\n    x = self.softmax(logits)\n    actions = torch.argmax(x, dim=-1)\n\n    return actions, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCritic","title":"<code>SACCritic</code>","text":"<p>               Bases: <code>LiquidNCPModule</code></p> <p>A Liquid NCP Critic Network for the SAC algorithm. Estimates Q-values given states and actions.</p> <p>Usable with continuous action spaces.</p> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>class SACCritic(LiquidNCPModule):\n    \"\"\"\n    A Liquid NCP Critic Network for the SAC algorithm. Estimates Q-values given\n    states and actions.\n\n    Usable with continuous action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_obs: int,\n        n_neurons: int,\n        num_actions: int,\n        *,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            num_obs (int): the number of input observations\n            n_neurons (int): the number of hidden neurons\n            num_actions (int): the number of actions\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__(num_obs + num_actions, n_neurons, 1, device=device)\n\n    def forward(\n        self,\n        obs: torch.Tensor,\n        actions: torch.Tensor,\n        hidden: torch.Tensor | None = None,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            actions (torch.Tensor): the batch of actions\n            hidden (torch.Tensor, optional): the hidden state\n\n        Returns:\n            q_values (torch.Tensor): the Q-Value predictions.\n            hidden (torch.Tensor): the new hidden state.\n        \"\"\"\n        inputs = torch.cat([obs, actions], dim=-1)\n        q_values, new_hidden = self.ncp(inputs, hidden)\n        return q_values, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCritic.__init__","title":"<code>__init__(num_obs, n_neurons, num_actions, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_obs</code> <code>int</code> <p>the number of input observations</p> required <code>n_neurons</code> <code>int</code> <p>the number of hidden neurons</p> required <code>num_actions</code> <code>int</code> <p>the number of actions</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>def __init__(\n    self,\n    num_obs: int,\n    n_neurons: int,\n    num_actions: int,\n    *,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        num_obs (int): the number of input observations\n        n_neurons (int): the number of hidden neurons\n        num_actions (int): the number of actions\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__(num_obs + num_actions, n_neurons, 1, device=device)\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCritic.forward","title":"<code>forward(obs, actions, hidden=None)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>actions</code> <code>torch.Tensor</code> <p>the batch of actions</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>q_values</code> <code>torch.Tensor</code> <p>the Q-Value predictions.</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state.</p> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>def forward(\n    self,\n    obs: torch.Tensor,\n    actions: torch.Tensor,\n    hidden: torch.Tensor | None = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        actions (torch.Tensor): the batch of actions\n        hidden (torch.Tensor, optional): the hidden state\n\n    Returns:\n        q_values (torch.Tensor): the Q-Value predictions.\n        hidden (torch.Tensor): the new hidden state.\n    \"\"\"\n    inputs = torch.cat([obs, actions], dim=-1)\n    q_values, new_hidden = self.ncp(inputs, hidden)\n    return q_values, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCriticDiscrete","title":"<code>SACCriticDiscrete</code>","text":"<p>               Bases: <code>LiquidNCPModule</code></p> <p>A Liquid NCP Critic Network for the SAC algorithm. Estimates Q-values given states and actions.</p> <p>Usable with discrete action spaces.</p> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>class SACCriticDiscrete(LiquidNCPModule):\n    \"\"\"\n    A Liquid NCP Critic Network for the SAC algorithm. Estimates Q-values given\n    states and actions.\n\n    Usable with discrete action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_obs: int,\n        n_neurons: int,\n        num_actions: int,\n        *,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            num_obs (int): the number of input observations\n            n_neurons (int): the number of hidden neurons\n            num_actions (int): the number of actions\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__(num_obs, n_neurons, num_actions, device=device)\n\n    def forward(\n        self,\n        obs: torch.Tensor,\n        hidden: torch.Tensor | None = None,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            hidden (torch.Tensor, optional): the hidden state\n\n        Returns:\n            q_values (torch.Tensor): the Q-Value predictions.\n            hidden (torch.Tensor): the new hidden state.\n        \"\"\"\n        q_values, new_hidden = self.ncp(obs, hidden)\n        return q_values, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCriticDiscrete.__init__","title":"<code>__init__(num_obs, n_neurons, num_actions, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_obs</code> <code>int</code> <p>the number of input observations</p> required <code>n_neurons</code> <code>int</code> <p>the number of hidden neurons</p> required <code>num_actions</code> <code>int</code> <p>the number of actions</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>def __init__(\n    self,\n    num_obs: int,\n    n_neurons: int,\n    num_actions: int,\n    *,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        num_obs (int): the number of input observations\n        n_neurons (int): the number of hidden neurons\n        num_actions (int): the number of actions\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__(num_obs, n_neurons, num_actions, device=device)\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCriticDiscrete.forward","title":"<code>forward(obs, hidden=None)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>hidden</code> <code>torch.Tensor</code> <p>the hidden state</p> <code>None</code> <p>Returns:</p> Name Type Description <code>q_values</code> <code>torch.Tensor</code> <p>the Q-Value predictions.</p> <code>hidden</code> <code>torch.Tensor</code> <p>the new hidden state.</p> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>def forward(\n    self,\n    obs: torch.Tensor,\n    hidden: torch.Tensor | None = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        hidden (torch.Tensor, optional): the hidden state\n\n    Returns:\n        q_values (torch.Tensor): the Q-Value predictions.\n        hidden (torch.Tensor): the new hidden state.\n    \"\"\"\n    q_values, new_hidden = self.ncp(obs, hidden)\n    return q_values, new_hidden\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCriticNCP","title":"<code>SACCriticNCP</code>","text":"<p>               Bases: <code>NCPModule</code></p> <p>An NCP Critic Network for the SAC algorithm. Estimates Q-values given states and actions.</p> <p>Usable with continuous action spaces.</p> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>class SACCriticNCP(NCPModule):\n    \"\"\"\n    An NCP Critic Network for the SAC algorithm. Estimates Q-values given\n    states and actions.\n\n    Usable with continuous action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_obs: int,\n        n_neurons: int,\n        num_actions: int,\n        *,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            num_obs (int): the number of input observations\n            n_neurons (int): the number of hidden neurons\n            num_actions (int): the number of actions\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__(num_obs + num_actions, n_neurons, 1, device=device)\n\n    def forward(self, obs: torch.Tensor, actions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n            actions (torch.Tensor): the batch of actions\n\n        Returns:\n            q_values (torch.Tensor): the Q-Value predictions.\n        \"\"\"\n        inputs = torch.cat([obs, actions], dim=-1)\n        q_values = self.ncp(inputs)\n        return q_values\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCriticNCP.__init__","title":"<code>__init__(num_obs, n_neurons, num_actions, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_obs</code> <code>int</code> <p>the number of input observations</p> required <code>n_neurons</code> <code>int</code> <p>the number of hidden neurons</p> required <code>num_actions</code> <code>int</code> <p>the number of actions</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>def __init__(\n    self,\n    num_obs: int,\n    n_neurons: int,\n    num_actions: int,\n    *,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        num_obs (int): the number of input observations\n        n_neurons (int): the number of hidden neurons\n        num_actions (int): the number of actions\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__(num_obs + num_actions, n_neurons, 1, device=device)\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCriticNCP.forward","title":"<code>forward(obs, actions)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <code>actions</code> <code>torch.Tensor</code> <p>the batch of actions</p> required <p>Returns:</p> Name Type Description <code>q_values</code> <code>torch.Tensor</code> <p>the Q-Value predictions.</p> Source code in <code>velora/models/sac/continuous.py</code> Python<pre><code>def forward(self, obs: torch.Tensor, actions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n        actions (torch.Tensor): the batch of actions\n\n    Returns:\n        q_values (torch.Tensor): the Q-Value predictions.\n    \"\"\"\n    inputs = torch.cat([obs, actions], dim=-1)\n    q_values = self.ncp(inputs)\n    return q_values\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCriticNCPDiscrete","title":"<code>SACCriticNCPDiscrete</code>","text":"<p>               Bases: <code>NCPModule</code></p> <p>An NCP Critic Network for the SAC algorithm. Estimates Q-values given states and actions.</p> <p>Usable with discrete action spaces.</p> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>class SACCriticNCPDiscrete(NCPModule):\n    \"\"\"\n    An NCP Critic Network for the SAC algorithm. Estimates Q-values given\n    states and actions.\n\n    Usable with discrete action spaces.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_obs: int,\n        n_neurons: int,\n        num_actions: int,\n        *,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Parameters:\n            num_obs (int): the number of input observations\n            n_neurons (int): the number of hidden neurons\n            num_actions (int): the number of actions\n            device (torch.device, optional): the device to perform computations on\n        \"\"\"\n        super().__init__(num_obs, n_neurons, num_actions, device=device)\n\n    def forward(self, obs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the network.\n\n        Parameters:\n            obs (torch.Tensor): the batch of state observations\n\n        Returns:\n            q_values (torch.Tensor): the Q-Value predictions.\n        \"\"\"\n        q_values = self.ncp(obs)\n        return q_values\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCriticNCPDiscrete.__init__","title":"<code>__init__(num_obs, n_neurons, num_actions, *, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_obs</code> <code>int</code> <p>the number of input observations</p> required <code>n_neurons</code> <code>int</code> <p>the number of hidden neurons</p> required <code>num_actions</code> <code>int</code> <p>the number of actions</p> required <code>device</code> <code>torch.device</code> <p>the device to perform computations on</p> <code>None</code> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>def __init__(\n    self,\n    num_obs: int,\n    n_neurons: int,\n    num_actions: int,\n    *,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters:\n        num_obs (int): the number of input observations\n        n_neurons (int): the number of hidden neurons\n        num_actions (int): the number of actions\n        device (torch.device, optional): the device to perform computations on\n    \"\"\"\n    super().__init__(num_obs, n_neurons, num_actions, device=device)\n</code></pre>"},{"location":"learn/reference/models/sac/#velora.models.sac.SACCriticNCPDiscrete.forward","title":"<code>forward(obs)</code>","text":"<p>Performs a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>torch.Tensor</code> <p>the batch of state observations</p> required <p>Returns:</p> Name Type Description <code>q_values</code> <code>torch.Tensor</code> <p>the Q-Value predictions.</p> Source code in <code>velora/models/sac/discrete.py</code> Python<pre><code>def forward(self, obs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the network.\n\n    Parameters:\n        obs (torch.Tensor): the batch of state observations\n\n    Returns:\n        q_values (torch.Tensor): the Q-Value predictions.\n    \"\"\"\n    q_values = self.ncp(obs)\n    return q_values\n</code></pre>"},{"location":"learn/theory/","title":"Theory","text":"<p>This chapter focuses on the theory of the algorithms used throughout the framework.</p> <p>Each section provides details about a different algorithm, explaining how they work from a mathematical and visual perspective using diagrams.</p> <p>We assume you already have a basic understand on the RL basics so that we can focus on the relevant aspects (this may change in the future).</p> In a Hurry? <p>Looking to jump to something specific? Use the navigation menu on the left \ud83d\udc48!</p> <p>When you're ready, click on the button below to read the theory for our main building block - Liquid Neural Networks! \ud83d\ude80</p>"},{"location":"learn/theory/lnn/","title":"Liquid Neural Networks (LNN)","text":"<p>Coming soon</p> <p>Is page is a WIP and will be based on the following Medium [] article.</p> <p>For those interested in learning more about LNNs, we highly recommend you check it out!</p>"},{"location":"learn/tutorial/","title":"User Guide - Tutorials","text":"<p>This tutorial series shows you how to use <code>velora</code> with most of its features, step by step.</p> <p>Each section shows a different agent algorithm or set of methods that you can work with.</p> In a Hurry? <p>Looking to jump to something specific? Use the navigation menu on the left \ud83d\udc48!</p> <p>When you're ready, click on the button below to start your first steps! \ud83e\udd29</p>"},{"location":"learn/tutorial/agent/","title":"Agent Basics","text":"<p>RL agents are at the heart of Velora's framework and are the fastest way to get started with experiments.</p> <p>Each agent has subtle differences but we've designed them to act like drop-in replacements of each other. Their underlying functionality may differ, but their core API is identical with the exception of optional hyperparameters.</p> <p>At it's core, you have three main operations:</p> <ul> <li>Initialize - creation of the model.</li> <li>Training - running the model on an environment.</li> <li>Prediction - making predictions on unseen data.</li> </ul>"},{"location":"learn/tutorial/agent/#creating-agents","title":"Creating Agents","text":"<p>Creating a agent is really easy! We simply declare our agent from the <code>velora.models</code> API and create a class instance.</p> <p>Each model requires three main parameters:</p> <ol> <li><code>env_id</code> - the Gymnasium environment ID. E.g., <code>CartPole-v1</code> or <code>InvertedPendulum-v5</code>.</li> <li><code>actor_neurons</code> - the number of decision/hidden nodes for the Actor network (e.g., <code>20</code> or <code>40</code>).</li> <li><code>critic_neurons</code> - the number of decision/hidden nodes for the Critic networks. We recommend this to be higher (<code>128</code> or <code>256</code>) than the Actor network.</li> </ol> <p>And that's it! Here's an example:</p> Python<pre><code>from velora.models import NeuroFlowCT\n\nmodel = NeuroFlowCT(\"InvertedPendulum-v5\", 20, 128)\n</code></pre> <p>This code should work 'as is'.</p> <p>Want to use a different agent? Just swap out <code>NeuroFlowCT</code> with a different one!</p> Python<pre><code>from velora.models import NeuroFlow\n\nmodel = NeuroFlow(\"CartPole-v1\", 20, 128)\n</code></pre> <p>It really is that easy! \ud83e\udd29</p> Agent Parameters <p>Each agent comes with a set of optional parameters that can be customized. </p> <p>You can read more about them in the <code>Agents</code> documentation section.</p>"},{"location":"learn/tutorial/agent/#training-an-agent","title":"Training an Agent","text":"<p>Training an agent is just as easy!</p> <p>We use the <code>train</code> method, supply it with a <code>batch_size</code> and boom \ud83d\udca5, your agent will start training for <code>1000</code> episodes:</p> Python<pre><code>from velora.models import NeuroFlowCT\n\nmodel = NeuroFlowCT(\"InvertedPendulum-v5\", 20, 128)\nmodel.train(128)\n</code></pre> <p>This code should work 'as is'.</p> <p>Want to change the number of episodes? Use the <code>n_episodes</code> parameter! What about the console logged training status frequency? Use the <code>display_count</code> parameter!</p> Python<pre><code>model.train(env, 128, n_episodes=10_000, display_count=10)\n</code></pre> <p>These are only two of the optional parameters for the <code>train()</code> method. Another worth mentioning is <code>callbacks</code> but we'll talk about them later.</p> <p>Like before, need a different agent? Just swap it out!</p> Python<pre><code>from velora.models import NeuroFlow\n\nmodel = NeuroFlow(\"CartPole-v1\", 20, 128)\nmodel.train(64)\n</code></pre> <p>The <code>train()</code> method will create a SQLite [] database called <code>metrics.db</code> in your local directory. This contains useful metrics that can be plotted to visualize the whole training process. How you use them is up to you!</p> <p>We personally use and recommend a cloud-based solution (see the Analytics Callbacks section) which uses these metrics automatically.</p> <p>However, we've included this offline method separately just in case you prefer it! \ud83d\ude09 We'll talk more about these metrics later in the Training Metrics section.</p>"},{"location":"learn/tutorial/agent/#making-predictions","title":"Making Predictions","text":"<p>For new predictions, we use the <code>predict()</code> method. This requires two parameters:</p> <ul> <li><code>state</code> - the item to make a prediction on. Must be a <code>torch.Tensor</code>.</li> <li><code>hidden</code> - the agent's hidden state.</li> </ul> <p>Liquid Neural Networks are a recurrent architecture so a hidden state is required!</p> <p>By default, we set <code>hidden</code> to <code>None</code>, so you don't need to provide it for a single prediction:</p> Python<pre><code>from velora.models import NeuroFlowCT\n\n# Set prediction environment\nenv = model.eval_env\n\nstate, _ = env.reset()\n\nmodel = NeuroFlowCT(\"InvertedPendulum-v5\", 20, 128)\naction, hidden = model.predict(state)\n</code></pre> <p>This code should work 'as is'.</p> <p>Here, we get back an <code>action</code> prediction and an updated <code>hidden</code> state.</p> <p>Things become slightly more complicated with multiple predictions because we need to feed the <code>hidden</code> state back into the <code>predict()</code> method like so:</p> Python<pre><code>from velora.models import NeuroFlowCT\n\nmodel = NeuroFlowCT(\"InvertedPendulum-v5\", 20, 128)\n\n# Set prediction environment\nenv = model.eval_env\n\nhidden = None\nstate, _ = env.reset()\n\nfor ep in range(1000):\n    action, hidden = model.predict(state, hidden)\n    state, reward, terminated, truncated, _ = env.step(action)\n\n    done = terminated or truncated\n\n    if done:\n        break\n\nenv.close()\n</code></pre> <p>This code should work 'as is'.</p> <p>Just like before, if you want to use a different agent, just swap out <code>NeuroFlowCT</code> with another one. Glorious isn't it? \ud83d\ude09</p> <p>That covers the basics! Next, we'll move onto <code>callbacks</code>. See you there! \ud83d\udc4b</p>"},{"location":"learn/tutorial/callback/","title":"Using Callbacks","text":"API Docs <p><code>velora.callbacks</code></p> <p>The normal process for training an agent is extremely limited. There is no ability to stop at a reward threshold or save a model's state.</p> <p>Let's be honest, do you really want to sit through 100k episodes and then manually have to save your model, even though it solved the environment at say 10k episodes? I know I don't! \ud83d\ude05</p> <p>Callbacks are a flexible and powerful way to change that.</p> <p>When calling the <code>train()</code> method you can use the <code>callbacks</code> parameter to extend the functionality of the training process.</p>"},{"location":"learn/tutorial/callback/#basic-usage","title":"Basic Usage","text":"<p>For example, let's say we want to stop our agent when it achieves an average reward of <code>15</code>.</p> <p>To do this, we use the <code>EarlyStopping</code> callback:</p> Python<pre><code>from velora.callbacks import EarlyStopping\nfrom velora.models import NeuroFlow\n\nmodel = NeuroFlow(\"InvertedPendulum-v5\", 20, 128)\n\ncallbacks = [\n    EarlyStopping(target=15.0, patience=3),\n]\n\nmodel.train(128, callbacks=callbacks)\n</code></pre> <p>This code should work 'as is'.</p> <p>Now, the model will automatically terminate when it reaches the reward target! It's as simple as that! \ud83d\ude0a</p>"},{"location":"learn/tutorial/callback/#callback-list","title":"Callback List","text":"Combining Callbacks <p>Callbacks alone are extremely powerful for enhancing your training process, but it becomes even more ridiculous when you stack them together! E.g.,</p> Python<pre><code>from velora.callbacks import EarlyStopping, SaveCheckpoints\n\n# ...\n\ncallbacks = [\n    EarlyStopping(15.0),\n    SaveCheckpoints(\"agent\"),\n]\n\nmodel.train(..., callbacks=callbacks)\n</code></pre> <p>We highly recommend you experiment with different <code>callbacks</code> yourself and find what ones work best for you.</p> <p>The possibilities are truly endless! \ud83d\ude0e</p> <p>There are a number of <code>callbacks</code> available. Here's an exhaustive list of them:</p> <ul> <li><code>EarlyStopping</code> - stops the training process when the average reward <code>target</code> is reached multiple times in a row based on the <code>patience</code> value.</li> <li><code>SaveCheckpoints</code> - saves the model state during the training process based on a <code>frequency</code>.</li> <li><code>RecordVideos</code> - adds video recording to the agents training process.</li> <li><code>CometAnalytics</code> - adds Comet [] experiment cloud-based tracking.</li> </ul>"},{"location":"learn/tutorial/callback/#early-stopping","title":"Early Stopping","text":"API Docs <p><code>velora.callbacks.EarlyStopping(target, patience)</code></p> <p><code>EarlyStopping</code> terminates the training process when the episodic reward <code>target</code> is reached multiple times in a row based on the <code>patience</code> value.</p> Python<pre><code>from velora.callbacks import EarlyStopping\n\ncallbacks = [\n    EarlyStopping(target=15.0, patience=3),\n]\n</code></pre> <p>By default, the <code>patience=3</code> and is the only optional parameter available.</p>"},{"location":"learn/tutorial/callback/#model-checkpoints","title":"Model Checkpoints","text":"API Docs <p><code>velora.callbacks.SaveCheckpoints(dirname)</code></p> <p>Sometimes it can be really useful to save the model state intermittently during the training process, especially when you are also using <code>EarlyStopping</code>.</p> <p>We can do this with the <code>SaveCheckpoints</code> callback. It requires one parameter:</p> <ul> <li><code>dirname</code> - the directory name to store the model checkpoints in the <code>checkpoints</code> directory.</li> </ul> <p>Checkpoints are automatically added to a <code>checkpoints</code> directory inside a <code>&lt;dirname&gt;/saves</code> folder. This design choice compliments the <code>RecordVideos</code> callback to help keep the experiments tidy.</p> <p>For example, if we want to train a <code>NeuroFlow</code> model and store its checkpoints in a model directory called <code>nf</code> we'd use the following code:</p> Python<pre><code>from velora.callbacks import SaveCheckpoints\n\ncallbacks = [\n    SaveCheckpoints(\"nf\"),\n]\n</code></pre> <p>Notice how we don't allow you to set a <code>prefixed</code> name for checkpoints. It's set automatically with the environment name and episode count, such as:</p> <ul> <li><code>InvertedPendulum_100/</code></li> <li><code>InvertedPendulum_final/</code></li> </ul> <p>We limit your control to the directory name to simplify the checkpoint process and to keep them organised.</p> Why only the <code>dirname</code>? <p>Under the hood, we use the <code>agent.save()</code> method for storing checkpoints (more on this later). It stores a variety of state files and two additional ones in the <code>saves</code> folder - <code>model_config.json</code> a file containing config details about the agent, and a <code>completed.json</code> file after training terminates with final stats and duration (<code>episodes</code>, <code>steps</code> and <code>time taken</code>).</p> <p>That way, you don't need any complex <code>dirnames</code>! \ud83d\ude09</p> <p>Only setting the required parameters will create an instance of the callback with the following default <code>optional</code> parameters:</p> <ul> <li><code>frequency=100</code> - the <code>episode</code> save frequency.</li> <li><code>buffer=False</code> - whether to save the buffer state.</li> </ul> <p>You can customize these freely using the required parameter name.</p> <p>When <code>buffer=True</code> the checkpoint state's <code>buffer</code> is saved at that episode. We'll discuss more about this in the Saving and Loading Models section.</p>"},{"location":"learn/tutorial/callback/#recording-videos","title":"Recording Videos","text":"API Docs <p><code>velora.callbacks.RecordVideos(dirname)</code></p> <p>Sometimes it's useful to see how the agent is performing while it is training. The best way to do this is visually, by watching the agent interact with its environment.</p> <p>To do this, we use the <code>RecordVideos</code> callback. Under-the-hood, we apply Gymnasium's RecordVideo [] wrapper to the environment with a minor expansion - it always records the final training episode.</p> <p>It has one required parameter:</p> <ul> <li><code>dirname</code> - the model directory name to store the videos. E.g., <code>nf</code>.</li> </ul> <p>Videos are automatically added to a <code>checkpoints</code> directory inside a <code>&lt;dirname&gt;/videos</code> folder. This design choice compliments the <code>SaveCheckpoints</code> callback to help keep the experiments tidy.</p> Python<pre><code>from velora.callbacks import EarlyStopping, SaveCheckpoints, RecordVideos\n\n# Solo\ncallbacks = [\n    RecordVideos(\"nf\"),\n]\n\n# With other callbacks\nCP_DIR = \"nf\"\nFREQ = 5\n\ncallbacks = [\n    SaveCheckpoints(CP_DIR, frequency=FREQ, buffer=True),\n    EarlyStopping(target=15., patience=10),\n    RecordVideos(CP_DIR, frequency=FREQ),\n]\n</code></pre> <p>This code should work 'as is'.</p> <p>For more control, you can also set any of the optional parameters:</p> <ul> <li><code>method=episode</code> - the recording method. Either: <code>episode</code> or <code>step</code>.</li> <li><code>frequency=100</code> - the record frequency for <code>method</code>.</li> </ul>"},{"location":"learn/tutorial/callback/#analytics","title":"Analytics","text":"<p>Experiment tracking is extremely important for understanding how an agent is performing. We offer two variants of this: <code>offline</code> and <code>online</code> (cloud-based).</p> <p>Our offline approach works out of the box with every Velora agent. We'll talk about this more in the Training Metrics section.</p> <p>However, online (cloud-based) tracking is optional. To integrate it we use callbacks! \ud83d\ude0a</p>"},{"location":"learn/tutorial/callback/#comet-analytics","title":"Comet Analytics","text":"API Docs <p><code>velora.callbacks.CometAnalytics(project_name)</code></p> <p>We've found Comet [] to be one of the best tools for RL experiments. It has a clean interface, an elegant category system for experiment details, and integrates well with video recordings.</p> <p>To use it, we need 3 things -</p> <ol> <li> <p>The required dependencies:</p> <pre><code>pip install velora[comet]\n</code></pre> </li> <li> <p>A <code>COMET_API_KEY</code> (found in your Comet account settings API Key Docs []):</p> <p>You can either configure this using an <code>.env</code> file or setting it manually in the terminal -</p> <code>.env</code> fileLinux/macOSWindows (CMD)Powershell <pre><code>COMET_API_KEY=\n</code></pre> <pre><code>export COMET_API_KEY=\n</code></pre> <pre><code>set COMET_API_KEY=\n</code></pre> <pre><code>$env:COMET_API_KEY=\"&lt;insert_here&gt;\"\n</code></pre> </li> <li> <p>The dedicated callback - <code>CometAnalytics</code>:</p> Python<pre><code>from velora.callbacks import CometAnalytics\n\ncallbacks = [\n    # other callbacks\n    CometAnalytics(\"nf\"),\n]\n</code></pre> </li> </ol> <p>The callback has one required parameter:</p> <ul> <li><code>project_name</code> - the name of the Comet ML project to add the experiment to.</li> </ul> <p>And three optional parameters:</p> <ul> <li> <p><code>experiment_name</code> - the name of the experiment. If <code>None</code>, automatically creates the name using the format: <code>&lt;agent_classname&gt;_&lt;env_name&gt;_&lt;n_episodes&gt;ep</code>.</p> <p>E.g., <code>NeuroFlow_InvertedPendulum_1000ep</code>.</p> </li> <li> <p><code>tags</code> - a list of tags associated with experiment. If <code>None</code>, sets tags automatically as: <code>[agent_classname, env_name]</code>.</p> </li> </ul> Note <p>We've limited the customization to keep things simple. By default, the experiment will be tied to your account associated to the <code>COMET_API_KEY</code>.</p> <p>You shouldn't have to tweak a million settings just to start tracking experiments! \ud83d\ude80</p> <p>We primarily focus on sending episodic metrics to Comet that provide a detailed overview of the training process. These include:</p> <ul> <li><code>episode/return</code> - the raw episodic reward (return).</li> <li><code>episode/length</code> - the number of steps completed in the episode.</li> <li><code>reward/moving_avg</code> - the episodic reward moving average based on the training <code>window_size</code>.</li> <li><code>reward/moving_upper</code> - the episodic reward moving upper bound (<code>moving_avg + moving_std</code>) based on the training <code>window_size</code>.</li> <li><code>reward/moving_lower</code> - the episodic reward moving lower bound (<code>moving_avg - moving_std</code>) based on the training <code>window_size</code>.</li> <li><code>losses/actor_loss</code> - the average Actor loss for each episode.</li> <li><code>losses/critic_loss</code> - the average Critic loss for each episode.</li> <li><code>losses/entropy_loss</code> - the average Entropy loss for each episode.</li> </ul> <p>Next, we'll look at how to <code>save</code> and <code>load</code> models. See you there! \ud83d\udc4b</p>"},{"location":"learn/tutorial/gym/","title":"Gymnasium Utility Methods","text":"<p>Velora uses the Gymnasium [] package as it's main environment provider, which is used to train agents and view their performance.</p> <p>Normally, you would use the <code>make()</code> method to create an environment, like so:</p> Python<pre><code>import gymnasium as gym\n\nenv = gym.make(\"InvertedPendulum-v5\")\n</code></pre> <p>We've removed the need to do this to simplify the implementation and to allow us to easily integrate the environment with our agents.</p> <p>Instead, now we pass in the <code>env_id</code> to the agent of our choice:</p> Python<pre><code>from velora.models import NeuroFlow\n\nmodel = NeuroFlow(\"InvertedPendulum-v5\", 20, 128)\n</code></pre> <p>Unfortunately, this takes away a lot of your freedom for adding custom wrappers and we are looking at ways to incorporate this in a future release. For now, we want to keep things simple while we fully flesh out the agents implementation details.</p>"},{"location":"learn/tutorial/gym/#utility-methods","title":"Utility Methods","text":"<p>Under rare circumstances you might want to use a Gymnasium environment to quickly explore it before using a Velora agent.</p> <p>To help with this, we've added some utility methods that you might find useful.</p>"},{"location":"learn/tutorial/gym/#wrapping-gymnasium-environments","title":"Wrapping Gymnasium Environments","text":"API Docs <p><code>velora.gym.wrap_gym_env(env, wrappers)</code></p> <p><code>wrap_gym_env</code> is a quick way to create new environments with wrappers automatically applied. Normally, you'd have to apply wrappers, one by one like this:</p> Python<pre><code>import gymnasium as gym\n\nfrom gymnasium.wrappers import NormalizeObservation, NormalizeReward\n\nenv = gym.make(\"InvertedPendulum-v5\")\nenv = NormalizeObservation(env, epsilon=1e-8)\nenv = NormalizeReward(env, gamma=0.99, epsilon=1e-8)\n</code></pre> <p>It's pretty tedious, so we've simplified it a little with the <code>wrap_gym_env</code> method:</p> Python<pre><code>from functools import partial\n\nfrom gymnasium.wrappers import (\n    NormalizeObservation, \n    NormalizeReward, \n    RecordEpisodeStatistics,\n)\n\nfrom velora.gym import wrap_gym_env\n\nenv = wrap_gym_env(\"InvertedPendulum-v5\", [\n    partial(NormalizeObservation, epsilon=1e-8),\n    partial(NormalizeReward, gamma=0.99, epsilon=1e-8),\n    RecordEpisodeStatistics,\n])\n</code></pre> <p>This code should work 'as is'.</p> <p>Now, you just supply the environment <code>name</code> and a of <code>List[gym.Wrappers]</code> or <code>List[partial]</code> wrappers and your environment is good to go! \ud83d\ude0e</p>"},{"location":"learn/tutorial/gym/#core-wrappers","title":"Core Wrappers","text":"API Docs <p><code>velora.gym.add_core_env_wrappers(env, device)</code></p> <p>We've also added a <code>add_core_env_wrappers</code> method that applies specific wrappers required by every Velora agent.</p> <p>It applies the following wrappers (in order):</p> <ul> <li>RecordEpisodeStatistics [] - for easily retrieving episode statistics.</li> <li>NumpyToTorch [] - for turning environment feedback into <code>PyTorch</code> tensors.</li> </ul> <p>Here's an example:</p> Python<pre><code>from functools import partial\n\nimport gymnasium as gym\nfrom gymnasium.wrappers import NormalizeObservation, NormalizeReward\n\nfrom velora.gym import wrap_gym_env, add_core_env_wrappers\n\nenv = wrap_gym_env(\"InvertedPendulum-v5\", [\n    partial(NormalizeObservation, epsilon=1e-8),\n    partial(NormalizeReward, gamma=0.99, epsilon=1e-8),\n])\nenv = add_core_env_wrappers(env, device=\"cpu\")\n\n# Or ..\nenv = gym.make(\"InvertedPendulum-v5\")\nenv = add_core_env_wrappers(env, device=\"cpu\")\n</code></pre> <p>In previous versions, you would manually need to use this yourself when working with the <code>predict()</code> method to quickly convert the environment from <code>numpy</code> arrays to <code>torch</code> tensors.</p> <p>We found this to be very tedious and quite confusing in some instances, so instead, we've simplified this process by adding a <code>.eval_env</code> attribute to every agent to remove this process:</p> Python<pre><code>from velora.models import NeuroFlowCT\n\nmodel = NeuroFlowCT(\"InvertedPendulum-v5\", 20, 128)\nenv = model.eval_env\n\n# \ud83d\udc46 Equivalent to ..\nimport gymnasium as gym\n\nenv = gym.make(\"InvertedPendulum-v5\")\nenv = add_core_env_wrappers(env, device=\"cpu\")\n</code></pre> <p>You can see an example of this in the Agent Basics - Making Predictions section.</p>"},{"location":"learn/tutorial/gym/#finding-environments","title":"Finding Environments","text":"API Docs <p><code>velora.gym.EnvSearch</code></p> <p><code>velora.gym.EnvResult</code></p> <p>We've also added a unique approach to quickly finding a specific environment at it's latest version without having to search through the Gymnasium [] documentation.</p> <p>You can do this with a utility class called <code>EnvSearch</code>.</p> <p>Every method attached to the class returns a <code>List[EnvResult]</code> where <code>EnvResult</code> are objects that have a <code>name</code> and <code>type</code> to help you quickly determine which environment fits your use case.</p> <p>There are three search methods split into two categories:</p> <ul> <li>Searching for a specific environment</li> <li>Getting a list of available environments by type</li> </ul>"},{"location":"learn/tutorial/gym/#specific-one","title":"Specific One","text":"API Docs <p><code>velora.gym.EnvSearch.find(query)</code></p> <p>Let's say you want to quickly find the latest <code>LunarLander</code> environment, specifically the <code>continuous</code> one.</p> <p>You can use the <code>find()</code> method:</p> Python<pre><code>import gymnasium as gym\n\nfrom velora.gym import EnvSearch\n\n\nresult = EnvSearch.find('LunarLander')\n# [\n#   EnvResult(name='LunarLander-v3', type='discrete'), \n#   EnvResult(name='LunarLanderContinuous-v3', type='continuous')\n# ]\n\nresult2 = EnvSearch.find('Pendulum')\n# [\n#    EnvResult(name='Pendulum-v1', type='continuous'),\n#    EnvResult(name='InvertedPendulum-v5', type='continuous'),\n#    EnvResult(name='InvertedDoublePendulum-v5', type='continuous')\n# ]\n\n# Quick usage with the Gymnasium API\nname = result[-1].name # 'LunarLanderContinuous-v3'\nenv = gym.make(name)\n</code></pre> <p>This code should work 'as is'.</p> <p>Given a <code>query</code> string (part of the name or the full name), it will give you a list of relevant results.</p>"},{"location":"learn/tutorial/gym/#by-type","title":"By Type","text":"API Docs <p><code>velora.gym.EnvSearch.discrete()</code></p> <p><code>velora.gym.EnvSearch.continuous()</code></p> <p>Not sure what environments are available? Looking for your next <code>discrete</code> one or <code>continuous</code> one? We've got you covered! \ud83d\ude09</p> <p>Simply use the <code>discrete()</code> or <code>continuous()</code> methods to get a complete list of available options:</p> DiscreteContinuous Python<pre><code>from velora.gym import EnvSearch\n\nresults = EnvSearch.discrete()\n# [\n#   EnvResult(name='CartPole-v1', type='discrete'),\n#   # ...\n# ]\n</code></pre> Python<pre><code>from velora.gym import EnvSearch\n\nresults = EnvSearch.continuous()\n# [\n#   EnvResult(name='MountainCarContinuous-v0', type='continuous'),\n#   # ...\n# ]\n</code></pre> <p>This code should work 'as is'.</p> Warning <p>Environments are split into two categories <code>discrete</code> or <code>continuous</code> based on their <code>action_space</code>:</p> <ul> <li><code>discrete</code> requires a <code>gym.spaces.Discrete</code> space</li> <li><code>continuous</code> requires a <code>gym.spaces.Box</code> space</li> </ul> <p>Any other <code>action_space</code> type is ignored.</p> <p>Up next, we'll take a look at the generic utility methods Velora has to offer \ud83d\udc4b.</p>"},{"location":"learn/tutorial/metrics/","title":"Working with Training Metrics","text":"<p>Understanding how your agent is learning is extremely important for figuring out how to optimize its performance and also fix it when it's broken.</p> <p>To do this offline, we use a SQLite [] database for storing our metrics called <code>metrics.db</code>.</p>"},{"location":"learn/tutorial/metrics/#whats-inside-it","title":"What's Inside It?","text":"<p>We've split the database into two main tables:</p> <ul> <li><code>experiment</code> - for tracking basic information about your experiment.</li> <li><code>episode</code> - for storing episode metrics.</li> </ul>"},{"location":"learn/tutorial/metrics/#experiments","title":"Experiments","text":"API Docs <p><code>velora.metrics.Experiment</code></p> <p>The <code>experiment</code> table is the simplest and is primarily used to maintain an <code>id</code> for different experiments. It stores the following details:</p> <ul> <li><code>id</code> - a unique identifier for each experiment.</li> <li><code>agent</code> - the class name of the agent used.</li> <li><code>env</code> - the name of the environment used during training.</li> <li><code>config</code> - the agent's configuration details stored in a JSON format (the same one when saving a model!).</li> <li><code>created_at</code> - the time and date when the experiment was created.</li> </ul>"},{"location":"learn/tutorial/metrics/#episodes","title":"Episodes","text":"API Docs <p><code>velora.metrics.Episode</code></p> <p>The <code>episode</code> table stores the metrics for each training episode. It's the most comprehensive table of the three and stores the following details:</p> <ul> <li><code>id</code> - a unique identifier for the episode.</li> <li><code>experiment_id</code> - the experiment ID associated to the episode.</li> <li><code>episode_num</code> - the episode index.</li> <li><code>reward</code> - the episodic reward (return).</li> <li><code>length</code> - the number of timesteps performed to terminate the episode.</li> <li><code>reward_moving_avg</code> - the episodes reward moving average based on the training <code>window_size</code>.</li> <li><code>reward_moving_std</code> - the episodes reward moving standard deviation based on the training <code>window_size</code>.</li> <li><code>actor_loss</code> - the average Actor loss for the episode.</li> <li><code>critic_loss</code> - the average Critic loss for the episode.</li> <li><code>entropy_loss</code> - the average Entropy loss for the episode.</li> <li><code>created_at</code> - the date and time when the the entry was created.</li> </ul>"},{"location":"learn/tutorial/metrics/#exploring-the-database","title":"Exploring the Database","text":"<p>Once you've run a training instance with an agent's <code>train()</code> method, the database will automatically store the above metrics. You can then freely access them for your own analysis whenever you want! \ud83d\ude0a</p> <p>If you want to quickly explore the database, we recommend you use DB Browser []. It's GUI interface is extremely useful for quickly checking the stored data and running SQL queries.</p>"},{"location":"learn/tutorial/metrics/#interacting-with-it","title":"Interacting With It","text":"<p>To get data out of the database and use it in your projects we can use a helper method to quickly get the <code>engine</code> and then build a session.</p> <p>We use SQLModel [] under the hood, so we can interact with our database tables Pydantic [] model style! \ud83d\ude0d</p> <p>We can use a <code>Session</code> as a context manager (recommended):</p> Python<pre><code>from velora.metrics import get_db_engine, Episode\nfrom sqlmodel import Session, select\n\nengine = get_db_engine()\n\n# Create a new session\nwith Session(engine) as session:\n    # Get some data\n    statement = select(Episode).where(\n        Episode.experiment_id == 1\n    )\n    results = session.exec(statement)\n\n    # Return specific elements\n    for episode in results:\n        print(episode.reward, episode.length)\n</code></pre> <p>This code should work 'as is'.</p> <p>Or, as an instance:</p> Python<pre><code>from velora.metrics import get_db_engine, Episode\nfrom sqlmodel import Session, select\n\nengine = get_db_engine()\n\n# Create a new session\nsession = Session(engine)\n\n# Get some data\nstatement = select(Episode).where(\n    Episode.experiment_id == 1\n)\nresults = session.exec(statement)\n\n# Return specific elements\nfor episode in results:\n    print(episode.reward, episode.length)\n\n# Close the session\nsession.close()\n</code></pre> <p>This code should work 'as is'.</p> <p>We highly recommend you read the SQLModel [] documentation for more details.</p> <p>Next, we'll dive into the utility methods for <code>Gymnasium</code> \ud83d\udc4b.</p>"},{"location":"learn/tutorial/save/","title":"Saving &amp; Loading Models","text":"<p>Saving a trained model and loading it are extremely common and useful practices when performing multiple experiments.</p> <p>Both are really easy to do with our API and work identically for all agents.</p> <p>Simply, select an agent you want to train, save it with it's instance <code>save</code> method and then <code>load</code> it with the agent class method.</p> <p>For our example, we'll use <code>NeuroFlow</code>.</p>"},{"location":"learn/tutorial/save/#saving-a-model","title":"Saving a Model","text":"API Docs <p><code>velora.models.base.RLModuleAgent.save(dirpath)</code></p> <p>To save a model we use the model instance's <code>save</code> method:</p> Python<pre><code>from velora.models import NeuroFlow\nfrom velora.utils import set_device\n\ndevice = set_device()\n\nmodel = NeuroFlow(\"InvertedPendulum-v5\", 20, 128, device=device, seed=64)\nmodel.train(128, n_episodes=10, window_size=5)\n\nmodel.save('checkpoints/nf/saves/InvertedPendulum_10')\n</code></pre> <p>This code should work 'as is'.</p> <p>The only thing we need to do is give it a <code>dirpath</code> where the last folder contains all the models state files. These include:</p> <ul> <li><code>metadata.json</code> - contains the model and optimizer metadata.</li> <li><code>model_state.safetensors</code> - contains the model weights and biases.</li> <li><code>optim_state.safetensors</code> - contains the optimizer states (actor and critic).</li> </ul> <p>Optionally, we can also save the buffer state with <code>buffer=True</code>:</p> <ul> <li><code>buffer_state.safetensors</code> - contains the buffer state.</li> <li><code>metadata.json</code> - extended to include the <code>buffer</code> metadata.</li> </ul> Python<pre><code>model.save(..., buffer=True)\n</code></pre> <p>And/or, optionally, the model config with <code>config=True</code> (stored in the <code>dirpath.parent</code>):</p> <ul> <li><code>model_config.json</code> - contains the core details of the agent.</li> </ul> Python<pre><code>model.save(..., config=True)\n</code></pre> Why the parent directory? <p>The <code>model_config.json</code> contains comprehensive details about the agent (its <code>model.config</code>). It never changes. Therefore, it should only be saved once.</p> <p>Typically, you'll save a model state during the training process after <code>n_episodes</code> (just like we do with the SaveCheckpoints callback).</p> <p>The file is only used to provide an overview of the model so you can easily identify an experiment without having to manually load a model's state. So, we store it above the <code>target</code> directory with the assumption that you are saving more than once for the same experiment.</p> <p>Notice how we are using safetensors []. This helps us maximize tensor security and performance! \ud83d\ude09</p>"},{"location":"learn/tutorial/save/#loading-a-model","title":"Loading a Model","text":"API Docs <p><code>velora.models.base.RLModuleAgent.load(dirpath)</code></p> <p>To load a model we use the <code>load</code> class method:</p> Python<pre><code>from velora.models import NeuroFlow\n\nmodel = NeuroFlow.load('checkpoints/nf/saves/InvertedPendulum_10')\n</code></pre> <p>Like before, the only thing we need to do is give it a <code>dirpath</code>. The complete model state will then be loaded into a new model instance.</p> <p>Again, we don't load the buffers state by default. <code>buffer=True</code> will do that.</p> Python<pre><code>model = NeuroFlow.load(..., buffer=True)\n</code></pre> Buffer Loading <p>Buffer's can only be loaded if they have previously been saved with the same model state.</p> <p>The load method checks for a <code>buffer_state.safetensors</code> and a <code>metadata.json</code> file with the <code>buffer</code> metadata.</p> <p>Next, we'll start looking at each agent individually! \ud83d\ude80</p>"},{"location":"learn/tutorial/utils/","title":"General Utility Methods","text":"<p>Velora has a couple of utility methods to simplify minor, but tedious operations.</p> <p>We've split these into two main categories:</p> <ul> <li><code>core</code> - ones you'll use often.</li> <li><code>torch</code> - extras focusing on <code>PyTorch</code> operations.</li> </ul>"},{"location":"learn/tutorial/utils/#core","title":"Core","text":"<p>When you start a new experiment there are two things you will often ALWAYS want to do: set a random <code>seed</code> and initialize your computation <code>device</code>.</p> <p>We've got two methods that help with this: <code>set_seed</code> and <code>set_device</code>.</p> Tip <p>We highly recommend you call these two methods at the <code>start</code> of your experiments. Setting a seed helps with reproducibility and setting a device reduces training time.</p> <p>Both are extremely useful and shouldn't be ignored.</p>"},{"location":"learn/tutorial/utils/#setting-a-seed","title":"Setting a Seed","text":"API Docs <p><code>velora.utils.set_seed()</code></p> <p>Setting a seed controls the randomness for <code>Python</code>, <code>PyTorch</code> and <code>NumPy</code>, making your experiment results consistent and reproducible.</p> <p>You have two options here:</p> <ol> <li>Pass your own seed value</li> <li>Let it generate one automatically</li> </ol> Python<pre><code>from velora.utils import set_seed\n\nseed = set_seed(64)\nprint(seed)  # 64\n\nseed = set_seed()\nprint(seed)  # A random seed\n</code></pre> <p>This code should work 'as is'.</p> <p>Then, you would pass the <code>seed</code> to the <code>seed</code> parameter of to any of Velora's agents <code>init</code> method.</p>"},{"location":"learn/tutorial/utils/#setting-a-device","title":"Setting a Device","text":"API Docs <p><code>velora.utils.set_device(device)</code></p> <p>Setting a device controls where your <code>PyTorch</code> computations are performed.</p> <p>You can pass a device name in manually, or you can leave it blank and the device will be assigned automatically to <code>cuda:0</code> (if GPU enabled) or <code>cpu</code> (without).</p> Python<pre><code>from velora.utils import set_device\n\n# Automatically assigned\ndevice = set_device()  # torch.device('cuda:0') | torch.device('cpu')\n\n# Static assignment\ndevice = set_device(\"cuda:1\")  # torch.device('cuda:1')\n</code></pre> <p>This code should work 'as is'.</p> <p>Then, you would pass the <code>device</code> to the <code>device</code> parameter to any of Velora's agents <code>init</code> method.</p>"},{"location":"learn/tutorial/utils/#torch","title":"Torch","text":"<p>During your own experiments, you might find yourself in need of a quick way to convert data to a <code>PyTorch</code> tensor, or want to perform a parameter update between two networks, or even quickly check the number of <code>parameters</code> in a model.</p> <p>We've got a few methods to help with this:</p> <ul> <li><code>to_tensor</code> - converts a list of data to a <code>torch.Tensor</code>.</li> <li><code>stack_tensor</code> - stacks a list of <code>torch.Tensors</code> into a single one.</li> <li><code>soft_update</code> - performs a soft parameter update between two <code>torch.nn.Modules</code>.</li> <li><code>hard_update</code> - performs a hard parameter update between two <code>torch.nn.Modules</code>.</li> <li><code>total_parameters</code> - calculates the total number of parameters for a <code>torch.nn.Module</code>.</li> <li><code>active_parameters</code> - calculates the number of active parameters for a <code>torch.nn.Module</code>.</li> </ul>"},{"location":"learn/tutorial/utils/#item-list-to-tensor","title":"Item List to Tensor","text":"API Docs <p><code>velora.utils.to_tensor(items)</code></p> <p>Let's say you have a <code>List[int]</code> that are action values and you want to load them onto your GPU quickly while maintaining the data type.</p> <p>We can quickly do this using <code>to_tensor</code>:</p> Python<pre><code>from velora.utils import to_tensor, set_device\nimport torch\n\nactions = [1, 2, 4, 5, 1, 1]\n\n# Set our device\ndevice = set_device()\n\n# Convert the actions to a tensor\nactions = to_tensor(actions, dtype=torch.int, device=device)\n</code></pre> <p>This code should work 'as is'.</p> <p>By default, <code>dtype=torch.float32</code> and <code>device=None</code> so if you had a set of reward values (<code>float</code>) you'd only need to set <code>device</code>.</p> Python<pre><code>from velora.utils import to_tensor, set_device\n\nrewards = [1., 5, -1., -10., 1., 1.]\n\n# Set our device\ndevice = set_device()\n\n# Convert the rewards to a tensor with default `dtype`\nrewards = to_tensor(rewards, device=device)\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/tutorial/utils/#stacking-tensors","title":"Stacking Tensors","text":"API Docs <p><code>velora.utils.stack_tensor(items)</code></p> <p>What if we wanted to merge two <code>torch.Tensors</code> together? A common example would be environment observations in a buffer.</p> <p>For our simple example, we'll merge the <code>actions</code> and <code>rewards</code> from our previous section:</p> Python<pre><code>from velora.utils import stack_tensor, set_device\nimport torch\n\nactions = torch.tensor([1, 2, 4, 5, 1, 1], dtype=torch.int)\nrewards = torch.tensor([1., 5, -1., -10., 1., 1.])\n\n# Set our device\ndevice = set_device()\n\n# stack tensors with default `dtype`\nvalues = stack_tensor([actions, rewards], device=device)  # (2, 6)\n</code></pre> <p>This code should work 'as is'.</p> <p>Like <code>to_tensor</code>, we use <code>dtype=torch.float32</code> and <code>device=None</code> as defaults.</p>"},{"location":"learn/tutorial/utils/#soft-network-parameter-updates","title":"Soft Network Parameter Updates","text":"API Docs <p><code>velora.utils.soft_update(source, target, tau)</code></p> <p>Some algorithms, like <code>NeuroFlow</code>, perform soft target parameter updates using a hyperparameter \\(\\tau\\).</p> <p>This method performs that exact process. Given a <code>source</code> network, <code>target</code> network, and soft update factor <code>tau</code>, we iterate through our parameters and perform a soft update:</p> Python<pre><code>from copy import deepcopy\n\nfrom velora.models.sac.discrete import SACActorDiscrete\nfrom velora.utils import soft_update\n\n# Set two networks\nactor = SACActorDiscrete(4, 10, 1)\ntarget = deepcopy(actor)\n\n# Perform soft parameter update\nsoft_update(actor, target, tau=0.005)\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/tutorial/utils/#hard-network-parameter-updates","title":"Hard Network Parameter Updates","text":"API Docs <p><code>velora.utils.hard_update(source, target)</code></p> <p>Are soft updates too slow? We can perform a hard parameter update (without the \\(\\tau\\) factor) using <code>hard_update</code> instead:</p> Python<pre><code>from copy import deepcopy\n\nfrom velora.models.sac.discrete import SACActorDiscrete\nfrom velora.utils import hard_update\n\n# Set two networks\nactor = SACActorDiscrete(4, 10, 1)\ntarget = deepcopy(actor)\n\n# Perform hard parameter update\nhard_update(actor, target)\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/tutorial/utils/#model-parameter-counts","title":"Model Parameter Counts","text":"API Docs <p><code>velora.utils.total_parameters(model)</code></p> <p><code>velora.utils.active_parameters(model)</code></p> <p>Ever been curious about the number of parameters a model has? We've got a few methods to quickly help with this!</p> <p><code>total_parameters()</code> for ALL parameters, and <code>active_parameters()</code> for ones in use:</p> Python<pre><code>from velora.models import SACActorDiscrete\nfrom velora.utils import total_parameters, active_parameters\n\nmodel = SACActorDiscrete(4, 10, 1)\n\ntotal_params = total_parameters(model)\nactive_params = active_parameters(model)\nprint(total_params, active_params)\n</code></pre> <p>This code should work 'as is'.</p> Active Parameters <p><code>active_parameters</code> is useful when you first initialize a sparsely connected model. </p> <p>Using it after training, returns the same result as <code>total_parameters</code> due to floating point precision errors. Sparsity masked weights are still close to <code>0</code> but with a small <code>+-</code> variance. </p> <p>That wraps up our utility methods and our user guide tutorials. Excellent work getting this far! \ud83d\udc4f</p> <p>Still eager to learn more? Try one of the options \ud83d\udc47:</p> <ul> <li> <p> Customization</p> <p>Learn how to create your own algorithms using Velora's building blocks.</p> <p> Read more</p> </li> <li> <p> Theory</p> <p>Read the theory behind the RL algorithms Velora uses.</p> <p> Start learning</p> </li> </ul>"},{"location":"learn/tutorial/agents/","title":"Velora Agents","text":"<p>Velora has a few pre-built agents that are easy to use and integrate into your projects.</p> <p>In this section we'll highlight each one individually, outlining their unique characteristics and implementation details.</p> In a Hurry? <p>Looking to jump to something specific? Use the navigation menu on the left \ud83d\udc48!</p> <p>When you're ready, click on the button \ud83d\udc47 to start learning about them! \ud83d\ude80</p>"},{"location":"learn/tutorial/agents/nf-ct/","title":"NeuroFlow - Continuous","text":"API Docs <p><code>velora.models.NeuroFlowCT(env_id, actor_neurons, critic_neurons)</code></p> <p>This algorithm focuses on the <code>continuous</code> action space implementation of <code>NeuroFlow</code> (NF).</p> Discrete vs. Continuous <p>The API and docs are identical to the <code>discrete</code> variant with some slight differences to the <code>init</code> parameters and the class name. We now use <code>NeuroFlowCT</code> instead of <code>NeuroFlow</code>.</p> <p>Feel free to jump to the part you need to save some time! \ud83d\ude00</p> <p>It builds on top of a Soft Actor-Critic (continuous) [] (SAC) base and combines a variety of well-known RL techniques with some of our own custom ones.</p> <p>These include the following features:</p> <ul> <li>Small Actor and large Critic networks - from the paper: Honey, I Shrunk The Actor: A Case Study on Preserving Performance with Smaller Actors in Actor-Critic RL  [].</li> <li>Differing Actor-Critic architectures - the Actor uses a <code>LiquidNCPNetwork</code> and the Critic's use <code>NCPNetworks</code>.</li> <li>Automatic Entropy Adjustment (Learned) - from the paper: Soft Actor-Critic Algorithms and Applications [].</li> </ul> <p>Plus more, coming soon.</p> <p>Agent's Future</p> <p>Cyber environments typically use <code>discrete</code> action spaces so this agent won't be used as heavily as the <code>discrete</code> variant.</p> <p>As such, this may be discontinued and removed in a future release. Alternatively, we may still keep it but limit it to it's current features without any additional ones that are planned for the full algorithm.</p> <p>To build one, we use the <code>NeuroFlowCT</code> class.</p>"},{"location":"learn/tutorial/agents/nf-ct/#building-the-model","title":"Building the Model","text":"<p>In it's simplest form, we can create one with just one line using three parameters:</p> Parameter Description Example <code>env_id</code> The Gymnasium environment ID. <code>InvertedPendulum-v5</code> <code>actor_neurons</code> The number of decision/hidden nodes for the Actor network. <code>20</code> or <code>40</code> <code>critic_neurons</code> The number of decision/hidden nodes for the Critic networks. We recommend this to be higher than the Actor network <code>128</code> or <code>256</code> Python<pre><code>from velora.models import NeuroFlowCT\n\nmodel = NeuroFlowCT(\"InvertedPendulum-v5\", 20, 128)\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/tutorial/agents/nf-ct/#optional-parameters","title":"Optional Parameters","text":"<p>This will create an instance of the model with the following default parameters:</p> Parameter Description Default <code>optim</code> The PyTorch optimizer. <code>torch.optim.Adam</code> <code>buffer_size</code> The <code>ReplayBuffer</code> size. <code>1M</code> <code>actor_lr</code> The actor optimizer learning rate. <code>0.0003</code> <code>critic_lr</code> The critic optimizer learning rate. <code>0.0003</code> <code>alpha_lr</code> The entropy optimizer learning rate. <code>0.0003</code> <code>initial_alpha</code> The starting entropy coefficient. <code>1.0</code> <code>log_std</code> The <code>(low, high)</code> bounds for the log standard deviation of the action distribution. Used to control the variance of actions. <code>(-5, 2)</code> <code>tau</code> The soft update factor for slowly updating the target network weights. <code>0.005</code> <code>gamma</code> The reward discount factor. <code>0.99</code> <code>device</code> The device to perform computations on. E.g., <code>cpu</code> or <code>cuda:0</code>. <code>None</code> <code>seed</code> The random generation seed for <code>Python</code>, <code>PyTorch</code>, <code>NumPy</code> and <code>Gymnasium</code>. When <code>None</code>, seed is automatically generated. <code>None</code> <p>You can customize them freely using the required parameter name.</p> <p>We strongly recommend that use the <code>set_device</code> utility method before initializing the model to help with faster training times:</p> Python<pre><code>from velora.models import NeuroFlowCT\nfrom velora.utils import set_device\n\ndevice = set_device()\n\nmodel = NeuroFlowCT(\"InvertedPendulum-v5\", 20, 128, device=device)\n</code></pre> <p>This code should work 'as is'.</p> <p><code>NeuroFlowCT</code> uses the <code>set_seed</code> utility method automatically when the model's <code>seed=None</code>. This saves you having to manually create it first! \ud83d\ude09</p>"},{"location":"learn/tutorial/agents/nf-ct/#training-the-model","title":"Training the Model","text":"API Docs <p><code>velora.models.NeuroFlowCT.train(batch_size)</code></p> <p>Training the model is equally as simple! \ud83d\ude0a</p> <p>We just use the <code>train()</code> method given a <code>batch_size</code>:</p> Python<pre><code>from velora.models import NeuroFlowCT\nfrom velora.utils import set_device\n\ndevice = set_device()\n\nmodel = NeuroFlowCT(\"InvertedPendulum-v5\", 20, 128, device=device)\nmodel.train(256)\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/tutorial/agents/nf-ct/#optional-parameters_1","title":"Optional Parameters","text":"<p>This will train the agent with the following default parameters:</p> Parameter Description Default <code>n_episodes</code> The number of episodes to train for. <code>10k</code> <code>callbacks</code> A list of training callbacks applied during the training process. <code>None</code> <code>log_freq</code> The metric logging frequency for offline and online analytics (in episodes). <code>10</code> <code>display_count</code> The console training progress frequency (in episodes). <code>100</code> <code>window_size</code> The reward moving average size (in episodes). <code>100</code> <code>max_steps</code> The total number of steps per episode. <code>1000</code> <code>warmup_steps</code> The number of samples to generate in the buffer before starting training. If <code>None</code> uses <code>batch_size * 2</code>. <code>None</code> <p>Like before, you can customize these freely using the required parameter name.</p>"},{"location":"learn/tutorial/agents/nf-ct/#making-a-prediction","title":"Making a Prediction","text":"API Docs <p><code>velora.models.NeuroFlowCT.predict(state, hidden)</code></p> <p>To make a new prediction, we need to pass in a environment <code>state</code> and a <code>hidden</code> state.</p> Python<pre><code>action, hidden = model.predict(state, hidden)\n</code></pre>"},{"location":"learn/tutorial/agents/nf-ct/#optional-parameters_2","title":"Optional Parameters","text":"<p>This will make a prediction with the following default parameters:</p> Parameter Description Default <code>train_mode</code> A flag for swapping between deterministic and stochastic action predictions. <ul><li>When <code>False</code> - deterministic action predictions. Recommend for evaluating the model.</li><li>When <code>True</code> - stochastic action predictions. Required for training the model.</li></ul> <code>False</code> <p>Every prediction returns the <code>action</code> prediction and the <code>hidden</code> state.</p> <p>If it's a one time prediction, <code>hidden=None</code> is perfect, but you'll likely be using this in a real-time setting so you'll need to pass the <code>hidden</code> state back into the next prediction and use a pre-wrapped environment (<code>model.eval_env</code>).</p>"},{"location":"learn/tutorial/agents/nf-ct/#example","title":"Example","text":"<p>Here's a code example:</p> Python<pre><code>from velora.models import NeuroFlowCT\nfrom velora.utils import set_device\n\ndevice = set_device()\n\nmodel = NeuroFlowCT(\"InvertedPendulum-v5\", 20, 128, device=device, seed=64)\nmodel.train(128, n_episodes=100)\n\n# Set prediction env\nenv = model.eval_env\n\n# Run trained agent for 5 episodes\nep_total = 5\nfor i_ep in range(1, ep_total + 1):\n    state, _ = env.reset()\n    hidden = None\n\n    while True:\n        action, hidden = model.predict(state, hidden)\n        state, reward, terminated, truncated, info = env.step(action)\n\n        episode_over = terminated or truncated\n\n        if episode_over:\n            ep_return = info[\"episode\"][\"r\"].item()\n            print(f\"Episode: {i_ep}/{ep_total}, Reward: {ep_return:.2f}\")\n            break\n</code></pre> <p>This code should work 'as is'.</p> <p>Now, let's see how we can use our <code>offline</code> training metrics! \ud83d\ude80</p>"},{"location":"learn/tutorial/agents/nf/","title":"NeuroFlow - Discrete","text":"API Docs <p><code>velora.models.NeuroFlow(env_id, actor_neurons, critic_neurons)</code></p> <p>Our first algorithm focuses on the <code>discrete</code> action space implementation of <code>NeuroFlow</code> (NF).</p> <p>It builds on top of a Soft Actor-Critic (discrete) [] (SAC) base and combines a variety of well-known RL techniques with some of our own custom ones.</p> <p>These include the following features:</p> <ul> <li>Small Actor and large Critic networks - from the paper: Honey, I Shrunk The Actor: A Case Study on Preserving Performance with Smaller Actors in Actor-Critic RL  [].</li> <li>Differing Actor-Critic architectures - the Actor uses a <code>LiquidNCPNetwork</code> and the Critic's use <code>NCPNetworks</code>.</li> <li>Automatic Entropy Adjustment (Learned) - from the paper: Soft Actor-Critic Algorithms and Applications [].</li> </ul> <p>Plus more, coming soon.</p> <p>To build one, we use the <code>NeuroFlow</code> class.</p>"},{"location":"learn/tutorial/agents/nf/#building-the-model","title":"Building the Model","text":"<p>In it's simplest form, we can create one with just one line using three parameters:</p> Parameter Description Example <code>env_id</code> The Gymnasium environment ID. <code>CartPole-v1</code> <code>actor_neurons</code> The number of decision/hidden nodes for the Actor network. <code>20</code> or <code>40</code> <code>critic_neurons</code> The number of decision/hidden nodes for the Critic networks. We recommend this to be higher than the Actor network <code>128</code> or <code>256</code> Python<pre><code>from velora.models import NeuroFlow\n\nmodel = NeuroFlow(\"InvertedPendulum-v5\", 20, 128)\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/tutorial/agents/nf/#optional-parameters","title":"Optional Parameters","text":"<p>This will create an instance of the model with the following default parameters:</p> Parameter Description Default <code>optim</code> The PyTorch optimizer. <code>torch.optim.Adam</code> <code>buffer_size</code> The <code>ReplayBuffer</code> size. <code>1M</code> <code>actor_lr</code> The actor optimizer learning rate. <code>0.0003</code> <code>critic_lr</code> The critic optimizer learning rate. <code>0.0003</code> <code>alpha_lr</code> The entropy optimizer learning rate. <code>0.0003</code> <code>initial_alpha</code> The starting entropy coefficient. <code>1.0</code> <code>tau</code> The soft update factor for slowly updating the target network weights. <code>0.005</code> <code>gamma</code> The reward discount factor. <code>0.99</code> <code>device</code> The device to perform computations on. E.g., <code>cpu</code> or <code>cuda:0</code>. <code>None</code> <code>seed</code> The random generation seed for <code>Python</code>, <code>PyTorch</code>, <code>NumPy</code> and <code>Gymnasium</code>. When <code>None</code>, seed is automatically generated. <code>None</code> <p>You can customize them freely using the required parameter name.</p> <p>We strongly recommend that use the <code>set_device</code> utility method before initializing the model to help with faster training times:</p> Python<pre><code>from velora.models import NeuroFlow\nfrom velora.utils import set_device\n\ndevice = set_device()\n\nmodel = NeuroFlow(\"InvertedPendulum-v5\", 20, 128, device=device)\n</code></pre> <p>This code should work 'as is'.</p> <p><code>NeuroFlow</code> uses the <code>set_seed</code> utility method automatically when the model's <code>seed=None</code>. This saves you having to manually create it first! \ud83d\ude09</p>"},{"location":"learn/tutorial/agents/nf/#training-the-model","title":"Training the Model","text":"API Docs <p><code>velora.models.NeuroFlow.train(batch_size)</code></p> <p>Training the model is equally as simple! \ud83d\ude0a</p> <p>We just use the <code>train()</code> method given a <code>batch_size</code>:</p> Python<pre><code>from velora.models import NeuroFlow\nfrom velora.utils import set_device\n\ndevice = set_device()\n\nmodel = NeuroFlow(\"InvertedPendulum-v5\", 20, 128, device=device)\nmodel.train(256)\n</code></pre> <p>This code should work 'as is'.</p>"},{"location":"learn/tutorial/agents/nf/#optional-parameters_1","title":"Optional Parameters","text":"<p>This will train the agent with the following default parameters:</p> Parameter Description Default <code>n_episodes</code> The number of episodes to train for. <code>10k</code> <code>callbacks</code> A list of training callbacks applied during the training process. <code>None</code> <code>log_freq</code> The metric logging frequency for offline and online analytics (in episodes). <code>10</code> <code>display_count</code> The console training progress frequency (in episodes). <code>100</code> <code>window_size</code> The reward moving average size (in episodes). <code>100</code> <code>max_steps</code> The total number of steps per episode. <code>1000</code> <code>warmup_steps</code> The number of samples to generate in the buffer before starting training. If <code>None</code> uses <code>batch_size * 2</code>. <code>None</code> <p>Like before, you can customize these freely using the required parameter name.</p>"},{"location":"learn/tutorial/agents/nf/#making-a-prediction","title":"Making a Prediction","text":"API Docs <p><code>velora.models.NeuroFlow.predict(state, hidden)</code></p> <p>To make a new prediction, we need to pass in a environment <code>state</code> and a <code>hidden</code> state.</p> Python<pre><code>action, hidden = model.predict(state, hidden)\n</code></pre>"},{"location":"learn/tutorial/agents/nf/#optional-parameters_2","title":"Optional Parameters","text":"<p>This will make a prediction with the following default parameters:</p> Parameter Description Default <code>train_mode</code> A flag for swapping between deterministic and stochastic action predictions. <ul><li>When <code>False</code> - deterministic action predictions. Recommend for evaluating the model.</li><li>When <code>True</code> - stochastic action predictions. Required for training the model.</li></ul> <code>False</code> <p>Every prediction returns the <code>action</code> prediction and the <code>hidden</code> state.</p> <p>If it's a one time prediction, <code>hidden=None</code> is perfect, but you'll likely be using this in a real-time setting so you'll need to pass the <code>hidden</code> state back into the next prediction and use a pre-wrapped environment (<code>model.eval_env</code>).</p>"},{"location":"learn/tutorial/agents/nf/#example","title":"Example","text":"<p>Here's a code example:</p> Python<pre><code>from velora.models import NeuroFlow\nfrom velora.utils import set_device\n\ndevice = set_device()\n\nmodel = NeuroFlow(\"InvertedPendulum-v5\", 20, 128, device=device, seed=64)\nmodel.train(128, n_episodes=100)\n\n# Set prediction env\nenv = model.eval_env\n\n# Run trained agent for 5 episodes\nep_total = 5\nfor i_ep in range(1, ep_total + 1):\n    state, _ = env.reset()\n    hidden = None\n\n    while True:\n        action, hidden = model.predict(state, hidden)\n        state, reward, terminated, truncated, info = env.step(action)\n\n        episode_over = terminated or truncated\n\n        if episode_over:\n            ep_return = info[\"episode\"][\"r\"].item()\n            print(f\"Episode: {i_ep}/{ep_total}, Reward: {ep_return:.2f}\")\n            break\n</code></pre> <p>This code should work 'as is'.</p> <p>That covers the <code>discrete</code> variant! Next, we'll look at the <code>continuous</code> one. See you there! \ud83d\udc4b</p>"},{"location":"starting/","title":"Getting Started","text":"<p>To get started, simply install it through pip [] using one of the options below.</p> <p>For PyTorch [] with CUDA (recommended):</p> Bash<pre><code>pip install torch torchvision velora --extra-index-url https://download.pytorch.org/whl/cu126\n</code></pre> <p>Or, for PyTorch [] with CPU only:</p> Bash<pre><code>pip install torch torchvision velora\n</code></pre>"},{"location":"starting/#example-usage","title":"Example Usage","text":"<p>Here's a simple example:</p> Python<pre><code>from velora.models import NeuroFlow, NeuroFlowCT\nfrom velora.utils import set_device\n\n# Setup PyTorch device\ndevice = set_device()\n\n# For continuous tasks\nmodel = NeuroFlowCT(\n    \"InvertedPendulum-v5\",\n    20,  # actor neurons \n    128,  # critic neurons\n    device=device,\n    seed=64,  # remove for automatic generation\n)\n\n# For discrete tasks\nmodel = NeuroFlow(\n    \"CartPole-v1\",\n    20,  # actor neurons \n    128,  # critic neurons\n    device=device,\n)\n\n# Train the model using a batch size of 64\nmodel.train(64, n_episodes=50, display_count=10)\n</code></pre> <p>This code should work 'as is'.</p> <p>Currently, the framework only supports Gymnasium [] environments and is planned to expand to PettingZoo [] for Multi-agent (MARL) tasks, with updated adaptations of CybORG [] environments.</p>"},{"location":"starting/#api-structure","title":"API Structure","text":"<p>The frameworks API is designed to be simple and intuitive. We've broken into two main categories: <code>core</code> and <code>extras</code>.</p>"},{"location":"starting/#core","title":"Core","text":"<p>The primary building blocks you'll use regularly.</p> Python<pre><code>from velora.models import [algorithm]\nfrom velora.callbacks import [callback]\n</code></pre>"},{"location":"starting/#extras","title":"Extras","text":"<p>Utility methods that you may use occasionally.</p> Python<pre><code>from velora.gym import [method]\nfrom velora.utils import [method]\n</code></pre>"},{"location":"starting/#next-steps","title":"Next Steps","text":"<ul> <li> <p> NeuroFlow Agents</p> <p>Learn how to use NeuroFlow models.</p> <p> Start learning</p> </li> <li> <p> Customize</p> <p>Learn how to build your own models.</p> <p> Go custom</p> </li> </ul>"},{"location":"starting/history/","title":"Brief History &amp; Motivation","text":"<p>During the LLM boom of 2023, AI became larger and larger. It was framed that scale was the solution to better, more powerful AI models. This required more money, more energy, and less explainability.</p> <p>For two years LLMs continued to reign supreme without any hope of slowing down. Fed up with scale being the only solution, we set our sights on a different one, a better one. One with explainability in mind, and thus, the idea of Velora was born.</p> <p>We focus on RL because of how fascinating the topic is. Agents that learn through trial-and-error, just like humans do. Does that not sound like true Artificial Intelligence to you? Our only problem was the size of models.</p> <p>One fateful day, browsing our YouTube feed, a hidden gem presented itself - Liquid Neural Networks [] (LNNs).</p> <p>An architecture that is powerful, small, robust and explainable. Yet, no one was talking about it. Have people not seen the architecture? Are they blinded by the power of LLMs? We couldn't put our finger on it. Months went by and LLMs still boomed, achieving great results but still growing in size without an end in sight, until finally we had enough.</p> <p>For 2 weeks, we studied LNNs, rebuilding the architecture from scratch. Understanding all the nooks and crannies of it and finally incorporated it into our first RL algorithm - DDPG. We started it up using the Inverted Pendulum Gymnasium environment [] and waited... it didn't work. The critic refused to learn. We then tried an official implementation and the same thing happened. Dreams shattered... Or so we thought.</p> <p>Determined to make it work, we decomposed the problem:</p> <ul> <li>Sparse linear layers on their own - success </li> <li>LTC cell without recurrence - fail </li> </ul> <p>We found our problem. Only the hidden state was being returned. There was no cell prediction! A simple projection layer from input size to output size + a residual connection to the hidden state and boom \ud83d\udca5. The algorithm sprung to life.</p> Old LTC CellNew LTC Cell Python<pre><code>#...\n\nnew_hidden = self._new_hidden(x, g_out, h_out)\nreturn new_hidden, new_hidden\n</code></pre> Python<pre><code>self.proj = nn.Linear(self.head_size, n_hidden, device=device)\n\n# ...\n\nnew_hidden = self._new_hidden(x, g_out, h_out)  # (1)\ny_pred = self._sparse_head(x, self.proj) + new_hidden  # (2)\nreturn y_pred, new_hidden\n</code></pre> <ol> <li>Old return - hidden state only.</li> <li>Residual connection - projection + hidden state.</li> </ol> <p>With this change, we had a fully functioning Liquid DDPG.</p> <p>Now to test it again - 3 layers, 20 'hidden' (decision) neurons, all sparsely connected, randomly wired in a specific way, trained for 500 episodes - environment complete .</p> <p>This was it, the first real step on our journey to making Velora a reality.</p> <p>That was on the 17th February 2025 and marks the true birth \ud83c\udf82 of Velora, our Liquid RL framework.</p> <p>After experimenting with different RL algorithms for two months (DDPG, PPO, SAC), we started to reconsider the direction to take for our framework. Rebuilding the same algorithms with a different architecture seemed like a pointless endeavor. We weren't pushing the field forward, just toying with the idea that LNNs are better.</p> <p>On the 12th April 2025, we decided to shift our focus to a single agent architecture, one of our own making that's fully autonomous and learns strictly through experience - NeuroFlow. This has now become our obsession and main focus of the framework.</p> <p>We have big plans for NeuroFlow and are interested to see where it takes us! It still uses LNNs as a baseline and we intend to keep explainability in mind, focusing on these 5 pillars:</p> <ul> <li>Causality - how changes in parameters and elements (e.g., NN building blocks) of the agent alter the decision making process.</li> <li>Fairness - the decisions made by the agent are not biased and are independent of a selected group of sensitive features (e.g., gender, ethnicity, image backgrounds).</li> <li>Robustness and Reliability - the agent is effective under input or parameter perturbations (e.g., noise).</li> <li>Usability - the model is simple to use for accomplishing a task.</li> <li>Trust - the user has high confidence when applying the model in production.</li> </ul> <p>We've got a long road ahead but we are excited to make it happen! \ud83d\ude80</p> <p>We hope you all enjoy using Velora and our NeuroFlow agents as much as we do making it and we look forward to seeing what you all create with it! \ud83c\udf7b</p>"},{"location":"starting/homage/","title":"Homage","text":"<p>Velora comes packed with a variety of packages and builds on a wealth of knowledge that have been fundamental to its creation.</p> <p>This page acts as a homage to show our  and appreciation for the libraries and researchers that have helped make its packages possible.</p>"},{"location":"starting/homage/#packages","title":"Packages","text":"<ul> <li> PyTorch for model creation</li> <li> Pydantic for data validation</li> <li> Gymnasium for environment management</li> <li> Poetry for package management</li> <li> Pytest for unit testing</li> </ul>"},{"location":"starting/homage/#research-papers","title":"Research Papers","text":"<p>We are thankful for a LOT of researchers in the RL space, more than we can count! \ud83d\ude05</p> <p>While their names are not mentioned here, their papers are outlined throughout the documentation accompanied with the algorithm implementations associated to their work.</p> <p>Instead, we want to dedicate this section to some special mentions that the framework centres around: Liquid Neural Networks and Neural Circuit Policies.</p>"},{"location":"starting/homage/#liquid-neural-networks","title":"Liquid Neural Networks","text":"<p> Liquid Time-Constant Networks</p> <p> Interpretable Recurrent Neural Networks in Continuous-Time Control Environments</p> <p> Closed-Form Continuous-Time Neural Networks</p>"},{"location":"starting/homage/#neural-circuit-policies","title":"Neural Circuit Policies","text":"<p> Neuronal Circuit Policies</p> <p> Neural Ordinary Differential Equations</p> <p> A Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits</p>"},{"location":"starting/license/","title":"License","text":"<p>Velora is licensed under the MIT license. The MIT license is a permissive license that allows commercial use, redistribution, and everything else as long as the original authors are credited in source files.</p> <p>The MIT license is the most popular license among all Open Source projects.</p>"},{"location":"starting/license/#mit-license","title":"MIT License","text":"<p>The MIT License gives you the freedom to work with the code with minimal restrictions. When using Velora, make sure to include the copyright notice and avoid future liability claims.</p> <p>Its appeal lies in its simplicity and clear terms, making it a go-to for projects of any scale. Its popularity stems from its simplicity, clarity, and development-friendly terms, making it suitable for all types of projects.</p>"},{"location":"starting/license/#advantages","title":"Advantages","text":"<p>The MIT License is favored for its balance between freedom and protection, appealing to those who find GNU/GPL licenses too restrictive or advocate for privatized software. Its broad wording supports both open community development and the use of components in proprietary software.</p> <p>Here\u2019s what this means for you:</p> <ul> <li>Freedom from vendor lock-in: Thanks to Open Source principles, you can use the Velora without any strings attached. Customize or integrate it into your projects without worrying about restrictions. Your projects belong to you, and you are free to change the software at any time.</li> </ul>"},{"location":"starting/roadmap/","title":"Project Roadmap","text":"<p>We've recently changed the direction of Velora, moving from a global RL framework that uses LNNs with all existing RL algorithms, to one that focuses on a custom architecture built up of unique RL techniques.</p> <p>Our goal is to create a completely autonomous system that learns from it's environment without human intervention. We want to help advance the field forward into the Era of Experience [], specifically, in the cyber domain.</p> <p>Cyber threats are never ending and always changing so it's easy to get overwhelmed by them. Autonomy is needed. Thus, we are working on a custom solution - <code>NeuroFlow</code>.</p> <p>You'll find it's details in the roadmap below along with additions we are planning. We've broken them down into different sections with checkboxes. Items that are green are already implemented.</p> <p>We've got a lot planned for and are excited for the future of Autonomous Cyber Defense.</p>"},{"location":"starting/roadmap/#road-to-10-release","title":"Road to 1.0 Release","text":"<p>So far, we've got a solid foundation for <code>Velora</code> and our <code>NeuroFlow</code> agents but there is still much to do!</p> <p>Here's our plans and progress so far:</p> <ul> <li> <p> Main Components/Modules</p> <ul> <li> Liquid Neural Networks (CfC) - 2022</li> <li> SAC: Continuous - 2018</li> <li> SAC: Discrete - 2019</li> <li> SAC: Automatic Entropy - 2018</li> <li> Replay Buffer</li> <li> Small Actor, Large Critics: Honey, I Shrunk the Actor - 2021</li> <li> CAT-SAC: SAC with Curiosity-Aware Entropy Temperature - 2020</li> <li> PlaNet: Learning Latent Dynamics for Planning from Pixels - 2018</li> <li> EWC: Overcoming Catastrophic Forgetting in Neural Networks - 2016</li> </ul> </li> </ul> <ul> <li> <p> Custom Components/Modules</p> <ul> <li> Liquid NCP Actor, NCP Critics</li> <li> Strategy Library</li> <li> Adaptive Network using fitness score</li> </ul> </li> </ul> <ul> <li> <p> Utility</p> <ul> <li> Setting seed and device</li> <li> Gymnasium environment search and wrappers</li> <li> Saving and Loading models &amp; buffers</li> <li> Training and predicting with models</li> <li> Agent performance tracking (offline &amp; online)</li> <li> Early stopping and checkpoint save system</li> <li> Recording episode performance</li> </ul> </li> </ul> <ul> <li> <p> Simulation Environments</p> <ul> <li> Gymnasium</li> <li> PettingZoo</li> <li> CybORG - Customized</li> </ul> </li> </ul>"}]}