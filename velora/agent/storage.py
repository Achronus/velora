from abc import ABC, abstractmethod
from typing import Any, Self
import torch

from pydantic import BaseModel, ConfigDict, PrivateAttr

from velora.exc import RolloutsFullError

SingleRolloutType = tuple[torch.Tensor, torch.LongTensor, torch.FloatTensor]


class Storage(ABC, BaseModel):
    """A base class for agent storage containers."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @abstractmethod
    def add(self, *args: Any, **kwargs: Any) -> None:
        pass  # pragma: no cover

    @abstractmethod
    def __getitem__(self, index: int) -> Any:
        pass  # pragma: no cover

    @abstractmethod
    def __len__(self) -> int:
        pass  # pragma: no cover

    @abstractmethod
    def __repr__(self) -> str:
        pass  # pragma: no cover


class Rollouts(Storage):
    """
    A finite sequence of agent steps through the environment.

    Args:
        size (int): maximum size of the rollouts
        obs_shape (tuple[int, int]): the shape of a single observation
        device (torch.device, optional): device to run computations on, such as `cpu`, `cuda` (Default is cpu)
    """

    size: int
    obs_shape: tuple[int, int]
    device: torch.device = torch.device("cpu")

    _obs: torch.Tensor = PrivateAttr(...)
    _actions: torch.LongTensor = PrivateAttr(...)
    _rewards: torch.FloatTensor = PrivateAttr(...)
    _count: int = PrivateAttr(default=0)

    def model_post_init(self, __context: Any) -> None:
        self._obs = torch.zeros((self.size, *self.obs_shape))
        self._actions = torch.zeros(self.size, dtype=torch.long)
        self._rewards = torch.zeros(self.size, dtype=torch.float)

    @property
    def obs(self) -> torch.Tensor:
        """Returns the rollout observations."""
        return self._obs

    @property
    def actions(self) -> torch.LongTensor:
        """Returns the rollout actions."""
        return self._actions

    @property
    def rewards(self) -> torch.FloatTensor:
        """Returns the rollouts rewards."""
        return self._rewards

    def add(
        self,
        obs: torch.Tensor,
        action: int,
        reward: float,
    ) -> None:
        """
        Add a single observation, action, and reward.

        Args:
            obs (torch.Tensor): the state generated by the environment
            action (int): the agent's action for the state
            reward (float): the reward for the state
        """
        if self._count >= self.size:
            raise RolloutsFullError("Rollouts are full. Cannot add another.")

        self._obs[self._count] = obs
        self._actions[self._count] = action
        self._rewards[self._count] = reward
        self._count += 1

    def returns(self, gamma: float = 0.9) -> torch.FloatTensor:
        """
        Computes the discounted return for each step.

        The return G_t at each timestep is calculated as:
        G_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... = Σ_{k=0}^∞ γᵏR_{t+k+1}

        Where:
        - G_t is the return at time t
        - R_t is the reward at time t
        - γ (gamma) is the discount factor
        - k is the number of steps into the future

        When iterating, this simplifies to use the future return:
        G_t = R_{t} + γG_{t+1}

        Args:
            gamma: Discount factor (default: 0.9)

        Returns:
            G (torch.FloatTensor): A tensor of discounted returns for each trajectory
        """
        if gamma == 0:
            return self.rewards

        n_rewards = len(self.rewards)
        m_shape = (n_rewards, n_rewards)

        mask = torch.triu(torch.ones(m_shape, device=self.device))
        r_matrix = self.rewards.expand(m_shape)
        idx_row = torch.arange(n_rewards, device=self.device)

        # Create diagonal discount matrix
        d_matrix = idx_row.expand(m_shape)
        d_matrix = d_matrix - idx_row.unsqueeze(1)
        d_matrix = gamma ** torch.clamp(d_matrix, min=0)

        # Sum each row
        return (mask * r_matrix * d_matrix).sum(dim=1).to(self.device)

    def score(self) -> float:
        """Calculates the total score of the rollouts."""
        return float(self._rewards.sum().item())

    def __getitem__(self, index: int | slice) -> Self | SingleRolloutType:
        if isinstance(index, slice):
            size = len(range(*index.indices(len(self))))
            sliced = Rollouts(
                size=size,
                obs_shape=self.obs_shape,
                device=self.device,
            )
            sliced._obs = self._obs[index]
            sliced._actions = self._actions[index]
            sliced._rewards = self._rewards[index]
            return sliced

        return (
            self._obs[index],
            self._actions[index],
            self._rewards[index],
        )

    def __len__(self) -> int:
        return self.size

    def __repr__(self) -> str:
        return f"Rollouts(size={self.size}, obs={self._obs}, actions={self._actions}, rewards={self._rewards})"

    def __str__(self) -> str:
        return f"[{",\n ".join([f"({obs}, {act}, {rew})" for obs, act, rew in zip(self._obs, self._actions, self._rewards)])}]"


class ReplayBuffer(Storage):
    """"""

    pass
