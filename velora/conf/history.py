from pydantic import BaseModel, ConfigDict
import numpy as np


class Trajectory(BaseModel):
    """A single agent trajectory generated by the environment."""

    action: int
    state: np.ndarray
    reward: float

    model_config = ConfigDict(arbitrary_types_allowed=True)


class History(BaseModel):
    """A set of agent trajectories."""

    items: list[Trajectory]

    def actions(self) -> list[int]:
        """Returns the actions for each trajectory."""
        return [t.action for t in self.items]

    def states(self) -> list[np.ndarray]:
        """Returns the states for each trajectory."""
        return [t.state for t in self.items]

    def rewards(self) -> list[int]:
        """Returns the rewards for each trajectory."""
        return [t.reward for t in self.items]

    def returns(self, gamma: float = 1) -> list[float]:
        """
        Computes the discounted return for each trajectory. Starts at last trajectory and iterates backwards through time `t`.

        The return G_t at each timestep is calculated as:
        G_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... = Σ_{k=0}^∞ γᵏR_{t+k+1}

        Where:
        - G_t is the return at time t
        - R_t is the reward at time t
        - γ (gamma) is the discount factor
        - k is the number of steps into the future

        When iterating, this simplifies to use the future return:
        G_t = R_{t} + γG_{t+1}

        Args:
            gamma: Discount factor (default: 1.0)

        Returns:
            G: A list of discounted returns for each trajectory
        """
        if len(self.items) == 0:
            return []

        n = len(self.items)
        G = [0.0] * n

        G[n - 1] = self.items[n - 1].reward

        for t in range(n - 2, -1, -1):
            G[t] = self.items[t].reward + gamma * G[t + 1]

        return G
